<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TLYu的學習筆記</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://TLYu0419.github.io/"/>
  <updated>2018-10-07T08:25:26.030Z</updated>
  <id>https://TLYu0419.github.io/</id>
  
  <author>
    <name>TL-Yu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>台灣的文化品味如何？階級與品味的多變量分析</title>
    <link href="https://TLYu0419.github.io/2018/09/29/Taste-Cluster/"/>
    <id>https://TLYu0419.github.io/2018/09/29/Taste-Cluster/</id>
    <published>2018-09-29T02:07:34.000Z</published>
    <updated>2018-10-07T08:25:26.030Z</updated>
    
    <content type="html"><![CDATA[<p>從前歐洲的封建社會有個很在意階級身分的貴族，有一天突然發現他竟然和一位地位最低下的僕人一樣－－都是用兩隻腳在走路！為此這位貴族煩惱了好一陣子，擔心自己走在路上時會不會也被旁人誤會自己是位僕人。這可是攸關面子的大問題呢，怎麼能容忍這樣的事情發生呢！於是貴族開始想方設法要用各種方式展現出自己的生活品味有多麼不同，首先在出門時交通方式，他雇用了一個乘騎大象的司機來接送自己出門之外，還要求在大象背上得有一個遮陽亭子，此外亭子的周圍還得有七種顏色亮麗的旗子裝飾。</p><p>品味是一種上層階級用來區隔自己有多麼不同的一種方式，除了上述外顯的交通工具之外，更有意思的是在平常生活中，看似不起眼的穿著、飲食、閱讀、聽音樂的類型…等等這些隱性的文化品味，這些品味會在日常生活中的舉手投足之間不經意地展現出來，反映了每個人背後的家庭背景、教育程度或知識類型。更可怕的是「品味」這件事情除了會藉由複製的方式傳遞在階級之間傳遞，也還會反過來成為一種階級再製的工具。</p><a id="more"></a><h1 id="相關文獻"><a href="#相關文獻" class="headerlink" title="相關文獻"></a>相關文獻</h1><p>探討文化品味與社會階層之間的關聯性一直是相當熱門的社會學議題。如<a href="http://www.facoltaspes.unimi.it/files/_ITA_/COM/Social_stratification_and_cultural_consumption_-_COM.pdf" target="_blank" rel="noopener">Social stratification and cultural consumption</a>研究提到的內容，階級與品味之間的關聯性有3個主要的論點，首先同源論(homology)認為品味與階級之間具有單一的對應關係(Bourdieu 1984)，也就是上層階級只喜歡高雅文化，而下層階級則偏好通俗文化。其次個人化論(individualisation)則認為在全球化與大眾傳播媒體的影響之下，階級之間的文化疆界已經毀壞，對於當今社會來說，品味是屬於個人的行為，已經不再與階級有關。另外雜食-純食論(omnivore–univore)則走在這兩種論點中間，認為在全球化與大眾傳播媒體的影響之下，上層階級的確開始也會消費一些通俗文化，但下層階級則仍然只消費通俗文化<a href="https://books.google.com.tw/books?hl=zh-TW&amp;lr=&amp;id=raPbbFuYOG4C&amp;oi=fnd&amp;pg=PA152&amp;ots=CZ6D_MnVVM&amp;sig=9E6FGFJBg6K50r7M_Bfe6yFEtH4&amp;redir_esc=y#v=onepage&amp;q&amp;f=false" target="_blank" rel="noopener">(Peterson and Simkus 1992)</a>。這時候區辨不同階級的方法就不再像過去只看單一的高雅/通俗文化指標，還會多考量涉略文化活動的廣度。</p><p>那麼這些國外的理論適不識合用來解釋台灣的社會呢？如果可以的話台灣社會又比較接近哪種觀點呢？今天我將透過中研院的<a href="http://www.ios.sinica.edu.tw/sc/cht/home.php" target="_blank" rel="noopener">台灣社會變遷調查</a>資料庫中的<a href="http://www.ios.sinica.edu.tw/sc/cht/download.php?fn=tscs071.pdf" target="_blank" rel="noopener">社會階層組的調查問卷(2007))</a>的資料進行分析!</p><h1 id="分析流程"><a href="#分析流程" class="headerlink" title="分析流程"></a>分析流程</h1><p>在原始資料中包含了相當豐富的資訊(2040份樣本，739個變項)，由於我們今天主要探討的是階級與品味間的關聯性，因此我僅從中挑選部分的題項。<br>如下表的說明，a1到f1是我使用的解釋變數，g3a到g3i是受訪者消費音樂頻率的變項，由於文化品味是抽象的概念，在此我們將透過這些這9個音樂品味的變項來創造出一個多分類的音樂品味類型，詳細操作方法可以參考統計分群的章節。最終我將運用下表中的解釋變數建置多分類的邏輯迴歸來解釋文化品味，找出階級與品味之間的關聯性。</p><style> table th:nth-of-type(1) { width: 50px; } </style><style> table th:nth-of-type(2) { width: 90px; } </style><table><thead><tr><th style="text-align:left">代碼</th><th style="text-align:left">變項名稱</th><th style="text-align:left">說明</th></tr></thead><tbody><tr><td style="text-align:left">a1</td><td style="text-align:left">性別</td><td style="text-align:left">分為男性和女性，以女性為對照組</td></tr><tr><td style="text-align:left">age</td><td style="text-align:left">年齡</td><td style="text-align:left">調查時受訪者的年齡</td></tr><tr><td style="text-align:left">c1</td><td style="text-align:left">教育程度</td><td style="text-align:left">分為國小、國中…碩士、博士等22個類別，轉換為對應的教育年數</td></tr><tr><td style="text-align:left">c8</td><td style="text-align:left">父親教育</td><td style="text-align:left">同c1題項的處理方式</td></tr><tr><td style="text-align:left">d2b3r</td><td style="text-align:left">父親職位</td><td style="text-align:left">15歲時父親的職業地位，按照黃毅志(2008)的研究，將每個職業類型轉換為對應的社經地位分數</td></tr><tr><td style="text-align:left">f1</td><td style="text-align:left">主觀地位</td><td style="text-align:left">請受訪者從1到10分中挑選符合自己社會地位的分數</td></tr><tr><td style="text-align:left">g3a</td><td style="text-align:left">台語流行歌</td><td style="text-align:left">詢問受訪者常不常聽台語流行歌，分為從不、很少、有時、經常等四類</td></tr><tr><td style="text-align:left">g3b</td><td style="text-align:left">國語流行歌</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3c</td><td style="text-align:left">日本流行歌</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3d</td><td style="text-align:left">西洋流行歌</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3e</td><td style="text-align:left">古典音樂</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3f</td><td style="text-align:left">國樂</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3g</td><td style="text-align:left">平劇</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3h</td><td style="text-align:left">歌仔戲</td><td style="text-align:left">同g3a題項的處理方式</td></tr><tr><td style="text-align:left">g3i</td><td style="text-align:left">布袋戲</td><td style="text-align:left">同g3a題項的處理方式</td></tr></tbody></table><h1 id="資料前置處理"><a href="#資料前置處理" class="headerlink" title="資料前置處理"></a>資料前置處理</h1><p>載入使用套件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">library(tidyverse)</span><br><span class="line">library(magrittr)</span><br><span class="line">library(foreign)</span><br><span class="line">library(PCAmixdata)</span><br><span class="line">library(nnet)</span><br><span class="line">library(DescTools)</span><br><span class="line">library(stargazer)</span><br><span class="line">library(fpc)</span><br></pre></td></tr></table></figure></p><p>按照上表的說明整理資料。由於資料處理的過程比較繁雜，在此僅列出程式碼提供參考，有興趣的讀者請自行檢視。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">tscs2007 &lt;- read.spss(file = &quot;https://www.ios.sinica.edu.tw/sc/cht/datafile/tscs071.sav&quot;, # 從中研院社會變遷調查資料庫下載資料</span><br><span class="line">                      use.value.labels = F, </span><br><span class="line">                      to.data.frame = T) %&gt;%</span><br><span class="line">  select(a1, age, c1, c8, d2b3_r, f1, g3a:g3i) %&gt;% # 選取性別、年齡、教育、父親教育、父親職業地位、主觀地位與音樂品味等9個變項</span><br><span class="line">  mutate(</span><br><span class="line">    a1 = case_when( # 將性別轉換為虛擬變項，女性為對照組</span><br><span class="line">      grepl(&quot;1&quot;, a1) ~ 1,</span><br><span class="line">      grepl(&quot;2&quot;, a1) ~ 0),</span><br><span class="line">    d2b3_r = case_when( # 將父親的職業類別轉換為對應的社經地位分數</span><br><span class="line">      grepl(&quot;110&quot;, d2b3_r) ~ 83.3,</span><br><span class="line">      grepl(&quot;120|130|140&quot;, d2b3_r) ~ 81.4,</span><br><span class="line">      grepl(&quot;201&quot;, d2b3_r) ~ 87.9,</span><br><span class="line">      grepl(&quot;202&quot;, d2b3_r) ~ 81.1,</span><br><span class="line">      grepl(&quot;211|221&quot;, d2b3_r) ~ 86.0,</span><br><span class="line">      grepl(&quot;212|213|214&quot;, d2b3_r) ~ 80.0,</span><br><span class="line">      grepl(&quot;222|223&quot;, d2b3_r) ~ 79.1,</span><br><span class="line">      grepl(&quot;230&quot;, d2b3_r) ~ 85.1,</span><br><span class="line">      grepl(&quot;250&quot;, d2b3_r) ~ 83.2,</span><br><span class="line">      grepl(&quot;301|302|303&quot;, d2b3_r) ~ 78.4,</span><br><span class="line">      grepl(&quot;311&quot;, d2b3_r) ~ 80.1,</span><br><span class="line">      grepl(&quot;312|314&quot;, d2b3_r) ~ 74.5,</span><br><span class="line">      grepl(&quot;313&quot;, d2b3_r) ~ 78.1,</span><br><span class="line">      grepl(&quot;321|322|340&quot;, d2b3_r) ~ 77.5,</span><br><span class="line">      grepl(&quot;331&quot;, d2b3_r) ~ 78.8,</span><br><span class="line">      grepl(&quot;332&quot;, d2b3_r) ~ 77.2,</span><br><span class="line">      grepl(&quot;350|360&quot;, d2b3_r) ~ 80.1,</span><br><span class="line">      grepl(&quot;370&quot;, d2b3_r) ~ 81.9,</span><br><span class="line">      grepl(&quot;410&quot;, d2b3_r) ~ 76.5,</span><br><span class="line">      grepl(&quot;420|511&quot;, d2b3_r) ~ 74.3,</span><br><span class="line">      grepl(&quot;431&quot;, d2b3_r) ~ 76.0,</span><br><span class="line">      grepl(&quot;432&quot;, d2b3_r) ~ 76.7,</span><br><span class="line">      grepl(&quot;512|514&quot;, d2b3_r) ~ 66.8,</span><br><span class="line">      grepl(&quot;513&quot;, d2b3_r) ~ 68.9,</span><br><span class="line">      grepl(&quot;515|516&quot;, d2b3_r) ~ 73.1,</span><br><span class="line">      grepl(&quot;520&quot;, d2b3_r) ~ 76.9,</span><br><span class="line">      grepl(&quot;531&quot;, d2b3_r) ~ 71.8,</span><br><span class="line">      grepl(&quot;532&quot;, d2b3_r) ~ 67.3,</span><br><span class="line">      grepl(&quot;610&quot;, d2b3_r) ~ 66.0,</span><br><span class="line">      grepl(&quot;620&quot;, d2b3_r) ~ 65.9,</span><br><span class="line">      grepl(&quot;710&quot;, d2b3_r) ~ 72.0,</span><br><span class="line">      grepl(&quot;720&quot;, d2b3_r) ~ 74.2,</span><br><span class="line">      grepl(&quot;790&quot;, d2b3_r) ~ 71.1,</span><br><span class="line">      grepl(&quot;810|840&quot;, d2b3_r) ~ 70.7,</span><br><span class="line">      grepl(&quot;820&quot;, d2b3_r) ~ 70.8,</span><br><span class="line">      grepl(&quot;830&quot;, d2b3_r) ~ 69.4,</span><br><span class="line">      grepl(&quot;910&quot;, d2b3_r) ~ 66.1,</span><br><span class="line">      grepl(&quot;920&quot;, d2b3_r) ~ 71.0,</span><br><span class="line">      grepl(&quot;930&quot;, d2b3_r) ~ 65.7,</span><br><span class="line">      grepl(&quot;940&quot;, d2b3_r) ~ 64.5,</span><br><span class="line">      grepl(&quot;950&quot;, d2b3_r) ~ 64.6,</span><br><span class="line">      grepl(&quot;960&quot;, d2b3_r) ~ 69.6),</span><br><span class="line">    f1 = case_when(between(f1, 1, 10) ~ f1)) %&gt;% # 主觀地位分數</span><br><span class="line">  mutate_at(vars(c1,c8), # 教育程度轉換為相對應的教育年數</span><br><span class="line">            funs(case_when(</span><br><span class="line">              . == 1 ~ 0, </span><br><span class="line">              . == 2 ~ 3,</span><br><span class="line">              . == 3 ~ 6,</span><br><span class="line">              between(., 4, 5) ~ 9 ,</span><br><span class="line">              between(., 6, 9) ~ 12 ,</span><br><span class="line">              between(., 10, 15) ~ 14 ,</span><br><span class="line">              between(., 16, 19) ~ 16 ,</span><br><span class="line">              . == 20 ~ 18,</span><br><span class="line">              . == 21 ~ 22)))%&gt;%</span><br><span class="line">  mutate_at(vars(g3a,g3b,g3c,g3d,g3e,g3f,g3g,g3h,g3i), # 文化品味題組，將「經常」重新編碼為4分，「從不」為1分</span><br><span class="line">            funs(case_when(</span><br><span class="line">              . == 1 ~ 4,</span><br><span class="line">              . == 2 ~ 3,</span><br><span class="line">              . == 3 ~ 2,</span><br><span class="line">              . == 4 ~ 1)))%&gt;%</span><br><span class="line">  na.omit(.) # 排除遺漏值</span><br></pre></td></tr></table></figure></p><h1 id="統計分群"><a href="#統計分群" class="headerlink" title="統計分群"></a>統計分群</h1><p>由於文化品味之間或多或少都會存在一些關聯性，並不適合直接進行k-medoids分群，在此我們會先透過主成分分析與因子轉軸<a href="需要留意的是，將順序尺度的資料當作等距尺度進行分析是存在統計上的疑慮的。實務上為了資料分析的方便性，我們仍然經常直接將這類資料進行當作等距資料進行加減、平均等等操作。若是追求精確、嚴謹的研究者，可以考慮運用[homals套件](https://www.jstatsoft.org/article/view/v031i04/v31i04.pdf)將順序尺度的資料轉換為等距尺度後，再進行後續的統計分析。">^1</a>來排除變項間相關性的問題，接著再將提取出的維度透過k-medoids進行統計分群。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">culture_PCA &lt;- PCAmix(X.quanti = select(tscs2007, g3a:g3i),</span><br><span class="line">                      graph = F) # 運用g3a到g3i等9項題目進行主成分分析</span><br><span class="line">culture_PCA_ROTATE &lt;- PCArot(culture_PCA,</span><br><span class="line">                             dim=sum(culture_PCA$eig[,1]&gt;=1),</span><br><span class="line">                             graph = F) # 提取特徵值大於1的維度並進行轉軸</span><br><span class="line">culture_PCA_ROTATE$sqload%&gt;%round(2)  # 檢視轉軸過後各維度的意義</span><br><span class="line">tscs2007 %&lt;&gt;% </span><br><span class="line">  bind_cols(culture_PCA_ROTATE$ind$coord %&gt;% </span><br><span class="line">              as_tibble(.)) # 將提取後的結果併回資料中</span><br></pre></td></tr></table></figure></p><style> table th:nth-of-type(1) { width: 50px; } </style><table><thead><tr><th style="text-align:left">Var</th><th style="text-align:center">dim1.rot</th><th style="text-align:center">dim2.rot</th><th style="text-align:center">dim3.rot</th></tr></thead><tbody><tr><td style="text-align:left">g3a</td><td style="text-align:center">0.06</td><td style="text-align:center"><strong>0.42</strong></td><td style="text-align:center">0.20</td></tr><tr><td style="text-align:left">g3b</td><td style="text-align:center">0.00</td><td style="text-align:center">0.02</td><td style="text-align:center"><strong>0.71</strong></td></tr><tr><td style="text-align:left">g3c</td><td style="text-align:center">0.07</td><td style="text-align:center">0.00</td><td style="text-align:center"><strong>0.41</strong></td></tr><tr><td style="text-align:left">g3d</td><td style="text-align:center">0.12</td><td style="text-align:center">0.04</td><td style="text-align:center"><strong>0.55</strong></td></tr><tr><td style="text-align:left">g3e</td><td style="text-align:center"><strong>0.44</strong></td><td style="text-align:center">0.01</td><td style="text-align:center">0.19</td></tr><tr><td style="text-align:left">g3f</td><td style="text-align:center"><strong>0.62</strong></td><td style="text-align:center">0.04</td><td style="text-align:center">0.02</td></tr><tr><td style="text-align:left">g3g</td><td style="text-align:center"><strong>0.51</strong></td><td style="text-align:center">0.06</td><td style="text-align:center">0.00</td></tr><tr><td style="text-align:left">g3h</td><td style="text-align:center">0.09</td><td style="text-align:center"><strong>0.57</strong></td><td style="text-align:center">0.03</td></tr><tr><td style="text-align:left">g3i</td><td style="text-align:center">0.04</td><td style="text-align:center"><strong>0.54</strong></td><td style="text-align:center">0.00</td></tr></tbody></table><p>在這裡我將因子載荷係數大於0.3的項目用粗體標示，如果文化品味在該維度上的係數值越高，表示該維度反映越多的該文化品味的意義。而上表的結果顯示，維度1主要反映的是古典音樂(g3e)、國樂(g3f)、平劇(g3g)等項目，我將其命名為消費高雅文化的程度。維度2主要反映的是台語流行歌(g3a)、歌仔戲(g3h)、布袋戲(g3i)等題目，我將其命名為消費傳統文化的程度。維度3主要反映的是國語流行歌(g3b)、日本流行歌(g3c)、西洋流行歌(g3d)等題目，我將其命名為消費流行文化的程度。</p><p>在排除了變數之間相關性的問題之後，我們接下來就運用k-medoids演算法對每位受訪者的文化品味進行分群。在此我使用的函數是fpc套件中的pamk函數，這個函數的優點是當我們在進行分群時，我們只需要輸入資料集與分群組數的範圍，系統就會幫我們把這個範圍內的分群組數都計算出來，並根據分成各群時的silhouette係數從中挑選最佳的分群結果。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">set.seed(123)</span><br><span class="line">pamk.best &lt;- pamk(select(tscs2007, ends_with(&quot;rot&quot;)), # 選擇提取出的三個維度進行分群</span><br><span class="line">                  krange=2:10) # 從區分成2到10群的結果中找出最佳分群組數</span><br><span class="line">tscs2007 %&lt;&gt;% mutate(cluster = pamk.best$pamobject$clustering) # 將分群結果併回資料集</span><br><span class="line"></span><br><span class="line">tscs2007 %&gt;% # 檢視分群結果</span><br><span class="line">  group_by(cluster)%&gt;%</span><br><span class="line">  select(ends_with(&quot;rot&quot;)) %&gt;%</span><br><span class="line">  summarise_all(funs(mean(.)))</span><br></pre></td></tr></table></figure></p><style> table th:nth-of-type(1) { width: 50px; } </style><table><thead><tr><th style="text-align:left">cluster</th><th style="text-align:center">dim1.rot</th><th style="text-align:center">dim2.rot</th><th style="text-align:center">dim3.rot</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:center">-0.17</td><td style="text-align:center">-1.01</td><td style="text-align:center">1.30</td></tr><tr><td style="text-align:left">2</td><td style="text-align:center">-1.09</td><td style="text-align:center">1.03</td><td style="text-align:center">0.083</td></tr><tr><td style="text-align:left">3</td><td style="text-align:center">2.17</td><td style="text-align:center">0.70</td><td style="text-align:center">0.24</td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">-0.26</td><td style="text-align:center">-0.85</td><td style="text-align:center">-1.74</td></tr></tbody></table><p>從以上的結果顯示，分群1只消費流行文化，但是在高雅文化與傳統文化的分數都是負值，我將這群人稱為「純流行」。分群2則偏好傳統文化與流行文化，(但流行文化的係數非常接近0)，並且不消費高雅文化，我將其稱為「純傳統」。分群三在每種文化品味上都有正面的偏好，其中最偏好高雅文化，另外在傳統、流行文化也會涉略，我將其稱為「雜食」。最後分群4則與分群3相反，對於每種音樂品味的偏好都是負值，我將其稱為「不聽音樂」。</p><h1 id="建置模型"><a href="#建置模型" class="headerlink" title="建置模型"></a>建置模型</h1><p>到目前為止我們已經從原始的9項音樂品味創造出一個「4分類的音樂品味」變項囉，這4分類分別是「純流行(popular)」、「純傳統(tradition)」、「雜食(omnivore)」與「不聽音樂(none)」的四個標籤。而這也就是我們後續分析的目標變數，接著我將運用受訪者的性別、年齡、教育年數、父親教育年數、主觀社會地位以及父親的階級來解釋文化品味。操作過程如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tscs2007 %&lt;&gt;%  </span><br><span class="line">  mutate_at(vars(a1, age, c1, c8, f1, d2b3_r),</span><br><span class="line">            funs(scale(.))) %&gt;%</span><br><span class="line">  mutate(cluster = factor(cluster,c(1, 2, 3, 4), c(&quot;popular&quot;,&quot;tradition&quot;,&quot;omnivore&quot;,&quot;none&quot;)),</span><br><span class="line">         cluster = relevel(cluster, ref = &quot;omnivore&quot;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mlogist &lt;- multinom(cluster ~ a1 + age + c8+ d2b3_r + c1 + f1 , # 運用a1、age、c8、d2b3_r、c1、f1等變數解釋文化品味</span><br><span class="line">                    data = tscs2007)</span><br><span class="line"></span><br><span class="line">PseudoR2(mlogist, c(&quot;McFaddenAdj&quot;,&quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;BIC&quot;, &quot;LogLik&quot;))</span><br><span class="line">stargazer(mlogist, type=&quot;text&quot;, out=&quot;multi1.htm&quot;,summary = T)</span><br></pre></td></tr></table></figure></p><p>輸出並整理以上的分析結果如下表</p><style> table th:nth-of-type(1) { width: 70px; } </style><table><thead><tr><th style="text-align:left"></th><th style="text-align:center">Popular<br>vs.<br>omnivore</th><th style="text-align:center">Tradition<br>vs.<br>omnivore</th><th style="text-align:center">none<br>vs.<br>omnivore</th></tr></thead><tbody><tr><td style="text-align:left">性別</td><td style="text-align:center">0.039<br>(0.075)</td><td style="text-align:center">0.286***<br>(0.075)</td><td style="text-align:center">0.136*<br>(0.082)</td></tr><tr><td style="text-align:left">年齡</td><td style="text-align:center">-0.936***<br>(0.110)</td><td style="text-align:center">-0.445***<br>(0.103)</td><td style="text-align:center">0.072<br>(0.109)</td></tr><tr><td style="text-align:left">父親教育</td><td style="text-align:center">-0.172*<br>(0.104)</td><td style="text-align:center">-0.407***<br>(0.104)</td><td style="text-align:center">-0.471***<br>(0.113)</td></tr><tr><td style="text-align:left">父親職位</td><td style="text-align:center">0.118**<br>(0.088)</td><td style="text-align:center">0.002<br>(0.091)</td><td style="text-align:center">0.043<br>(0.100)</td></tr><tr><td style="text-align:left">教育年數</td><td style="text-align:center">0.075<br>(0.136)</td><td style="text-align:center">-1.085***<br>(0.125)</td><td style="text-align:center">-0.918***<br>(0.132)</td></tr><tr><td style="text-align:left">主觀地位</td><td style="text-align:center">0.002<br>(0.084)</td><td style="text-align:center">-0.115<br>(0.079)</td><td style="text-align:center">-0.321***<br>(0.086)</td></tr><tr><td style="text-align:left">常數</td><td style="text-align:center">-0.179*<br>(0.100)</td><td style="text-align:center">0.491***<br>(0.078)</td><td style="text-align:center">0.042<br>(0.088)</td></tr><tr><td style="text-align:left">PseudoR2</td><td style="text-align:center">0.150(McFaddenAdj)<br>0.353(CoxSnell)<br>0.377(Nagelkerke)</td><td style="text-align:center"></td></tr></tbody></table><p>如上表，整體模型的PseudoR^2值為0.150(McFaddenAdj)、0.353(CoxSnell)與0.377(Nagelkerke)。首先在Popular vs. omnivore 的模型中，年齡與父親教育年數越高者容易成為雜食，而父親職業地位的增加則較容易成為純流行。其次在Tradition vs. omnivore的模型來看，男性較容易成為純傳統，而年齡、父親教育年數與教育年數的增加則較容易成為雜食。最後在none vs. omnivore的模型來看，男性較容易成為不聽音樂，而父親教育年數、教育年數與主觀地位的增加則較容易成為雜食。</p><p>整體來看，從文化品味的分群結果可以發現，文化品味並非如同源論的論述(各階級僅偏好屬於自己階級的文化)也非個人化論的論述(文化品味完全是屬於個人的偏好)，實際上台灣的文化品味較接近走在這兩個論點中間的雜食論，固然有一群消費者僅消費流行文化或傳統文化，然而也有消費各式各樣文化品味的群體以及都不消費任何文化活動的群體。並且我們從多項邏輯回歸中也發現在相關文獻中提到的個人特質(性別、年齡)、家庭背景(父親教育、父親地位)與代表個人的階級地位(教育年數、主觀地位)等都是影響文化品味的重要變數。</p><p>另外一項有趣的發現是同樣都是代表家庭背景的父親教育與父親職位有著截然不同的發現，如果研究者使用父親教育作為家庭背景的操作化指標，會發現父親教育越高越容易成為雜食者；然而若是使用父親職位作為操作化指標，則會發現父親職業地位越高受訪者較容易成為純流行。這會隨著受訪者採用不同的指標來測量家庭背景的概念會有不同的研究發現。</p><h1 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h1><p>這篇文章是從我的碩士論文修改而來，主要演示了一次運用PCA與PAM分群的方法提取潛在結構，將「多個文化品味的消費頻率」題項簡化為一個多分類的文化品味標籤。並將其作為目標變數，運用性別、年齡、教育程度、父親教育、父親職位等變項來建立多項邏輯回歸的模型的方法。<br>透過上述分析我們可以發現許多有意思的發現，如文化品味可以如何分群，以及文化品味與階級之間的關聯性又是如何。另外在原始資料中，除了測量音樂品味消費頻率的題項之外還有休閒品味、閱讀品味的消費頻率題項。有興趣的讀者可以修改上述的代碼，再進行一次以上分析，相信還會有許多有意思的發現！</p><h1 id="資料來源"><a href="#資料來源" class="headerlink" title="資料來源"></a>資料來源</h1><p> <a href="http://www.ios.sinica.edu.tw/sc/cht/download.php?fn=tscs07.pdf" target="_blank" rel="noopener">張苙雲、廖培珊（2008）。臺灣社會變遷基本調查計畫第五期第三次調查計畫執行報告書。臺北市：中央研究院社會學研究所。</a><br> <a href="https://cran.r-project.org/web/packages/factoextra/factoextra.pdf" target="_blank" rel="noopener">Kassambara, A., &amp; Mundt, F. (2016). Factoextra: extract and visualize the results of multivariate data analyses. R package version, 1(3).</a><br> <a href="https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf" target="_blank" rel="noopener">Wickham, H. (2017). Tidyverse: Easily install and load’tidyverse’packages. R package version, 1(1).</a><br> <a href="https://cran.r-project.org/web/packages/magrittr/magrittr.pdf" target="_blank" rel="noopener">Bache, S. M., &amp; Wickham, H. (2014). magrittr: a forward-pipe operator for R. R package version, 1(1).</a><br> <a href="https://cran.r-project.org/web/packages/NbClust/NbClust.pdf" target="_blank" rel="noopener">Charrad, M., Ghazzali, N., Boiteau, V., Niknafs, A., &amp; Charrad, M. M. (2014). Package ‘NbClust’. Journal of Statistical Software, 61, 1-36.</a><br> <a href="https://cran.r-project.org/web/packages/FactoMineR/FactoMineR.pdf" target="_blank" rel="noopener">Lê, S., Josse, J., &amp; Husson, F. (2008). FactoMineR: an R package for multivariate analysis. Journal of statistical software, 25(1), 1-18.</a><br> <a href="https://drive.google.com/file/d/0B60EW3tN3P3GZTloeGhKbFNRRzZRUWkxQlpvWVd5QQ/view" target="_blank" rel="noopener">黃毅志(2008)。如何精確測量職業地位？”改良版台灣地區新職業聲望與社經地位量表”之建構. 臺東大學教育學報, 19(1), 151-159.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;從前歐洲的封建社會有個很在意階級身分的貴族，有一天突然發現他竟然和一位地位最低下的僕人一樣－－都是用兩隻腳在走路！為此這位貴族煩惱了好一陣子，擔心自己走在路上時會不會也被旁人誤會自己是位僕人。這可是攸關面子的大問題呢，怎麼能容忍這樣的事情發生呢！於是貴族開始想方設法要用各種方式展現出自己的生活品味有多麼不同，首先在出門時交通方式，他雇用了一個乘騎大象的司機來接送自己出門之外，還要求在大象背上得有一個遮陽亭子，此外亭子的周圍還得有七種顏色亮麗的旗子裝飾。&lt;/p&gt;
&lt;p&gt;品味是一種上層階級用來區隔自己有多麼不同的一種方式，除了上述外顯的交通工具之外，更有意思的是在平常生活中，看似不起眼的穿著、飲食、閱讀、聽音樂的類型…等等這些隱性的文化品味，這些品味會在日常生活中的舉手投足之間不經意地展現出來，反映了每個人背後的家庭背景、教育程度或知識類型。更可怕的是「品味」這件事情除了會藉由複製的方式傳遞在階級之間傳遞，也還會反過來成為一種階級再製的工具。&lt;/p&gt;
    
    </summary>
    
    
      <category term="R" scheme="https://TLYu0419.github.io/tags/R/"/>
    
      <category term="k-medoids cluster" scheme="https://TLYu0419.github.io/tags/k-medoids-cluster/"/>
    
      <category term="multinomial logistic regression" scheme="https://TLYu0419.github.io/tags/multinomial-logistic-regression/"/>
    
  </entry>
  
  <entry>
    <title>高薪就能換來忠誠？預測模型的選擇與評估</title>
    <link href="https://TLYu0419.github.io/2018/09/01/%E5%A6%82%E4%BD%95%E8%A1%A1%E9%87%8F%E9%A0%90%E6%B8%AC%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%93%81%E8%B3%AA/"/>
    <id>https://TLYu0419.github.io/2018/09/01/如何衡量預測模型的品質/</id>
    <published>2018-09-01T03:39:02.000Z</published>
    <updated>2018-09-09T16:24:24.387Z</updated>
    
    <content type="html"><![CDATA[<p>前些日子有幸參與一場總經理與公司高級長官們的會議，長官們討論著是什麼原因讓公司在近年來取得了如此的成功？<br>長官們紛紛表達成功的關鍵因素是行銷策略、通路經營、客戶服務、基站建設、還有帳務管理…等等都有長官提到。<br>最後總經理卻總結道：「如果是最關鍵的因素，那絕對是人力資源的管理…」</p><p>沒錯!我們今天要運用統計分析方法解的就是跟人力資源管理有關的議題!人力資源在勞動市場上的流動是相當常見的現象，但我們並不希望績效良好的離開公司，其中如果是傑出員工的離開，對公司來說更是莫大的損失。因此，我們能不能建置一個精準的預測模型，事先掌握公司每位員工的離職率呢？</p><h1 id=""><a href="#" class="headerlink" title=""></a><a id="more"></a></h1><h1 id="背景知識"><a href="#背景知識" class="headerlink" title="背景知識"></a>背景知識</h1><p>由於模型的預測結果與實際狀況或多或少都會存在不一致的狀況，因此我們可以將這些一致與不一致的狀況繪製成如下表格。其中「預測與實際皆為正」稱作TP，「預測與實際皆為負」稱為TN，「預測為正實際為負」稱為FP，而「預測為負，實際為正」則稱作FN。而如何評價統計模型的品質則是由這四種狀況衍生出相當多的指標，其中我們最常使用的三項指標分別是Accurcy、Precision和Recall rate。</p><p><img src="/2018/09/01/如何衡量預測模型的品質/ConfusionMatrix.PNG" alt="ConfusionMatrix"><br>Accurcy是檢視正確預測在這四種事件中的佔比，計算公式為：(TP+TN)/(TP+TN+FP+FN)。<br>Precision關注的是被我們預測為正類的樣本中，實際為正的客戶有多少，計算公式為：TP/(TP+FP)。<br>Recall 在乎的則是實際為正類的樣本中，有多少正類的樣本能被預測出來，計算公式為：TP/(TP+FN)。</p><p>然而隨著不同的專案需求，我們並不會總是以0.5作為判定樣本為正類/負類的標準(閥值)。如果FP所付出的代價相當高，我們會將閥值提升；而如果FN的成本很高，我們則會將閥值降低。簡單來說，前者的思考邏輯是「寧可放過，不可錯殺」，而後者則是「寧可錯殺，不可放過」。至於應該如何設定這個閥值，這會隨著不同專案，在經過評估兩種犯錯情境付出的成本後，才能決定我們能接受的結果為何。</p><p>接著如果我們把每個閥值的TP、FP結果分別記錄在X與Y軸上，我們將能夠繪製出評估整體模型品質的ROC曲線，而ROC曲線下的面積被稱為AUC值。AUC是經常用來評估整體模型品質的指標，其數值介於0至1。當AUC值為0.5時，表示模型的預測結果相當於隨機猜測的結果（沒有價值），AUC越接近1，表示模型的品質越接近完美的模型，而越接近0則表示比隨機猜測的結果更糟。</p><p>如果你對於以上幾種評估模型的指標想要有更進一步的了解的話，在這裡提供兩個維基百科上的補充資料<a href="https://en.wikipedia.org/wiki/Precision_and_recall#F-measure" target="_blank" rel="noopener">Precision and recall</a>與<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">Receiver operating characteristic</a>。</p><h1 id="資料說明"><a href="#資料說明" class="headerlink" title="資料說明"></a>資料說明</h1><p>我們今天要運用<a href="https://www.kaggle.com/" target="_blank" rel="noopener">kaggle</a>上<a href="https://www.kaggle.com/c/sm" target="_blank" rel="noopener">Human Resources Analytics</a>的資料，在資料中包含了相當豐富的員工資料，我們將運用薪資、工作滿意度、完成幾項專案、近5年是否獲得升遷、服務部門…等等資訊，來預測員工的離職行為。<br>首先我會運用邏輯回歸與隨機森林兩種演算法建置兩個預測模型。並按照kaggle指定評價模型的Accuracy指標，選擇較佳的模型。接著我會檢視各個因子解釋離職行為的效果，找出影響員工離職的關鍵因素。<br>在建模的過程中，我們將運用邏輯回歸與隨機森林兩種演算法分別建模，並且進一步比較模型在測試資料中的預測能力。</p><h1 id="安裝與載入套件"><a href="#安裝與載入套件" class="headerlink" title="安裝與載入套件"></a>安裝與載入套件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 此部分僅第一次安裝套件的同學執行即可</span><br><span class="line">install.packages(&quot;breakDown&quot;)</span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">install.packages(&quot;fastDummies&quot;)</span><br><span class="line">install.packages(&quot;magrittr&quot;)</span><br><span class="line"># 載入套件</span><br><span class="line">library(breakDown)</span><br><span class="line">library(tidyverse)</span><br><span class="line">library(caret)</span><br><span class="line">library(fastDummies)</span><br><span class="line">library(magrittr)</span><br></pre></td></tr></table></figure><h1 id="讀取並初步清理資料"><a href="#讀取並初步清理資料" class="headerlink" title="讀取並初步清理資料"></a>讀取並初步清理資料</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data(HR_data)</span><br><span class="line">str(HR_data) # 顯示我們的資料集當中有哪些資料</span><br></pre></td></tr></table></figure><p>按照輸出的結果顯示，這份資料中共有14,999筆資料，並包含了10個變項。詳細的欄位說明可以到<a href="https://www.kaggle.com/c/sm" target="_blank" rel="noopener">Human Resources Analytics</a>上檢視。這10個變項由上到下分別代表的是工作滿意度、最後一次考核至今的時間(年)、完成專案數量、平均每月工時、在公司服務年數、是否發生過工作事故、員工是否離職、五年內是否有升遷、工作部門、薪資水準。其中工作部門與薪資水準都屬於類別尺度的資料。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&apos;data.frame&apos;:14999 obs. of  10 variables:</span><br><span class="line"> $ satisfaction_level   : num  0.38 0.8 0.11 0.72 0.37 0.41 0.1 0.92 0.89 0.42 ...</span><br><span class="line"> $ last_evaluation      : num  0.53 0.86 0.88 0.87 0.52 0.5 0.77 0.85 1 0.53 ...</span><br><span class="line"> $ number_project       : int  2 5 7 5 2 2 6 5 5 2 ...</span><br><span class="line"> $ average_montly_hours : int  157 262 272 223 159 153 247 259 224 142 ...</span><br><span class="line"> $ time_spend_company   : int  3 6 4 5 3 3 4 5 5 3 ...</span><br><span class="line"> $ Work_accident        : int  0 0 0 0 0 0 0 0 0 0 ...</span><br><span class="line"> $ left                 : int  1 1 1 1 1 1 1 1 1 1 ...</span><br><span class="line"> $ promotion_last_5years: int  0 0 0 0 0 0 0 0 0 0 ...</span><br><span class="line"> $ sales                : Factor w/ 10 levels &quot;accounting&quot;,&quot;hr&quot;,..: 8 8 8 8 8 8 8 8 8 8 ...</span><br><span class="line"> $ salary               : Factor w/ 3 levels &quot;high&quot;,&quot;low&quot;,&quot;medium&quot;: 2 3 3 2 2 2 2 2 2 2 ...</span><br></pre></td></tr></table></figure></p><p>為了讓後續的分析更方便與穩定，在這裡我簡單對資料進行整理。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">HR_data %&lt;&gt;%</span><br><span class="line">  # 將類別資料轉換為虛擬變項的資料</span><br><span class="line">  dummy_cols(select_columns = c(&quot;sales&quot;,&quot;salary&quot;), remove_most_frequent_dummy = T)%&gt;% </span><br><span class="line">  select(-sales, -salary)%&gt;%</span><br><span class="line">  # 將目標變數重新編碼為Y跟N</span><br><span class="line">  mutate(left = factor(left, c(0,1), c(&quot;N&quot;,&quot;Y&quot;))) %&gt;%</span><br><span class="line">  na.omit()</span><br></pre></td></tr></table></figure></p><p>將資料按照70/30的比例切割為訓練與測試資料，我們將運用訓練資料建置模型，並將模型套用在測試資料來檢視預測模型的解釋效果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># split data to train_set and test_set</span><br><span class="line">set.seed(123)</span><br><span class="line">c &lt;- createDataPartition(y = HR_data$left, p = 0.7,list = F) </span><br><span class="line">train_set &lt;- HR_data[c,]</span><br><span class="line">test_set &lt;- HR_data[-c,]</span><br></pre></td></tr></table></figure></p><h1 id="Model-training"><a href="#Model-training" class="headerlink" title="Model training"></a>Model training</h1><p>在這裡我將運用邏輯迴歸與隨機森林兩種演算法來建置模型，並從中挑選出解釋效果較佳的模型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 邏輯迴歸</span><br><span class="line">model_glm &lt;- train(left ~ .,</span><br><span class="line">                   data = train_set,</span><br><span class="line">                   method = &quot;glmStepAIC&quot;,</span><br><span class="line">                   metric = &quot;ROC&quot;,</span><br><span class="line">                   preProc = c(&quot;zv&quot;,&quot;nzv&quot;,&quot;center&quot;, &quot;scale&quot;),</span><br><span class="line">                   trControl = trainControl(method = &quot;repeatedcv&quot;,</span><br><span class="line">                                            classProbs = T,</span><br><span class="line">                                            summaryFunction = twoClassSummary,</span><br><span class="line">                                            verboseIter = TRUE),</span><br><span class="line">                   family = binomial())</span><br><span class="line"># 隨機森林</span><br><span class="line">model_rf &lt;- train(left ~ .,</span><br><span class="line">                  data = train_set,</span><br><span class="line">                  method = &quot;rf&quot;,</span><br><span class="line">                  metric = &quot;ROC&quot;,</span><br><span class="line">                  preProc = c(&quot;zv&quot;,&quot;nzv&quot;,&quot;center&quot;, &quot;scale&quot;),</span><br><span class="line">                  trControl = trainControl(method = &quot;repeatedcv&quot;,</span><br><span class="line">                                           classProbs = T,</span><br><span class="line">                                           summaryFunction = twoClassSummary,</span><br><span class="line">                                           verboseIter = TRUE),</span><br><span class="line">                  tuneGrid = expand.grid(mtry = floor(sqrt(ncol(train_set)))))</span><br></pre></td></tr></table></figure></p><h1 id="Model-selection"><a href="#Model-selection" class="headerlink" title="Model selection"></a>Model selection</h1><p>這裡是本篇文章最核心的內容，在我正式開始說明之前，我先分享個小故事幫助大家了解狀況</p><p>以前還在讀書時，我每天出門的第一件事情就是看一下天氣，猜測今天會不會下雨。如果會下雨，我就得多帶把傘，反之則不帶傘。<br>平常我的「感覺」還滿準確的，當我正確預測天氣的時候，有需要用傘我才會帶，不需要的話就不帶傘(減少包包的重量)。<br>但是偶爾我的「感覺」也會有預測錯誤的情況，如果我預測會下雨，但實際上沒有下雨的話，我就平白無故增加了包包的重量了；<br>接著另一種預測錯誤的情境則需要付出較高的代價，因為我預測不會下雨，但實際上卻下雨了，我不只有可能全身淋濕，甚至還有可能造成書籍、手機、筆記型電腦的毀損…</p><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p>那麼我們到底要怎麼進行模型的評估呢？最簡單的方式是檢視Accuracy，也就是用猜對的次數/猜測的總次數。那麼我們就來看一下這份資料在兩種算法下的表現如何吧!<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusionMatrix(table(predict(model_glm,test_set), test_set$left))</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">       N    Y</span><br><span class="line">  N 3144  677</span><br><span class="line">  Y  284  394</span><br><span class="line">                                          </span><br><span class="line">               Accuracy : 0.7864          </span><br><span class="line">                 95% CI : (0.7741, 0.7983)</span><br><span class="line">    No Information Rate : 0.7619          </span><br><span class="line">    P-Value [Acc &gt; NIR] : 5.279e-05                          </span><br><span class="line">                  Kappa : 0.3262          </span><br><span class="line"> Mcnemar&apos;s Test P-Value : &lt; 2.2e-16                               </span><br><span class="line">            Sensitivity : 0.9172          </span><br><span class="line">            Specificity : 0.3679          </span><br><span class="line">         Pos Pred Value : 0.8228          </span><br><span class="line">         Neg Pred Value : 0.5811          </span><br><span class="line">             Prevalence : 0.7619          </span><br><span class="line">         Detection Rate : 0.6988          </span><br><span class="line">   Detection Prevalence : 0.8493          </span><br><span class="line">      Balanced Accuracy : 0.6425          </span><br><span class="line">                                          </span><br><span class="line">       &apos;Positive&apos; Class : N</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusionMatrix(table(predict(model_rf,test_set), test_set$left))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Confusion Matrix and Statistics</span><br><span class="line"></span><br><span class="line">       N    Y</span><br><span class="line">  N 3418   31</span><br><span class="line">  Y   10 1040</span><br><span class="line">                                          </span><br><span class="line">               Accuracy : 0.9909          </span><br><span class="line">                 95% CI : (0.9877, 0.9935)</span><br><span class="line">    No Information Rate : 0.7619          </span><br><span class="line">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16                            </span><br><span class="line">                  Kappa : 0.9747          </span><br><span class="line"> Mcnemar&apos;s Test P-Value : 0.001787                                </span><br><span class="line">            Sensitivity : 0.9971          </span><br><span class="line">            Specificity : 0.9711          </span><br><span class="line">         Pos Pred Value : 0.9910          </span><br><span class="line">         Neg Pred Value : 0.9905          </span><br><span class="line">             Prevalence : 0.7619          </span><br><span class="line">         Detection Rate : 0.7597          </span><br><span class="line">   Detection Prevalence : 0.7666          </span><br><span class="line">      Balanced Accuracy : 0.9841          </span><br><span class="line">                                          </span><br><span class="line">       &apos;Positive&apos; Class : N</span><br></pre></td></tr></table></figure><p>從以上兩個輸出結果顯示，邏輯回歸的Accuracy值是0.7864；而隨機森林的Accuracy則高達0.9909<br>但是<br>因此從Accuracy的角度來看隨機森林的表現趨近於</p><p>如果我們有個100%正確的模型，我們就可以完全避免掉錯誤預測帶來的損失，然而我們並不是上帝，沒辦法讓每次的預測都有100%的準確度。<br>但是我們卻可以用一些方法來提升模型的準確度，例如尋找更多解釋變數來解釋目標變數、嘗試運用不同的演算法……等等。<br>然而即使如此，我們能做到的也只是提升模型的準確度而已，實際上並不存在100%正確的預測模型。<br>因此，我們就得進一步精算兩種錯誤預測會帶來的損失，藉以決定我們應該接受多少的錯誤損失。</p><p>由於在原始競賽中指定我們是以整體正確率作為評量標準，因此我們…<br>什麼時候會在意Preceion多一些?什麼時候會在意recall?</p><h1 id="當我們運用訓練集資料完成統計建模之後呢，我們接著就會將模型套用在未參與建模的測試資料上，並檢視模型在測是資料中的預測結果與實際結果是否一致，藉以評估模型的準確度。"><a href="#當我們運用訓練集資料完成統計建模之後呢，我們接著就會將模型套用在未參與建模的測試資料上，並檢視模型在測是資料中的預測結果與實際結果是否一致，藉以評估模型的準確度。" class="headerlink" title="當我們運用訓練集資料完成統計建模之後呢，我們接著就會將模型套用在未參與建模的測試資料上，並檢視模型在測是資料中的預測結果與實際結果是否一致，藉以評估模型的準確度。"></a>當我們運用訓練集資料完成統計建模之後呢，我們接著就會將模型套用在未參與建模的測試資料上，並檢視模型在測是資料中的預測結果與實際結果是否一致，藉以評估模型的準確度。</h1><h1 id="因為我們運用了2種不同的算法進行統計建模，為了要比較兩種模型的準確度，在這裡我們會透過混淆矩陣的概念來幫助我們評估與選擇"><a href="#因為我們運用了2種不同的算法進行統計建模，為了要比較兩種模型的準確度，在這裡我們會透過混淆矩陣的概念來幫助我們評估與選擇" class="headerlink" title="因為我們運用了2種不同的算法進行統計建模，為了要比較兩種模型的準確度，在這裡我們會透過混淆矩陣的概念來幫助我們評估與選擇"></a>因為我們運用了2種不同的算法進行統計建模，為了要比較兩種模型的準確度，在這裡我們會透過<a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener">混淆矩陣</a>的概念來幫助我們評估與選擇</h1><p># </p><h1 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h1><h1 id="看氣象報導，提升預測的準確度"><a href="#看氣象報導，提升預測的準確度" class="headerlink" title="看氣象報導，提升預測的準確度"></a>看氣象報導，提升預測的準確度</h1><h1 id="因此我們在運用統計模型進行決策時，我們除了要不斷提升模型的準確度之外，還得進一步考量兩種犯錯的成本。"><a href="#因此我們在運用統計模型進行決策時，我們除了要不斷提升模型的準確度之外，還得進一步考量兩種犯錯的成本。" class="headerlink" title="因此我們在運用統計模型進行決策時，我們除了要不斷提升模型的準確度之外，還得進一步考量兩種犯錯的成本。"></a>因此我們在運用統計模型進行決策時，我們除了要不斷提升模型的準確度之外，還得進一步考量兩種犯錯的成本。</h1><h1 id="有時候我們會其中一種的犯錯成本非常小，因此我們會寧願犯這種錯誤，也不願意有犯第二種錯誤的情況發生。就向我寧可把雨具放在包包中，每天上班都帶著出門，也不願意有任何被雨淋濕的狀況出現"><a href="#有時候我們會其中一種的犯錯成本非常小，因此我們會寧願犯這種錯誤，也不願意有犯第二種錯誤的情況發生。就向我寧可把雨具放在包包中，每天上班都帶著出門，也不願意有任何被雨淋濕的狀況出現" class="headerlink" title="有時候我們會其中一種的犯錯成本非常小，因此我們會寧願犯這種錯誤，也不願意有犯第二種錯誤的情況發生。就向我寧可把雨具放在包包中，每天上班都帶著出門，也不願意有任何被雨淋濕的狀況出現"></a>有時候我們會其中一種的犯錯成本非常小，因此我們會寧願犯這種錯誤，也不願意有犯第二種錯誤的情況發生。就向我寧可把雨具放在包包中，每天上班都帶著出門，也不願意有任何被雨淋濕的狀況出現</h1><h1 id="但是如果我們今天是在"><a href="#但是如果我們今天是在" class="headerlink" title="但是如果我們今天是在"></a>但是如果我們今天是在</h1><h1 id="在這裡我借用了wiki上說明以上正確-錯誤預測的說明表格"><a href="#在這裡我借用了wiki上說明以上正確-錯誤預測的說明表格" class="headerlink" title="在這裡我借用了wiki上說明以上正確/錯誤預測的說明表格"></a>在這裡我借用了wiki上說明以上正確/錯誤預測的說明表格</h1><h1 id="用來評價模型的統計指標有相當多，在這裡我們先以競賽網頁中指定的指標選出最佳模型後，接著再跟讀者說明其他指標的意義"><a href="#用來評價模型的統計指標有相當多，在這裡我們先以競賽網頁中指定的指標選出最佳模型後，接著再跟讀者說明其他指標的意義" class="headerlink" title="用來評價模型的統計指標有相當多，在這裡我們先以競賽網頁中指定的指標選出最佳模型後，接著再跟讀者說明其他指標的意義"></a>用來評價模型的統計指標有相當多，在這裡我們先以競賽網頁中指定的指標選出最佳模型後，接著再跟讀者說明其他指標的意義</h1><h1 id="在這裡我們將模型放入一開始未參與建模的資料中，檢視模型的預測效果，並從中選取最佳模型"><a href="#在這裡我們將模型放入一開始未參與建模的資料中，檢視模型的預測效果，並從中選取最佳模型" class="headerlink" title="在這裡我們將模型放入一開始未參與建模的資料中，檢視模型的預測效果，並從中選取最佳模型"></a>在這裡我們將模型放入一開始未參與建模的資料中，檢視模型的預測效果，並從中選取最佳模型</h1><h1 id="從以上混淆矩陣的輸出結果我們可以看到在這個案例中是以隨機森林的ACC最高，其次是xgbtree，這兩者差不多，而邏輯回歸則較低"><a href="#從以上混淆矩陣的輸出結果我們可以看到在這個案例中是以隨機森林的ACC最高，其次是xgbtree，這兩者差不多，而邏輯回歸則較低" class="headerlink" title="從以上混淆矩陣的輸出結果我們可以看到在這個案例中是以隨機森林的ACC最高，其次是xgbtree，這兩者差不多，而邏輯回歸則較低"></a>從以上混淆矩陣的輸出結果我們可以看到在這個案例中是以隨機森林的ACC最高，其次是xgbtree，這兩者差不多，而邏輯回歸則較低</h1><h1 id="那麼在這裡輸出的其他指標又是什麼意義呢"><a href="#那麼在這裡輸出的其他指標又是什麼意義呢" class="headerlink" title="那麼在這裡輸出的其他指標又是什麼意義呢?"></a>那麼在這裡輸出的其他指標又是什麼意義呢?</h1><h1 id="在正式說明之前，我先來跟大家用個生活中的小例子"><a href="#在正式說明之前，我先來跟大家用個生活中的小例子" class="headerlink" title="在正式說明之前，我先來跟大家用個生活中的小例子"></a>在正式說明之前，我先來跟大家用個生活中的小例子</h1><h1 id="結論與討論"><a href="#結論與討論" class="headerlink" title="結論與討論"></a>結論與討論</h1><p>模型的整體準確度、召回率等等</p><p>薪資不是最重要的因素，而是滿意度<br>那麼什麼又是影響滿意度最重要的原因呢?<br>有興趣的同學可以自行替換上述的參數，並將滿意度設定為目標變數<br>帶傘/不帶傘付出的成本相當小，但是工作上預測錯誤成本則會相當大。錯誤的預測會導致…。而我們也不可能把每個人都當作會離開，這會導致維繫的成本大幅提升</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前些日子有幸參與一場總經理與公司高級長官們的會議，長官們討論著是什麼原因讓公司在近年來取得了如此的成功？&lt;br&gt;長官們紛紛表達成功的關鍵因素是行銷策略、通路經營、客戶服務、基站建設、還有帳務管理…等等都有長官提到。&lt;br&gt;最後總經理卻總結道：「如果是最關鍵的因素，那絕對是人力資源的管理…」&lt;/p&gt;
&lt;p&gt;沒錯!我們今天要運用統計分析方法解的就是跟人力資源管理有關的議題!人力資源在勞動市場上的流動是相當常見的現象，但我們並不希望績效良好的離開公司，其中如果是傑出員工的離開，對公司來說更是莫大的損失。因此，我們能不能建置一個精準的預測模型，事先掌握公司每位員工的離職率呢？&lt;/p&gt;
&lt;h1 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot;&quot;&gt;&lt;/a&gt;&lt;/h1&gt;
    
    </summary>
    
    
      <category term="混淆矩陣(Confusion matrix)" scheme="https://TLYu0419.github.io/tags/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%99%A3-Confusion-matrix/"/>
    
      <category term="model selection" scheme="https://TLYu0419.github.io/tags/model-selection/"/>
    
      <category term="R語言" scheme="https://TLYu0419.github.io/tags/R%E8%AA%9E%E8%A8%80/"/>
    
  </entry>
  
</feed>
