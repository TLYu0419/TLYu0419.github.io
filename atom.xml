<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>興趣沒有目的地</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://TLYu0419.github.io/"/>
  <updated>2019-12-26T15:41:58.764Z</updated>
  <id>https://TLYu0419.github.io/</id>
  
  <author>
    <name>Tony</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>如何爬取 GooglePlay 的APP留言與評分</title>
    <link href="https://TLYu0419.github.io/2019/12/26/Crawl-GooglePlay/"/>
    <id>https://TLYu0419.github.io/2019/12/26/Crawl-GooglePlay/</id>
    <published>2019-12-26T10:15:22.000Z</published>
    <updated>2019-12-26T15:41:58.764Z</updated>
    
    <content type="html"><![CDATA[<p>隨著行動裝置越來越普及，各家公司紛紛推出了自己的手機 APP 來降低跟客戶間的距離，如果我們想要了解客戶對於 APP 的想法，我們當然可以透過發放問卷的方式來收集客戶想法，但這種方式的缺點是會需要花費較長的時間。</p><p>另一種方式是我們可以直接到 Google Play 的 APP 頁面抓取客戶的留言，如果再搭配一些文字分析的技巧，我們就能更快速、即時的了解 APP 的優缺點。</p><p>在這篇文章我將示範怎麼透過 Selenium 幫我們自動抓取Google Play 上 Foodpanda 的客戶評分與留言等資料</p><a id="more"></a><h1>最終成果</h1><p>老樣子我們先看最後的成果，我們最終會將左邊的客戶留言轉成格式化的 excel 表格<br><img src="https://lh3.googleusercontent.com/Qx862wPuasoREv2KBhW77KXhaNphIO3q0jgOaKfOyNNZwRbhaqF9PodY_biXwg4C8aQU1C6P3_hdJ8EbulOM50IUWlc2o743L2UTaenjmwFnZ3WKnkXTatim9LZTMkaQ42njQ8yuMqvPnxI-gtpbMl2pnFsfEOWB_Lckg7XdZeGQi8x_SADH7HShZQE_C6GsHC8J6su1zeaePL1mfAqvPnTOaKPv2DZRYO-AcRFOhtdRKadaInOGj1_s4wY2qPnwW4D433R8Ec0LU-xgHaCNcvBS-5gsgzODlIVhhg9gsY1Ziy6vybtEO0dIXWEAltizfPG0BSX8SDaQhlYjPjDz6lFb1EpNaqAXcZjm9yg69ZFg1stpZcCq0MF3HnOnlpgTp7aPaLRVJA6u7ZJ8Kz3bzJdsfkLeycWaFPxCtVcjUgSISgshA0THJnI1whko7K9BCnLjrxj9sirqT_waymA-ZIXu-4Zm4yCuBMeNKc5Tkbsdsda6xHEGs13gWliLSEJx_Uz18_3COMV73H9o9LAlP0KgE_4FM8OVySFTuuubrNaMNi7PRgHAO9SHlYF-36dsCr2ybizL_vPDk8LYiFWOhsEO8zt036jf8pz9DED47tvb7fEKIZo7w19IptHvAztqR0qRLaodfFWTA3uwJ89THJkwh4HCx111gcxIpH0ggSy2JhOr7-HcCJk=w1739-h978-no" alt=""></p><h1>爬蟲流程</h1><p>這次的爬蟲會經過的流程如果</p><ol><li>安裝套件</li><li>載入套件</li><li>選擇想爬的頁面</li><li>加載更多留言</li><li>檢視完整留言</li><li>解析資料</li><li>保存資料</li></ol><h2 id="安裝套件"><a class="header-anchor" href="#安裝套件">¶</a>安裝套件</h2><p>這次的爬蟲會需要使用Selenium，安裝的方法如下<br><img src="https://lh3.googleusercontent.com/m6PjNPESUqsilH1DNfaURaID1HaNb3anLF-uaT-XCTgycg9oLqB2lQZ_NcmQKFRfpuHMl7YUCHtH3BH0i0HNLKMkJyFfPGu5t1fPk1PIkdTxHDSSDV4NZQ4-hhEEMc8LAxQ_G-hdgHP6HBY9nssAUL7y6_J-ku-V1ascTZKYCXo3-edYw9sB7zwkSm10CWC5aWp4hnmArynEN_d1aleig5mEznwNf7EaZBEYcSlpsc69z8hNhDBUEOJe9aWwy-xKEsqJpblp0uYN8Il1HSxJ0FgO4nhipl2ccOBk6CWAFntPTERNwzLqSwTD6kRGvQ33k0MtMQYXX6LLD6LzNsrxZnqCV02aq0L4-esbKSiHUZXaaF3tUiAWS-hlgl8VDtgk5AYnbFQdaOKf4pUm-r-tezaLx1nLy-HmF8v26t0PpynQ5gU7eJFqzHRxMxdarxC3yCocA4Y3J_a9vDIorMZS5B0lgS4noUWs3gx0AKx0JjBdJ31PQogVQ35f4bmaCTY8YWRsL2eHD7sN7tlWffRt0b6evN7TzTz-GFm_lcX-oanvBYrEmRDvXnk_pm2eMfzw7NGVB43FHSLLnRYuZL5Z-wTd6jwNN8raTLE9uYIPNSMPnWFoNcJCq3JrjfmLJFyaHNS8JzK_KvJuTJJMTaay6ZPkO5YmrLqXco13aeXfAjh2f7fCiU3bnWI=w1509-h90-no" alt=""></p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p>載入需要使用到的套件<br><img src="https://lh3.googleusercontent.com/Q-aLaTmFZBwybQc6pPcSivze-ZG2E522Cw_BvGeZVASDwhkGgMjZ8n7vqYrmaHJcvMnvCs-6ZA7fk8_sQ5AVl3iHlAaw_TRphrbOxKXpe2PY-BDG2LUdm9DSCANE94tcYbVmKM9bzp3QDEI4BgDwpAYM2A4qx6WEV-EKA9Kekn5lq9ei2nOW8XS5YaaDIYl69IX8W3m1jlGzL-3lHfGd9naur1ZO0UyWMI1VRv3NHq7vqtIksvFqzjq7lMNlvJdM_Snz3Zn7WT2uUGhaIKnM8v1_1WP4epC8mBQ288J4Beb0jYtE7_vNNBHkl7yKz07DMHTek6TqGQQ-qcVkUIGXwoMnZax3WeopWI8TpEzLi-_UnyDaZig34D03FNsnJ1W01OEIkG11VBFaGXcQiqCb7_EkTCIOxK9w5EEe_UQSzOOXFlf4ZnEXz74DCpB2tDZtkFS6tsyb__l-r1hTpHYI97fFfurMwKZsmHbfSqQPrT4CBesCe59L6h8omWRKztlo4wfgcwU2n3U-Zl5Gejr8PWdJ0yZw3qhla7lcFtl6PqGJPjRYVlIwJJjj7QcqjhSrbkW_AZKptRJ0hEif0OaePX6-Y2X4invdY2BhNZH-iJjwxNXWDRW0iILf6C7oOX-c4vCEjXUkJTaklQQQk_c3Dge55Oz7whEuSn5TG8h1uscvQKE7eMSy7_4=w1515-h145-no" alt=""></p><h2 id="選擇想爬的頁面"><a class="header-anchor" href="#選擇想爬的頁面">¶</a>選擇想爬的頁面</h2><p>在此以 <a href="https://play.google.com/store/apps/details?id=com.global.foodpanda.android&amp;hl=zh_TW&amp;showAllReviews=true" target="_blank" rel="noopener">Foodpanda APP</a> 頁面作為範例，想爬其他網站的話自己更改連結即可!</p><p><img src="https://lh3.googleusercontent.com/iK91grOv8nJJe4FDiQ4EDMeSB0Dg-yDioEz-LMSjaQbXukez5NMFgmabHlK1SBvUlwKvKnr8mUE8CKP0ZSz7bwuuOW8Nu6E_2uy6UTaUX8RTtsAiq_A3wWQMzYJ7sL3S_FKRWQW7FB1JrOtpGXLMExfVf5jnqSYbMvY-WNx5yLG_pnuGQrGyaXFsDARcOB1JeKt-AqiXnHOTdLBk6kZU7HoTsbJF1o8SyX60weQT1UJxGCqeNri_2wAsvezEPBk42dAr3WYFW2mEXjYMPShYIYRlL1eXZt5BaY1FB3QGj4dTgQRX0v9tPoinE15TLs9Zo8batCY0ypPWmLcnms-W6PVpgpbHwxQeBfgJmrgvF9QC8cOM1maLW0QSUbMLvZyIq72_3TVpd89mKmxdTfgpCHeoY0ks2vsnGTt2pfdk5Ax_hNsEgno4Kq08qZR-V1fZS7SkRvyErn-GwJX6ZZNAtEvWAw3PSsJb8Lx-aiXZusEo8NfPMvtwpISPWtsK11D93FgoBt9SxljPYKXlJZcgaLa3imS0gxW-Z281lyrTlo4u5OR2IdGRYKZ0Y6AwOBuN2gv8OjYAwgjIaE7Cv2zouISx2nzW5I2y9NZFRV4HADtw8N_y_KCNsPuZBjixl17-ycpZ561DvcgYxNrx9fVcX9nf_He07Bl93q_8uFNbeEpB5nfBVREcmGg=w1739-h978-no" alt=""></p><p><img src="https://lh3.googleusercontent.com/CPmCuDKJAP0lKEsfhV3lyzFbmMqwGFYpTF7BKQQhfSeR9W_711fAYqdc5kw_1Oq4eDf2EPaHk0tD9rH8HkEBdzIC9X34COP-rMNcQSHhk4zKiOkcJv2of6XIdSJkDiVD4vaxVT0lNB0vYiRk4GfYzPTxXtSnIrKw_u1fsQbByIkHpaMwJw8mBX9yAidHabYigCr9M0dYxJdQEU1d2MXY1VkVyRCqdm_KFa6RPtfFKny59VtyVp673hP2h9-J8xLS6f2T36d_F3IgDSIRlwi_lcKTtYfqn5jT4ZcHxNKNOlSjS56xmSZhIKFE4h4c4LdKVMbaV4sUPJK1YVA5xYGyXK6AvCLeai6p5-kCUz2lzvZ-_gaVPx2Kg_aWZmMJ8xkZ7k23mIxi09HD3cciqPRbkaNSFz0MVjRQ_b5bS1sVruCWL8tOicebg1PxU6TlWPWLLUUhf-Gykca93JeTqb0HFyGt1_JJQOtwoyytn7mijt9NOq6r0-7cFcFWfkjaRenMtTaChMaIZdwb7ginzBERQ_-FRLcEKCkLSnxKokFZlCJz14VF2ma3c1Df1MTnj5NofROUZytFsc_FrASzcaofbDMYg73rshXlStpVZbWJnOg8-s6SOlfw0zmTfTZsO6YC2iRSyv2G6pCrmC_K4d7oLTcMbjeYlhzQjQTipbr0_PNeAbwWJUhYwBc=w1507-h295-no" alt=""></p><h2 id="加載更多留言"><a class="header-anchor" href="#加載更多留言">¶</a>加載更多留言</h2><ul><li>透過觀察的方式可以發現，當網頁滾動到最底部的時候會自動加載更多留言，但是每加載5次會需要點擊一次「顯示更多內容」</li><li>另外需要留意第一次與第二次之後的「顯示更多內容」是不同的按鈕，所以在這裡使用了一個 try 函數</li><li>在這裡我設定做 10個點擊「顯示更多內容」，實際上可以依據資料量的大小自行增加或減少<br><img src="https://lh3.googleusercontent.com/UvNjhu7xwKq7RLikJcYCBBGrb4Wm39RkvznDeFyie1BpQ2KrIde4jTHHRP8jqzBUfZ4X8-V9fUSAbboYHamony1jA0D7fvReTQsvyhTcPHXLOIscas7VIRktvVYiakbEs7D-K_nGKCxkJ-GaT3q7P_B70W3sg7bb0DWDXGRI3m8V0ea4go5v13Il0haW4xmrPSaRMj-sAnvruTXHCTBo8gp_P1MXWefqC3xv0FUqN1jCenbQD4eKsUYvcrN-KCzzMgfYEBxfdY8hCCvLegs-lXF3o80OOLXdsYaFPIa0U9Ft0inFzrFKlSrNl8-10JNns6vjXaGXYDarC0QvE6XQPd1y3YbgpUKByYUPmpD5w5I7JpaObmr0HuoW-M8OZEWcVVF3RfrhcSZrxoN0n5_Tl1lqIuFT2P6DnPtFbJpjeqUftuGA2NBsy-C9nM8BkbZwaey5YZ0IU0QOI1PsH_2-ErjGKvpR3xmeT7JOUBjtB4SQNtizVOYEndFA12f34rY2NFj_v6tOMjAXGTiGREBulOoyLeaPk5VSm8IKO9iCJEQxBxEABU-yMFmd-M5QTYz1SemeQH5vxDQe_Y4dsc8UavCTklEOY-wpMYXLIaIf3icU47NvSCzrSYcOEwlHL0kSa_i7Ppo_7MnXgW1TIcCvD2DsY-KMw6cmyBdp1C55HSmkDUP_70n6c4c=w1514-h242-no" alt=""></li></ul><h2 id="檢視完整留言"><a class="header-anchor" href="#檢視完整留言">¶</a>檢視完整留言</h2><ul><li>對於長度較長的留言需要另外點擊「完整留言」的按鈕才會顯示完整留言，顯示後我們才能抓到資料!</li><li>這裡我找的是頁面中有沒有「完整留言」的按鈕可以點擊，有的話就點擊將留言展開</li></ul><p><img src="https://lh3.googleusercontent.com/YKSV2CFs-TGvnbnBzWP1PvJczTioKJzUyJA_x7eQqli6G3pYL2W2rTrtwyGfc2wYprI_dLKngQroECIGbDtZPiWu9xnTzI9_w28iPGlVZfmJIgVgaHH6mRjzkB6ADzwF43KaOxdFAUK4mW6xsF0wimz8VD4w8UEeGGkuv6-AixwNZ98WVpQpEu5caBwnENIg01XJb2bzxJrpCJjCFN3U2qCkrqizcTSAmsGYuEgLekX8IE2TomJiONQ-oHB_6FRNb5Wx9WjwOlC1_xwaRKrZvAQyzKhXrASiOdHHHLKRsh_VZqPGZrEYyaq76XaXob4fqQPzmyBSZ3hYaEMi_2cdYPL0ms4OU33l8_AxQhfS0mlOrdQ6vM04vXwJBczOGDTbmTmlBUp5qWQiLDxf4InOn05_dz-G1AWFiEtA9lVQtX-ElErhQcc3glerWarN5e9UE3zxMf-xp3Qgr0HXqj2ynRM4mb8YONNUnJYkJbWZ-kdNeofsgeC6t9BOQjk28K1KtP4sYABZwOKkDg_W_X1L_m2EIOfW0Y16pLvTjBAzBnf5xTFkN6lsa0lLn77za9aYZOwL4sim_nT2bSLrdimF5Ay5i8x58Mrx4e-B8MrDxCGTwhPS9V7ZcAuxSunO8yavq_1T4z9lrkCWSFCE-ry0uMoXUnE96y5LHu15D2lFsMTUR4HEzYuaRGM=w1739-h978-no" alt=""></p><p><img src="https://lh3.googleusercontent.com/QJzwab2aj4SjQsxIbBOoMslSEMT7MA2PCbOg_aE92OI2q84TG0hg6rF6DeM01CbGFdiEG7zwpvLIAjh0JdoqJNeifckNUfjG_vQ6rknESmueaIAk1zEnSJj639PUXUsoTW7KRZyJM9WxMXj7KE4RiWWpaEmTrTMphtNJdANc5lv_zoQXz-1NNldLQPY1yBy7JZJdXuOyJj_L4K02YiAlpLpFKKddjFAFOB_jaVi__THoBy89psEiRd-sNCtTkTiXomMljFFsIeDIRaI5hccd-qo79IbbHg4mEoFd31LsRVdY99fGL7ymlnfwRFu0zehLhxQH6AcBiR1eARxoaw0OzI2M6KB8f_r7HdnmPMchveH4j8XJNbfChP_b5muiC1U00Todp6ETwhUu2e63CO5W9tCM9iwnwPsXTCNqZU29pf5QgiAQ3er6GFDRtzmHS6wIYfspnYBy0LKkC7-NotkOpV7zWBu_zfS4RcGwKZcX1XOmqI72Rgn9G9p8Vt5v5UaVFOQQobNwxv9NKeGo0meUavK9k7XdhqZiKM6XWjd6S5kyMXeT3f1e63VzkYPt2btXNyoyosKpE4bSJPh8f4O2pHXcmBv8N1HvbREh489KD2R7PvHZliJE6VT_cNIdJ6gNl-LgSWwkCeNLfiy23ic2hNSDvH1kyLhsVbNhBZuosHL14Yl853lTM1Y=w1512-h168-no" alt=""></p><h2 id="解析資料"><a class="header-anchor" href="#解析資料">¶</a>解析資料</h2><ul><li>當我們把所有資料加載完成後，我們的資料是html的半結構化的網頁資料</li><li>因此我們在這裡需要透過BeautifulSoup來幫我們把資料轉成DataFrame</li></ul><p><img src="https://lh3.googleusercontent.com/8kbpnn8SQruf0Xfl4IfWlcyLFat45mArTP1BpKHfYvfORfxBETYjBBNSZwfldIfr6bXfYUS8vuI_DT1qGK3bnR4fbbUAnK0PPby4q7YhbiA65iwb5GkAtjJkF0_kIfCG6aH_RmHPO3MHciwm5HL7l5_MzrA36P5s1RVpIsG86SgBHbbNxM7E0aDHBe6Bao50Vq0T4dcjBvbA3gntftpCo_lUpI5718IyNwbZew4LP7SssN1wNdGCoQfY7k4EA-teS7Ob3F_tRem0CoSSL_t3q30CKDHy0iNwV6Lkxfv-0qXU7GYQpwh58G7HSdBffew3zoZcyAg5hhU1v5lmKeKvY0rPnnh9ilhOJ5JgsVhV4mv7mhbf8g6jrRV1HW2I_ahu6uvcgl-8EbRqApN3xbtR-J47V7qBMRLozhHyBP3ylvB-1RgvLSRB7nIRZqYJLmDXAjc8b2HZ5HvtKJ3qtnA5Hq0Ja3A6bE48m3yBARewfD-AQ5QWktrB0MfhNSpnMGSAgDc_pzkLy1DqTrQaZt56pQURsSxOuud09Wga_tjG6WDxBCFZxu9p93hqN4b44kd-Qdq9a51LT9FjOlkDb95tXsZU7OoqmQoZIVgtYCGl2ywm5EBMxjZ-DnShlB2OypWegn-Jzrs5Z8rGYGDqXEk7caC6VqvpPSmAJXJqG6Ju0EJQf71L_bBG84g=w1510-h478-no" alt=""></p><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><ul><li>在辛苦(?)的抓完資料後，最後就只剩下把資料保存下來囉!</li><li>先跟 Google Drive 空間串接，再將資料存在 Google Drive 上</li><li>資料放在 Google Drive 上的 Colab Notebooks 資料夾中<br><img src="https://lh3.googleusercontent.com/1RY9JXc87whTGNVpHY8GNm77RarneoCpsxulPb5vj6wfXeVyWIK7fZBDibHwqNOzCts_l01rF4cRfW4DT_PgQw5TTiy2GJzPkBOHVUF5Se406acjFQeu-5aIdJaSPN2BfZUe9IcH8jsejbAkBIKV-bnLT3iSwDcQDrEWWPShM-aY6Ik-ZCVgg0ru5gEzl69wz9jxslwry5qIlm1bqfFgsr9thO8KxVdWK7izA4PkHv9KtRW09_wmPr7XxfomxanTKhe2l65vpe_oTs36w03dz8huj3tKWHo9ZafJ2N4gSqdPfiabN47tKetfHlgRY48nV99oVeIQTMSehsIm9hS7ppdygWz_h04W5oR1-q4mvIREU3wwbpojD_9ae87F4aTIiLEVHKi9TOvJ9RXkgKI-z-pShLKrrEgMcSdb0XxHMXWEQNqLSFZ3u39GRG2yJMDMmF0Fr_Tj3kj7RCqRCezX0c4OsgR81NMV6Wt5CdB6NvbxbQX2HOxfc0txXze0hdGjgwaTsBf7dFYWFCw3iKBVqlVbdE0bywKFqky1i2GnxVBWOIgTijG-B48BHy_tDhWpvxB3bUnYGtfotQsMV6HExX2T4iii8EdAfpV8zHdHEAvY3yOuN_lDUfz7Iw67VZQjVx9M7M5i3tQPYJLoOCf-s8xNk9V_XCzwwCNsYsUVfZNXK8-4CzdzGGg=w1508-h67-no" alt=""><br><img src="https://lh3.googleusercontent.com/vbM-BfMd3-qRvcXEo0ROQj_xubD6sDgxRenFirWnqQyMZKTCDiOH0R3r6ygLNNzgs32iUgHYy-TotV2HlaI240JJMV4NN1dcHZvRVQtVeQx8iQsiO5yEZ3P9wwXN1PKit9q_yR0i_RcL4MR5UAa3o0VuEqPqHKKhAg7orRZu0S6_C0lFtJFPfyTRNqsiR33iFkWvZDPzOuvjjzQXWqCue7ufVALDV_B0qU_6Nx8dkNcbPJ_wLI-DEriNX8zqTVa8hU9D7rZ-zaMjNO3BtNpK7uW2x1bi-xK97eHBmK1qENnups7kIvUb4ukyKZC2t88w3thrO25vwlxgc-oyZvAXB_23CiUeE7pbGvBwypdfAqg76WDm31-RZMb_HrlhB0LaR2cXaj3Kp7JUjZNNMce4dH5WppTuVI_j5BXikdkaW_igKv0MO6i-s9Ikuuq7Dt6pJxzgjL3nWsypQAMwhGiSzoSFzYVJmLyHz85WTBbWxp5xQcX5b4YCOj3qk0I9B3vbClvj5utdmYl8XKjtGrNL8HMhwY0FqjdMXirhfFmftH4XL-bip9fU6xM__1DT0ofUgh-xo-io1hfsqWz8VutTDixxv9ry-SXwvAIOliMD9WKH2rzsa4iRo6KhpU5RvsJI7ZI8136j3AlyNMdGjY07Jx7ghy72QrGZwWMmn1JOUf36mAy_eWPUE4k=w1507-h85-no" alt=""></li></ul><p><img src="https://lh3.googleusercontent.com/XqGbKWIZbCLvd8iGRR41WLodgwts4h9B7NEEWBh_Mg2Xo1k07Fm8FP5i-K_si7_fq1-9EuMtyTa8a-UC7_jupFpdtg8ePU2NSZgjtxya5QV115eKFcUrQeytCRqteC-XJ13otanJFZOkbm7LwG8tt0gRrSF7qVaWaOwcG2XTLVdVrJvvAnxzrK2f1G3eQEz7gZ9zLOyZMU_X9dnMh-Tx5I7ajmYgOFs1ulghQH5S54eLD_hu62mUyGOg8qc06xH7ms1K2TTNnoifjshDMfOzU8eTbfakyoV9Wt3SiZlplLxpQ-1Hxps4SAeNrHZB2MdP3J14rR5Vjz20EWscSY6t2Z1QESIYbBS8RZO9qZG3g83WnVGA9r195OiKTXKX70wVjlPGSzz8cLPE05QXpOM9ZcL_YZAgZc5eKtI8fjcFUsyiEeUyfbVluEdYNScPOizNLsm4ovhkuzROfmTwBAo_we3qysHbqHgvvRTYOq5PKV4MjiwD0Ytmi4cvrccpt_YRzMvDCfxKWmxwmbiaEhTZNVy7nQ4JH0xxDM3akaIfu34voRjUovMfalsns28I4gK-a11Ux96saA9E1VY5AG4PY_Gy3xXKXEIYBl6dznEFW7IBJt3Wtk1x18tmlJNpW7bsnI9qBuqIyNyM-gm0NtffNhxKoV2C0ayEpLnJ0tD-AKkzPOfbGOCJWvE=w1920-h878-no" alt=""></p><h2 id="後記"><a class="header-anchor" href="#後記">¶</a>後記</h2><ul><li>我將程式碼都放在Colab上了，如果你/妳有想要抓取的網頁，可以直接打開這份 <a href="https://drive.google.com/file/d/1_jgNnCa8SYXJK-XO3VzG6TdvZ4JMK11u/view?usp=sharing" target="_blank" rel="noopener">GooglePlay_Crawler.ipynb</a> 執行程式!</li><li>這裡抓到的資料也挺適合用來訓練情感評分模型，因為客戶在留言的同時會一併評分。並將訓練出的模型套用在其他沒有標註的資料上(如 Facebook 的留言)</li><li>如果有問題的話歡迎在底下留言或寫信給我，最後祝大家聖誕節快樂囉🙂</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;隨著行動裝置越來越普及，各家公司紛紛推出了自己的手機 APP 來降低跟客戶間的距離，如果我們想要了解客戶對於 APP 的想法，我們當然可以透過發放問卷的方式來收集客戶想法，但這種方式的缺點是會需要花費較長的時間。&lt;/p&gt;
&lt;p&gt;另一種方式是我們可以直接到 Google Play 的 APP 頁面抓取客戶的留言，如果再搭配一些文字分析的技巧，我們就能更快速、即時的了解 APP 的優缺點。&lt;/p&gt;
&lt;p&gt;在這篇文章我將示範怎麼透過 Selenium 幫我們自動抓取Google Play 上 Foodpanda 的客戶評分與留言等資料&lt;/p&gt;
    
    </summary>
    
    
      <category term="網路爬蟲" scheme="https://TLYu0419.github.io/tags/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2/"/>
    
      <category term="python" scheme="https://TLYu0419.github.io/tags/python/"/>
    
      <category term="Selenium" scheme="https://TLYu0419.github.io/tags/Selenium/"/>
    
      <category term="評分" scheme="https://TLYu0419.github.io/tags/%E8%A9%95%E5%88%86/"/>
    
      <category term="GooglePlay" scheme="https://TLYu0419.github.io/tags/GooglePlay/"/>
    
  </entry>
  
  <entry>
    <title>如何透過爬蟲抓取中時新聞</title>
    <link href="https://TLYu0419.github.io/2019/10/30/Crawl-ChinaTimes/"/>
    <id>https://TLYu0419.github.io/2019/10/30/Crawl-ChinaTimes/</id>
    <published>2019-10-30T12:46:01.000Z</published>
    <updated>2019-12-22T05:23:36.657Z</updated>
    
    <content type="html"><![CDATA[<p>以前我們在收集新聞資料時，是透過複製、貼上的方式逐一的把將新聞的標題、時間、內文…等等資訊從網頁上複製進 Excel 的表格當中。但是當我們需要收集上千、上萬篇新聞的時候這種方法就顯得不太可行。</p><p>那我們要怎麼解決呢?其實只需要引入網路爬蟲的技術，就能透過電腦幫我們自動瀏覽、收集指定網頁的資訊。今天我們要透過 python 撰寫網路爬蟲，練習在中時電子報的網站上爬取新聞的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊。</p><a id="more"></a><h1>先看結果</h1><ul><li>下圖是我們將新聞爬下來之後的結果，我們會將上面提到的新聞標題、時間、內文等等資訊整理成Dataframe</li><li>有需要下載的人也可以在與Google Drive連結，將資料下載到自己的電腦中<br><img src="https://lh3.googleusercontent.com/N8gNwNU6T7-ZGiKIyCee4wq87LsHDPv23QWgou6AzpqOm-8bkkeuCG_DB44Jt6mUv3NYx0VCH5mKMRZhvI0wsGznuwTlllNLwem5faa94G8i8GO_W8xAHNNVvLY6eYNxBDfMaXEwkFOtGSeTxrd1pqF-bFKWWvnPQGUPBOYHA2CZxA657AYw1sjGIXQMMwqZCFRmzviU_7bTPuHPV0vYR71S-5sKPk6ZL23Suv5RcV5s3z1o3JbOJI7Ou6TBSLOcxskn7Y-i3t8vly1DiQAF85SaRB_8SI5hI8uO1FJnqVUuKpGKcSOkdFB3gIVV9gMxNjFlVN7m4Xt3idCZYm3x_nQ2yz-CnpAA-tXlU5-5Jjjc4aQEJHEKB01xj4vgkjb5Anc9vqA498XJsGMFT5ypGgra3wx97mfpxtFS08QViz_ogQ5DLLHQkRx8jVWqoWMUtbQIyXcCx5YwCJESJGEX7J6tNu-MDVIa2La5UiSpqC7gPFHyiLpe5N42p4aMkXOoIjQ0T5SGomzVF-kFdqfzB4NlPd8rwR0WZID4Ay-OgOg68LCwMPjDPWChbMJvTU85znuWSlCYre-WLkuB3c6nCzDVuYNPQUbaQmVurZyQgZwRP--JD-3x4HZm-OqK20zzlQRYtaaQvs6RmwEq1bYyfuSquLn1mCHztBruzxkyLSD9c9fpYonA96M6C0LcijLlJZ4pV8HMAsosApx1znI-YN6qAc-lDBW6HwHGfM8q2zus0RnY=w1840-h413-no" alt="新聞爬蟲結果.jpg"></li></ul><h1>抓取一篇新聞</h1><h2 id="解析網站結構"><a class="header-anchor" href="#解析網站結構">¶</a>解析網站結構</h2><ul><li><p>在開始爬蟲之前，我們需要知道怎麼從複雜的網站結構中找到我們需要的資訊的位置，接著才能透過爬蟲自動幫我們爬取資料。</p></li><li><p>那麼在上面提到的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊又放在網站結構的哪裡呢?</p><ul><li>以中時電子報的新聞 <a href="https://www.chinatimes.com/realtimenews/20191030001004-260408" target="_blank" rel="noopener">英發動大選歐盟換領導人 歐英新火花受矚目</a> 作為範例，打開這個連結後會看到新聞網頁(左半)，接著按下 F12 或 點擊 滑鼠右鍵 接著選「檢查」，我們就可以看到網站的架構囉(右半)!<br><img src="https://lh3.googleusercontent.com/6bTMD0hNVHYTNZwkXc0WzpT_Y46ZlJILp2M9XeTOEPlFJDD0ktrZVC6KcQpXFfvhtE_nO4lCka4FnMMWWX0MVLiDZK-lkOpOmliJfjSRgt5vUobV3Dm5QC46fmRt18MPGRO5cvY5BHsp4RwZIo-ppTaUmIwkpD5dPopTHGn7ybFo8Ijm8tpCIiYbOZEc4g9q0VEmVtYKSEb8KPh6RpawkDMqF-s-nMNUqJxUplMBZG02IKibjLeeaeEOv-gWcQfoF4dp0rHJrraeruZLJB15kYDJ2GJTfF7IuYFRtgcAvw318WLdPFeSy_L6GtAmktI4ufK-hHUpYsVGmY--ESlAjVRIAYwkXDK6TqTdP8ZFYKgEEpe5xBYu2mksYmryl0Hi5iVARC0dSSAsRJ_nClYp6QbFsSEHKo-vQkITGSwLcCvCTvpD1Qwx8h5db-CEuUj-FjFwNXLL6vgIYme5zTmIoKH019D-pwUpomkgCUKx5A9vV5uBGYpxGljfKsOYwczquCYPl3kjzHL93S7vBZK5MhOspgqzl1l1gA0FOYwI22OAHpG97M-BND-GZKxjx_GLQ2GreG7R7PQ22WdarBZlBNpWN9BuS1C6NPCrGFzyzP9esLGdCU41k_UqLLNOnkH7mVIXNRUhBKXpYBLJZiul9PFrSh3916NQ8i0_lvfn4NSIDARnaaHFj0CkRavqBY31GW81zY5SwBEO9Yn_zqq7sqAuQr9T4mpl82eaJ694C-pf8mJo=w1739-h978-no" alt="網站結構.jpg"></li></ul></li><li><p>其實我們看到左邊這麽漂亮的版面格式都是來自於右邊的網頁結構中，也因此我們需要的資訊都能夠在右邊複雜的程式碼找到</p><ul><li>以新聞標題為例，我們就可以複製左邊的新聞標題，並在右邊的視窗中使用搜尋功能(CTRL + F)，就能夠找到我們要的資訊位置囉!如下圖，當我搜尋新聞標題之後，就會發現標題藏在名稱為 script 且type 為 “application/ld+json” 的元素(element)當中</li><li>另外我們還發現新聞的發佈時間、新聞類型、連結等等資訊也都在這個元素裡面<br><img src="https://lh3.googleusercontent.com/MY8vc8gGbLzMt02Ai9FI9hhTZTfu_hMmGBAz-0n1ZkwSuoAe_Wh3TUjQBkx0sPq609thcEwP1tKPUD_VuNmaIJPBfc0OeaA0REe2ob0jZgNmwTVNvwRt78FhaZUqjnYVazz3pdDxZT4bC1TQJVsmqeLf6FFnoY1mA_dgS56rHSn2dLTSmajL31dkl0CRYEM_CVTWHQZ38cb91t16DTX_aPWCsXN9j0Y-1AtW5IlcXAeXda3hIAkGCkm-ojFlB66U0fFjnUXDcIW3XBSnppC8K__Knjpn9ZjywDNxAwmX2t8QUmtVhuazbgczbStaqZmeAhAdXAZcuaOkgcMh1eV9HppzG0BiRxXdIj5NSxDJKs2nNk8AKaWhcH1NJM5IVAm7K4X7NtrPImVblqUB--L7vQHS331EuQvamcJk20Y4w9f9sQp-DE6hqox-wBDa8o5XqVsuz1LTvkqXgjJLvUShArXqXAYE7gyITn39V4ufg1G4zkuZ2chPh-Xq4VCQAHxEDHCJW1uHWL8Ag-4AApvZbQyr4tAYpbNFrgPLb3whGoxvv6hMW74zkqMuelDCwI1vXNvuc-xMbohRPjHMcQEO3unumYZK-XPxjjkgwWI7cmC2t6rO_27GD1hYAo2nSImdlxccYDHZ1qtKrQzXRE2cAFvJ_MI1434LsvLLAR6abs6U3n6McDmQvwbIqrO9Heb8TRM_8Tmy89jwZ5LRpxGd1XvW8yR_aHGFOYmLb14IZtxK6Dhl=w1418-h978-no" alt=""></li></ul></li><li><p>經過一番找尋後，我們會找到我們要的資訊分別在以下的位置當中</p><ul><li>標題： soup.find(‘h1’, attrs={‘class’:‘article-title’}).text</li><li>時間： soup.find(‘meta’, attrs={‘property’:‘article:published_time’})[‘content’]</li><li>類型： soup.find(‘meta’,attrs={‘property’:‘article:section’})[‘content’]</li><li>摘要： soup.find(‘meta’,attrs={‘name’:‘description’})[‘content’]</li><li>內文： soup.find(‘div’,attrs={‘class’:‘article-body’}).text</li><li>關鍵詞： soup.find(‘meta’,{‘name’:‘news_keywords’})[‘content’]</li><li>新聞源： soup.find(‘meta’,{‘name’:‘publisher’})[‘content’]</li><li>連結： soup.find(‘meta’, {‘property’:‘og:url’})[‘content’]</li></ul></li></ul><h2 id="撰寫爬蟲函數"><a class="header-anchor" href="#撰寫爬蟲函數">¶</a>撰寫爬蟲函數</h2><ul><li>盤點完我們需要的資訊的位置後，我們就可以把這些資料放進 DataFrame 囉!</li><li>在這裡我寫了一個簡單的 GetNews_chinatimes 函數，讓我們只要輸入新聞網址，就回傳資料表供後續使用<br><img src="https://lh3.googleusercontent.com/1O-4wLHHQ78GCOzmgAK71p3mHasOtfBVEwI45doSG2T_N0_z5ZP1d2U143osp7oL1lFi8d5UWC53A0WtMokhuleNfLwtTAilQzKaQUP-zbHqqFVgfUN5zraUyEG57MifDVCdRlcqg3xlm40LAz3TtlCO40RjaF_S-BUvKRq2fO3UPVldfFoLuF1wPZW8jGFys-JR9roOd-tygiUHXf8RaESWszJvoKyyxhDJFrttGltTRlSmFpm5SFX5iwCUcyL3cvHXthXt3j-YfR3gYP2ndx3CJE63tfzHOAubKNrC23TQbezeOec0AhSXtIZRd-zOUDZduqLQcfBi53eRm5bfmVc4ZpgcOnqFq2H1VUDIQZUoJiG_pF95ps1GdY_uyW4NqS9GchwQfVTj9JJ5_EEL1dBS8sf7YMyuh_m8rfzi0B6Aab9-5yn17yB5nghjaPmhpG7yIDJU84m4p7lEwYBJduJSHABsyUjhm95QsShyzTS16_nLRoiSDc8B_LJMZlo0tQvhoZa8jq_QjQWzmkGl2d_yjL2EJrsfIqRAB5Y_G1xOKvAzjHyxcUDVJJ3dk9Keh9YVkxFRarsPLqdVevH_aguIrPUO7eMI9pr4DC-RA1nYyeY5yN7GO3GQog4igKFOX5oE2gV05QFszZuVB1P-O6EyA7PB9eJ6CkRjPQF7KXRW-JDgI0w5yyzKrqnye5udEhag5z7_xGgiAj6Eh-e9qUc7G3qhLRAl9hHj-IxqNqOuyfBi=w1861-h526-no" alt=""></li></ul><h1>搜尋特定關鍵詞的新聞清單</h1><h2 id="解析網站結構-v2"><a class="header-anchor" href="#解析網站結構-v2">¶</a>解析網站結構</h2><ul><li>雖然上面已經寫好 GetNews_chinatimes 函數，但仍不能滿足實務上的需求，因為我們不會只需要一篇新聞，我們想要抓取的是大量的新聞!</li><li>因此我們需要運用網站上的搜尋引擎功能，幫我們搜尋特定關鍵詞的新聞，並將回傳結果中的新聞連結都抓下來，而要抓下來一樣得先找到新聞連結放在網站結構中的位置<ul><li>這裡我搜尋的關鍵詞是<a href="https://www.chinatimes.com/search/%E7%BE%8E%E5%9C%8B?chdtv" target="_blank" rel="noopener">「美國」</a>(左半)，並透過 F12 的檢查功能找出連結藏在 h3 元素下的 a 元素當中(右半)<br><img src="https://lh3.googleusercontent.com/JwEFSUZGq6nPNiZidVByMG6eyVPUt9U8uJVARkd4D9t7h9VZTPLbZOfTgsbmLh4TOBLrM08XevDHbc7UW29fX0tMW4tpds9Q0TTSKEiJYzA8zgeB3h1CbsqN2TfXc7SzL9sPB2YrUMzf-D5jvX25JsOCHSOW2yNhFQyc0-0343asVUDJCeOBibIiARPuxz3QLGG-zPAQ6tUHtx3j-9VmOa0nL5dRmKb2c_-uF--GGKv7utloxgiDnoD0OqdZbhfxrVSTCzHftAjAbVg6JWKYSWwqcaHe8_0fsRBRPP2NxfM7HSvzZWKvsahkZscTiilyGwblN0fODArJxZjike1xCCizpocBTbkjBv05A_LXn3-XqcPwdtC1RV2R_a7sPLNKDPF1boy3X40qG7nMWSoYTQPCJ4oT9As4ZMnHNJ5w5wQXlmfA4g-cKGesxVVtLJ-r9cN3ycz9NmpqAgF6H9xVjL2oXmQF8Orr-OHKpHT1hMHHV0BTL4FjhkNN2BQq9bgwKx8LsEcIy-D2MLIdy0o9PqoMjSsowEE6cAD3S4I4622KG_0ja10qSAtvI0hfxztS5vRPl-rVE2kT3yC8M1ssu2afnmJjdsM9xhGNwlnsZAUwFpAHejP1AdTXTZLprAo4DgbKLxxOxnb8qjfHQv6l00Jcf1iSys-gGF1FfKjXDtDl-sftztM0glqXTPjdEOJOXoUpAVNSru1PULNZ-IssPuoQm2L3nMpUTdhOB8_5N9VWK9rS=w1787-h978-no" alt=""></li></ul></li></ul><h2 id="撰寫爬蟲函數-v2"><a class="header-anchor" href="#撰寫爬蟲函數-v2">¶</a>撰寫爬蟲函數</h2><ul><li>要抓取連結的方式非常簡單，只需要找尋 h3 下 a 的 ‘href’ 屬性就可以了!<br><img src="https://lh3.googleusercontent.com/KgL3_3_7QDzaPngbVCTwN33x3ShFgiaZJM_ZERhfc35Nnr7eEwuD3Tf64TVsSLD0f27EnvzgiyFjWrGeNEUgWm2zLKFGHsFv9z6yyI2FEQQBspN96T-klIgEpVeAHpmot9Ci8HKLXlOX2gkHiko6dd2Qw_CC7fW9w52j9sJqTIxP8vNpKzo_fcRdWEalvvuZlNhpBmzuo-cbUBpqA-6_MkMLpdGqh7poC8HB-o6lvP1Hcpd8EatSamX6cZ-ppb8KP533NfXmgRvLB95vKxrYuWmB3rWHxdfqZ3l_xTsJuLx2qo_8QBLSqS4tBE13O_JZrfetHV0FxGvEp_Vw_ILl02Jvg3tC6meXahi0H8p2XY_lbRbQTYuoXDkNn2tkNYGncFCfniCS3Ek-viClaEhdMfmtzygwzPpR9PFlATipEHlFlj8MbeOhNHRwpqC_6w37JtNNDRGPygtMagEbatvoYQrvnJkO9_ldZZXXmhoWOCO4PY9aeeUHlajpHelhwVdnYH5VBwFtccblV2w467-BdxCDbzNQxU4ZiNnzakpCaYrx8h61MrcB9rw0uZEvDKOVyHTUYaOxWhxzkvg5GM9SwJJ2vEDsFImlDqXHpJ7uYeY0lBQXSSBMS_wfFpbwvlxxj2lpLISpHadsYi1c8z2aOa_wDhqw1Wb3GXzP4I-vNVnJMUPW_dcGECrnbA7vX54YrQo6tgpG3N9RQ2QERQYqIqtqT1hN12C4d2evRlXY0bAV-8LK=w1789-h978-no" alt=""></li></ul><h1>開啟多線程功能</h1><ul><li>只要組合以上兩個函數，其實就已經可以實現自動化的爬蟲作業了，具體方式其實就是逐一的把新聞連結清單放入 for 迴圈中，並透過 GetNews_chinatimes 函數解析新聞就可以了</li><li>這樣的方式在少量資料的時候還可以，但當我們需要抓上千、萬篇新聞時，「逐一」這件事請就顯得非常沒有效率。那有什麼辦法解決呢? 答案是我們可以運用多核心多線程的功能同時抓取多篇新聞內容，只需要運用 tomorrow 套件就能讓開啟多線程變得非常簡單，使用方式如下<br><img src="https://lh3.googleusercontent.com/Ru1GOLmFbwajDukJ7y5iu2Gi7YeDkqau6Kug_ctByP_hMNUkv6H3F4uukxOpEuyzHdYQ_DWne_DuMbnoRsZqtQncplZGBLkQH7HYiudDjee3ssrGYS1m_HnLdOMuB4UuEL5a1E4TlT7Q4EBBVqaqud5w5C_5jTUrse12z5Uego3h_Dk4nCZsHzYucNQSuPMzG7eSrWI6mk0ZaXbV5D7wwHavziP9vgElg1-B8htkNbJtoDZtuR_SHSMIAZlD3U0myd6x-tIgWQSevfNB7IGtI4ujmgLqg3KXNXsD2X2evUfUk4GQ7nIbx1Mj-IR8Z72PwpGR5k9Y9bJU6mlHQcfjzdJNdaOrpbZIjMqBXZLY93wpnOqEoU75aX3i4pg-Hxa38EI7GN8lVBvIN2EQUjAdjYYiL53b7UuPm5SLxfIlDqToxQRGFx-XwQBuZoJwozhnMpSo8ju4JFU5b6e4UXYY18WwWVh32F0fxbO3Hj2L3hoDhZoH9cEMm33WlIRcEimWx_V41gsDVMZ3jIPx9q4qFaK14obE1G1kYrGNa0J0lpFS7TVa9NlBLDT_Fys5FmNL5Dcm2cmI9oNRbOBq00RbyWarEabelG6iV3MSTA-XX3943uMTPtrQS6PmuX5W-EoyNtTKTgYDJqpua-5HcLA-KzOXqI5tqC79GeBido8C2Z1bCyzAsobpDrA7XrDhUYz6qJA7Z5oSFXX6ec4h56dgil2y61iW65fxV8XjbXk6GBjQv304=w1878-h202-no" alt=""><blockquote><p>使用提醒：</p><ul><li>threads 數開得太高有可能會導致被網站封鎖 IP 而禁止連線。</li><li>具體要設定多少 threads 數才不會被鎖?這需要慢慢嘗試過後能才知道，每個網站的反爬蟲機制不太一致。</li><li>建議在抓資料時使用手機分享的網路，因為被封鎖 IP 狀況發生時只需要開啟飛航模式再關閉，就會替換成新 IP 而解除封鎖囉!</li></ul></blockquote></li></ul><h1>組合應用</h1><ul><li>最後我們就來組合以上提到的內容，嘗試抓取大量的新聞資料。</li><li>具體的操作流程如下<ol><li>在搜尋引擎搜索我們感興趣的關鍵詞</li><li>收集各個分頁的新聞連結清單</li><li>開啟多線程抓取上千篇新聞資料</li></ol></li></ul><h2 id="定義函數"><a class="header-anchor" href="#定義函數">¶</a>定義函數</h2><ul><li><p>輸入想要查詢的關鍵詞與要抓幾個分頁的資料<br><img src="https://lh3.googleusercontent.com/EgzhP7GsbJxKBGY8-MIbyEnzVZSfTAcwRw_ITzkOthcph4pRVGHFXsUGyiPufbm6V2AXeJBwzbp-gBtlvXMuothZj4d4Ox3wwyfm2GFjNgzvHtr_M3m6-WC8eipt4BeVUemEbypkric5hJN7_-z4NLy44WmQcZJMO167NBQXqmNuFonatvmdVipfEkzJOeZwuHQdrv_w0UGeKWefx63FpuhPCbAA_LzK-iN6odEJ9cPCfqTm1nMVWlKKl6tbabaj-kyEio-rihYIMrhv8Iuao0j0cVasIhMT7zusm2Of90RQlIPGqlT1vGnn5dOMmcKPXrv9-oDPURnHqnptujePrcuiDaR_mQNfC2PDSSRDg4XZVmf68rzw0H_5UT_pQLsnnuKFmCpLZtnyFz3ubU0ibgzBU0fQQM04E6xtCAvlN9kO16sPMEZzExMS7W9hKkyJDBRaAQfbBSwdaRtQsm8G2T4WAffqanzjVG53J3GSVita_LydGgZlnxcvzzn1IrkAMa43Kb0aO7Bupei3Bv6PsrUp_i67UFNkRo5shPyw0bF_ozAfVTRnL0q0QyP5n_297wbBXor2jG7zcg3sWaMvlh7W-bPvXpkh7zyzcYhT_REqlTZYCKjcr98czUKZd0wxVcp1Y6Fq8xiQM88u9J84D71jK5cS9qqf639ck7skjY11Ak19srFASAapTAAfXmkCyI88POXQvqguekwPHB80EWkpXjRuteX1oX7gvEX0qzvdcMn4=w1874-h525-no" alt=""></p></li><li><p>使用定義完的函數抓取資料<br><img src="https://lh3.googleusercontent.com/DZu_DrjAq13cKmvz2B2vDQopBCo3uJ-U1izOWb2rkJObUZejoXxZl14ke0ETNrvbJsyrkpM3LSMKt4hNT--DMNalxuJg6774RH_9VdyNEzx4UKQd4atGw41ZhZGAoGQN4DSZNk7ihpUAfS6F-GKRtyy_pmqTBRxMXSJUVxobCTclTHTZj0YBKQ2ELHSqGJ6UkDXy2w8yVOahP2B34xv5EUvMfWSXbJ2QoFQ6dYO-x_Z9AamnpPYXu3MNIOZmiOFrkNfChu02G337g3ufdw6qAZCoXzC3hysrUpKmY69RNG1IBGs_BZj5hNQlGsGoUrdMOSbfVwezPOXPrMvrm-fFE0JqqHBNvyB-9Lm_pza9oRggCufuSdSpItCQTzfbh4D73GzT6ha9l20d1ovYru6RFwmx2SrKdBhni0rBhwkvlmHGkue83ecjKp5_XhfXRNLgIly466i9T617iil8ABpW3izK4UzrTdXd96yrsKu6aHfddX3_jET6Ib_0uT7xKs0lTN-3E2DBzXMWv97We8N22916AIW6NgvV9T1RCk_9G-ec0y9G-0wIlUieKuVI-Ie2ul1FjktP4ymcMC_k9UB7MIQP74p5r9jmM8rqaEXxohL-Fb1n92DCqWKH8iVDesWML5Js_Y2Tj6HbcApq2WlgF7d7Oe6KZX-HxvoQG3ogvTYphUW4Y5LN4K9Upq62vlyX1cjEVmU63nZIflxO8sLstkaVKmKcKcGh5hbH30Wut-sSLMJ1=w1876-h859-no" alt=""></p></li></ul><h1>保存結果</h1><ul><li>將資料存在 Google Drive 空間中，有需要提取資料的人可以從 Google Drive 空間下載到本機<br><img src="https://lh3.googleusercontent.com/2J2k1g29zujtoSpyi0gGhYRxruAd6KSkAC9ojdDBui5qDKmi6_m_RFg7-VYEl8FVDimmOrIm8SUlzuPRR8vUZ9MSb0X5wQQMMcMZArufTCoWqOS6QZavpUKbyPn_UdFw19KUrLnHWSzHebFduhfhs2UdX1SR1yPwtQk2WB4nUgYUbHPe3RQbikQ0QhzRsDFo0EVhS6Ik-fwpGdho-8IcQLH2PNoBhB2yQF5gA9FWt3Fza9g8lCegL2h3BJ8QNJa-qzCAGj2VTpbI6dUNXpqB3opixORD28rg9xWBg9DMq70G1GANij20dzpqCqI3_IgNT7kPfC-ToMPmULkjvPwMbzOA7hdPZrbSWMIsyYrdF24pmjcNf16E7b2K5cYK6UR_OJklMXxpZKkyLSweZAUFBLAtfB3jc6HMlH9iY_oEc9adz23MCzny_qsAQou--Cnj_ituBQ1DV9ObHwhw8oDeHv3bjfSUcWlsEf90EhIjeXVB7f4dS9NciVv1JZmrMOzBQAuODAmJOm9JLtht9tGJzLzkb5IalfQmH1KIK6g8PXnAOyF4skq28b9pO6Ul5KtRkdI4cfjSPMqjHOCCORE3DALvMkS4STQatXNCXZzINaCd6va4K5iSTabiMgszqIc62ylM8fYZUwyWhkOR1H-fwoU0Nu3bHZJqAr5O-nd-fgJaxh5UfdAHRwcmZMqKufSeazXehBFDhpv1M3XJ1Y2cKsf9XHiEQt6J9sCtGBK2eonBPW4e=w1874-h476-no" alt=""></li></ul><h1>後記</h1><ul><li>以上是中時電子報的爬蟲實作，完整的程式代碼我已經上傳至 Google 的 Colab空間，只要點擊 <a href="https://colab.research.google.com/drive/12CvYz3OMLgMl1dVW69KEO5IhWgsqUJCj" target="_blank" rel="noopener">中時電子報新聞爬蟲.ipynb</a> 就可以直接在線上執行這個爬蟲程式，並將結果保存在自己的Google Drive 空間囉</li><li>如果覺得有幫助或者相關建議的話歡迎在底下留言給我~</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以前我們在收集新聞資料時，是透過複製、貼上的方式逐一的把將新聞的標題、時間、內文…等等資訊從網頁上複製進 Excel 的表格當中。但是當我們需要收集上千、上萬篇新聞的時候這種方法就顯得不太可行。&lt;/p&gt;
&lt;p&gt;那我們要怎麼解決呢?其實只需要引入網路爬蟲的技術，就能透過電腦幫我們自動瀏覽、收集指定網頁的資訊。今天我們要透過 python 撰寫網路爬蟲，練習在中時電子報的網站上爬取新聞的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊。&lt;/p&gt;
    
    </summary>
    
    
      <category term="中時電子報" scheme="https://TLYu0419.github.io/tags/%E4%B8%AD%E6%99%82%E9%9B%BB%E5%AD%90%E5%A0%B1/"/>
    
      <category term="網路爬蟲" scheme="https://TLYu0419.github.io/tags/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2/"/>
    
      <category term="新聞" scheme="https://TLYu0419.github.io/tags/%E6%96%B0%E8%81%9E/"/>
    
      <category term="python" scheme="https://TLYu0419.github.io/tags/python/"/>
    
      <category term="多線程" scheme="https://TLYu0419.github.io/tags/%E5%A4%9A%E7%B7%9A%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>網路爬蟲_Facebook粉絲團貼文與留言</title>
    <link href="https://TLYu0419.github.io/2019/05/01/Crawl-Facebook/"/>
    <id>https://TLYu0419.github.io/2019/05/01/Crawl-Facebook/</id>
    <published>2019-05-01T10:40:39.000Z</published>
    <updated>2019-05-02T13:44:16.508Z</updated>
    
    <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>上個月參加某公司的職缺面試時，面試長官詢問「有沒有辦法將臉書上的貼文時間、內容、按讚數、留言數與分享數、甚至是粉絲的留言資訊都抓下來？」</p><p>受限於當時還不太熟悉爬蟲，只能簡單回應說透過python與Selenium應該是可以，但卻沒辦法說出更詳細的方法與步驟。後來經過一番研究，總算知道怎麼把這些資料爬來下來了！廢話不多說，我們就開始吧！</p><a id="more"></a><h1>輸入與輸出結果</h1><p>這是我們要爬的臉書頁面，從畫面上我們可以找到許多住要的資訊，如貼文的人名、ID、貼文發佈時間、貼文內容、多少個心情數量（按讚、生氣或哈哈）、以及留言數與分享數等等資訊。如果貼文底下有留言，我們也希望一併把這些留言都抓下來。</p><p><img src="/2019/05/01/Crawl-Facebook/01.JPG" alt="01.JPG"><br>而這是我們抓下來的結果，結果分成兩張表格：</p><p>第一張表格紀錄貼文資訊與互動摘要</p><ul><li>貼文資訊<ul><li>Name：貼文者名稱</li><li>ID：Facebook的客戶編號</li><li>Time：貼文發佈時間</li><li>Content：貼文內容</li></ul></li><li>互動摘要<ul><li>Like：按讚數</li><li>ANGER：生氣數</li><li>HAHA：哈哈數</li><li>commentcount：留言數</li><li>share：分享數</li></ul></li></ul><p>第二張表格則是記錄粉絲留言的資訊。</p><ul><li>留言資訊<ul><li>CommentID：留言人ID</li><li>CommentName：留言人姓名</li><li>CommentTime：留言時間</li><li>CommentContent：留言內容</li><li>Link：貼文連結</li></ul></li></ul><p><img src="/2019/05/01/Crawl-Facebook/02.JPG" alt="02.JPG"></p><h1>過程中曾遇過的坑</h1><p>在這裡先分享一些我在過程中遇到的坑與解決方式</p><ul><li>瀑布式網頁：網頁不會一次把所有貼文都加載給你，當我們把頁面滾動到最下端時才會加載新貼文。<ul><li>解決方案：透過java指令讓視窗滾動到底部</li></ul></li><li>系統彈窗：當我們把頁面往下滾動後，會彈出一個請我們登入或註冊的視窗，阻礙我們爬資料的流程。<ul><li>解決方案：偵測「Not Now」的element位置，並透過程式點擊這個element<br><img src="/2019/05/01/Crawl-Facebook/03.JPG" alt="03.JPG"></li></ul></li><li>心情互動：心情互動分成「讚」、「生氣」與「哈哈」等心情，這邊的坑在於當心情數量大於1才會顯示，若我們單純的設定要抓生氣的心情數量，而該篇文章沒有這個心情，就會顯示錯誤。<ul><li>解決方式：抓資料要運用try-except的方式嘗試抓該項資料，若無法抓這個資料則帶入0</li></ul></li><li>檢視留言：留言不會自動載入我們需要點擊「Comments」後才會顯示留言。<ul><li>解決方式：偵測「Comments」的element位置，並透過程式點擊該element<br><img src="/2019/05/01/Crawl-Facebook/04.JPG" alt="04.JPG"></li></ul></li><li>看更多留言：即使點擊「Comments」後，也只會顯示部分留言，需要反覆點擊「More」後才能不斷加載資料，但問題在於我們不知道到底要點幾次。<ul><li>解決方式：透過while迴圈，偵測頁面上是否還有「More comments」的選項能點選，停止的條件沒有「More comments」後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/05.JPG" alt="05.JPG"></li></ul></li><li>看更多內容：留言中若內容太長，系統只會顯示部分留言，需要點擊「See more」的選項後才會顯示完整訊息。<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/06.JPG" alt="06.JPG"></li></ul></li><li>看更多回覆：除了回應給貼文的留言之外，還有另一種留言是在回應別人的留言。我們也需要將這些留言抓下來<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/07.JPG" alt="07.JPG"></li></ul></li><li>相同element名稱：透過Chrome的檢查功能，我們可以看到我們想要的資訊放在span的timestampContent位置。但是我們如果只輸入這個條件並沒有法辦找出正確的資訊。因為在這個頁面中相同條件的element有許多筆…<ul><li>解決方式：抓資料應用逐層搜索的方式擷取資料，在一開始就設定清楚要抓哪個大區塊中的這個element。<br><img src="/2019/05/01/Crawl-Facebook/08.JPG" alt="08.JPG"><br><img src="/2019/05/01/Crawl-Facebook/09.JPG" alt="09.JPG"></li></ul></li></ul><h1>程式代碼</h1><p>完整的程式代碼會在文末附上，在這裡大家可以先把焦點放在程式碼的理解<br>載入使用套件</p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p><img src="/2019/05/01/Crawl-Facebook/10.JPG" alt="10.JPG"></p><h2 id="搜尋貼文連結"><a class="header-anchor" href="#搜尋貼文連結">¶</a>搜尋貼文連結</h2><p>在這裡我們先定義一個函數，希望把網頁中各篇貼文的連結都找出來!<br>ulr放我們要爬的Facebook網址，n是稍後要送出幾次滾動網頁到底部的命令，藉以加載更多資料。<br><img src="/2019/05/01/Crawl-Facebook/11.JPG" alt="11.JPG"></p><h2 id="展開所有留言"><a class="header-anchor" href="#展開所有留言">¶</a>展開所有留言</h2><p>定義一個展開所有留言的函數，透過while迴圈反覆搜尋與點擊「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕。<br>在過程中會出現請我們登入或註冊的彈跳視窗，但我們不確定到底什麼時候會跳出，因此需要在過程中反覆偵測是有出現這個彈窗，若有就點擊「Not Now」<br><img src="/2019/05/01/Crawl-Facebook/12.JPG" alt="12.JPG"></p><h2 id="擷取貼文資訊與互動摘要"><a class="header-anchor" href="#擷取貼文資訊與互動摘要">¶</a>擷取貼文資訊與互動摘要</h2><p>透過逐層搜索的方式，逐步定位我們要找的資訊<br>在這個環節需要反覆透過Chrome的功能比對資料，需要花一些心力進行比對<br>另外在這部分也使用了大量個try-except，原因是許多資料是有內容才會出現。例如並非每天貼人都會收到「哈哈」、「生氣」的心情。<br><img src="/2019/05/01/Crawl-Facebook/13.JPG" alt="13.JPG"></p><h2 id="擷取粉絲留言資訊"><a class="header-anchor" href="#擷取粉絲留言資訊">¶</a>擷取粉絲留言資訊</h2><p>這邊要留意雖然都是粉絲留言，但實際上分成「回應貼文的留言」與「回應留言的留言」。<br>函數中的第一個迴圈是用來抓「回應貼文的留言」，第二個則是抓「回應留言的留言」。讀者可以自行比較一下兩個迴圈中不同的地方。<br><img src="/2019/05/01/Crawl-Facebook/14.JPG" alt="14.JPG"></p><h1>實作</h1><p>今天要爬的是<a href="https://zh-tw.facebook.com/taiwanmobile/" target="_blank" rel="noopener">台灣大哥大</a>的粉絲團頁面。暫時先設定加載20次資料，若想抓更多/更少資料的話可以自行調整n的數值。<br>注意這裡會透過Selenium開啟一個Chrome瀏覽器，若沒有下載的人可以參考<a href="https://medium.com/@NorthBei/%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%9Dpython-selenium-%E7%B0%A1%E6%98%93%E6%95%99%E5%AD%B8-eade1cd2d12d" target="_blank" rel="noopener">在Windows上安裝Python &amp; Selenium + 簡易教學</a>。<br><img src="/2019/05/01/Crawl-Facebook/15.JPG" alt="15.JPG"><br>接著先創造兩個DataFrame，一個放文章內容，另一個放留言內容。<br>我讓程式自動輸出目前的處理的網址，若有無法抓出的頁面也會送出一個訊息提醒。方便我們後續追蹤哪裡出現錯誤。<br><img src="/2019/05/01/Crawl-Facebook/16.JPG" alt="16.JPG"><br>跑完之後我們就可以看到抓下來的結果囉！<br><img src="/2019/05/01/Crawl-Facebook/17.JPG" alt="17.JPG"><br>將資料保存到桌面，打開檔案的結果在文章的開頭，這裡就不再重複放囉!<br><img src="/2019/05/01/Crawl-Facebook/18.JPG" alt="18.JPG"></p><h1>完整程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re, time, requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FindLinks</span><span class="params">(url, n)</span>:</span></span><br><span class="line">    Links = []</span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 這裡會跳出要我們登入的大畫面，找到「稍後再說」的按鈕並點擊</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">'//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">    soup = BeautifulSoup(driver.page_source)</span><br><span class="line">    posts = soup.findAll(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'clearfix y_c3pyo2ta3'</span>&#125;)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> posts:</span><br><span class="line">        Links.append(<span class="string">'https://www.facebook.com'</span> + i.find(<span class="string">'a'</span>,&#123;<span class="string">'class'</span>:<span class="string">'_5pcq'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'?'</span>,<span class="number">2</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> Links</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(url)</span>:</span></span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//a[@lang="en_US"]'</span>).click()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Now is in EN_US"</span>)</span><br><span class="line">    driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 點擊「comments」，藉以展開留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//a[@data-testid="UFI2CommentsCount/root"]'</span>).click()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.find_element_by_id(<span class="string">'expanding_cta_close_button'</span>).click() </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'There is no comment!'</span>)</span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k != <span class="number">0</span>:</span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> driver.find_elements_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//div[@data-testid="UFI2CommentsList/root_depth_0"]//a[@role="button"]'</span>): </span><br><span class="line">            <span class="comment"># 反覆偵測是否有「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕，若有擇點擊</span></span><br><span class="line">            <span class="keyword">if</span> bool(re.search(<span class="string">'comment|More|Repl'</span>,i.text)) == <span class="keyword">True</span> :</span><br><span class="line">                driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    driver.find_element_by_xpath(<span class="string">'//div[@style="display: block;"]//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'No pupup!'</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    i.click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'Nothing'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文章內容與互動摘要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PostContent</span><span class="params">(soup)</span>:</span></span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># po文人資訊區塊</span></span><br><span class="line">    PosterInfo = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'l_c3pyo2v0u i_c3pynyi2f clearfix'</span>&#125;)</span><br><span class="line">    <span class="comment"># 互動摘要區(讚、留言與分享)</span></span><br><span class="line">    feedback = soup.find(<span class="string">'form'</span>, &#123;<span class="string">'class'</span>:<span class="string">'commentable_item collapsed_comments'</span>&#125;)</span><br><span class="line">    <span class="comment"># 名稱</span></span><br><span class="line">    Name = PosterInfo.find(<span class="string">'img'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="comment"># ID</span></span><br><span class="line">    ID = PosterInfo.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pb8 o_c3pynyi2g _8o _8s lfloat _ohe'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'/?'</span>,<span class="number">2</span>)[<span class="number">0</span>].split(<span class="string">'/'</span>,<span class="number">-1</span>)[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 網址</span></span><br><span class="line">    Link = driver.current_url</span><br><span class="line">    <span class="comment"># 發文時間</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'abbr'</span>).attrs[<span class="string">'title'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_1atc fsm fwn fcg'</span>&#125;).text</span><br><span class="line">    <span class="comment"># 文章內容</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Content = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pbx userContent _3576'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Content = <span class="string">""</span></span><br><span class="line">    <span class="comment"># Like</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Like = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_LIKE'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Like = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># Angry</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ANGER = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_ANGER'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        ANGER = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># HAHA</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        HAHA = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_HAHA'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        HAHA = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># 留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        commentcount = feedback.find(<span class="string">'a'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2CommentsCount/root'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        commentcount = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># 分享</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        share = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_355t _4vn2'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        share = <span class="string">'0'</span> </span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        data = [&#123;<span class="string">'Name'</span>:Name,</span><br><span class="line">                 <span class="string">'ID'</span>:ID,</span><br><span class="line">                 <span class="string">'Link'</span>:Link,</span><br><span class="line">                 <span class="string">'Time'</span>:Time,</span><br><span class="line">                 <span class="string">'Content'</span>:Content,</span><br><span class="line">                 <span class="string">'Like'</span>:Like,</span><br><span class="line">                 <span class="string">'ANGER'</span>:ANGER,</span><br><span class="line">                 <span class="string">"HAHA"</span>:HAHA,</span><br><span class="line">                 <span class="string">'commentcount'</span>:commentcount,</span><br><span class="line">                 <span class="string">'share'</span>:share&#125;],</span><br><span class="line">        columns = [<span class="string">'Name'</span>, <span class="string">'ID'</span>, <span class="string">'Time'</span>, <span class="string">'Content'</span>, <span class="string">'Like'</span>, <span class="string">'ANGER'</span>, <span class="string">'HAHA'</span>, <span class="string">'commentcount'</span>, <span class="string">'share'</span>, <span class="string">'Link'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留言</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CrawlComment</span><span class="params">(soup)</span>:</span></span><br><span class="line">    Comments = pd.DataFrame()</span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 用戶留言區</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 回應貼文的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_0'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg0'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 回應留言的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_1'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg1'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)        </span><br><span class="line">    <span class="keyword">return</span> Comments</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">Links = FindLinks(url = <span class="string">'https://facebook.com/taiwanmobile/'</span>,</span><br><span class="line">                  n = <span class="number">20</span>)</span><br><span class="line">Links</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抓下來所有留言</span></span><br><span class="line">PostsInformation = pd.DataFrame()</span><br><span class="line">PostsComments = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Links:</span><br><span class="line">    print(<span class="string">'Dealing with: '</span> + i)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        expand(i)</span><br><span class="line">        soup = BeautifulSoup(driver.page_source)</span><br><span class="line">        PostsInformation = pd.concat([PostsInformation, PostContent(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">        PostsComments = pd.concat([PostsComments, CrawlComment(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'Load Failed: '</span> + i)</span><br><span class="line"></span><br><span class="line">PostsInformation</span><br><span class="line">PostsComments</span><br><span class="line"></span><br><span class="line">PostsInformation.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsInformation.xlsx'</span>)</span><br><span class="line">PostsComments.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsComments.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;本文僅限於學習使用，請勿用於商業目的&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;上個月參加某公司的職缺面試時，面試長官詢問「有沒有辦法將臉書上的貼文時間、內容、按讚數、留言數與分享數、甚至是粉絲的留言資訊都抓下來？」&lt;/p&gt;
&lt;p&gt;受限於當時還不太熟悉爬蟲，只能簡單回應說透過python與Selenium應該是可以，但卻沒辦法說出更詳細的方法與步驟。後來經過一番研究，總算知道怎麼把這些資料爬來下來了！廢話不多說，我們就開始吧！&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://TLYu0419.github.io/tags/python/"/>
    
      <category term="Crawler" scheme="https://TLYu0419.github.io/tags/Crawler/"/>
    
      <category term="Facebook" scheme="https://TLYu0419.github.io/tags/Facebook/"/>
    
      <category term="Selenium" scheme="https://TLYu0419.github.io/tags/Selenium/"/>
    
  </entry>
  
  <entry>
    <title>爬蟲_104人力銀行工作清單</title>
    <link href="https://TLYu0419.github.io/2019/04/18/Crawl-JobList104/"/>
    <id>https://TLYu0419.github.io/2019/04/18/Crawl-JobList104/</id>
    <published>2019-04-18T14:18:06.000Z</published>
    <updated>2019-04-18T15:30:22.563Z</updated>
    
    <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>找工作是一件很辛苦的事情，當我們在<a href="https://www.104.com.tw/jobs/main/" target="_blank" rel="noopener">104人力銀行</a>的網站上輸入關鍵字查詢職缺時，經常會直接跳出上百、千份的職缺。雖然有很多職缺對於求職者是好事，但這也造成了求職者的大困擾！</p><p>因此我希望能透過Python的爬蟲，一次把我想要查詢的結果(工作內容、地點、薪資、要求技能、工作地點…等等資訊)抓下來，在我自己的電腦上按照我的需求進行篩選，讓我能更有效率的挑選工作並投遞履歷。</p><p>另一方面，如果想了解各個產業會需要哪些工具、技能，也可以從這份資料中進一步的分析。那麼我們就開始吧！</p><a id="more"></a><p>我們直接先看輸入與最終的輸出結果吧!<br>在左邊的圖是104的查詢系統，我在這邊搜尋的關鍵字幾項條件分別是「資料科學」、「台北市」、「最近一個月有更新」等項目，經過搜尋之後，系統幫我查詢到25頁，共730筆職缺。並且也有跟我說各職缺的公司名稱、學歷要求、工作經歷等初步資訊。</p><p>但只有這些資訊是不夠的，當我們看到有興趣的職缺時，還需要進一步的點開超連結，檢視詳細的職務說明(右邊的圖)。在這個分頁中我們就能夠看到詳細的工作內容、條件要求、公司福利與聯繫方式等資訊囉！我最終的目的是希望透過程式自動幫我們這些資訊都整理成一份excel表格！<br><img src="/2019/04/18/Crawl-JobList104/104HomePage.JPG" alt="HomePage"><br>而我們最終的目標就是將所有職缺以及各職缺的內容都整理成這份excel表格，讓我能按照自己的方式篩選資料，提升找工作的效率！<br><img src="/2019/04/18/Crawl-JobList104/JobList.JPG" alt="JobList"></p><h1>載入使用套件</h1><p><img src="/2019/04/18/Crawl-JobList104/01.JPG" alt="01.JPG"></p><h1>設定查詢條件</h1><p>這些查詢條件可以在<a href="https://www.104.com.tw/jobs/search/" target="_blank" rel="noopener">104的搜尋網頁</a>上搜索，在這裡不多做說明<br><img src="/2019/04/18/Crawl-JobList104/02.JPG" alt="02.JPG"></p><h1>展開所有工作清單，後續將依序開始爬蟲</h1><p>這裡會透過Selenium打開一個瀏覽器並開始跑程式~<br><img src="/2019/04/18/Crawl-JobList104/03.JPG" alt="03.JPG"></p><h1>解析爬蟲資料並整理成DataFrame</h1><p>在正式開始爬蟲之前，我預先定義一個函數，專於用來處理「職務類別」這個複選題，稍後將用這個函數將其串接在一起<br><img src="/2019/04/18/Crawl-JobList104/04.JPG" alt="04.JPG"></p><p>開始逐筆爬資料囉!<br><img src="/2019/04/18/Crawl-JobList104/05.JPG" alt="05.JPG"></p><h1>結果</h1><p>這裡爬得很快，大約10分鐘就抓完這700筆資料囉!<br><img src="/2019/04/18/Crawl-JobList104/06.JPG" alt="06.JPG"></p><p><img src="/2019/04/18/Crawl-JobList104/07.JPG" alt="07.JPG"></p><p>因為內容並沒有太難，因此我就不做太多說明了，不過有問題的人也歡迎在底下的留言提出！</p><h1>完整程式代碼</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import re, time, requests</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span><br><span class="line">headers = &#123;&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&apos;&#125;</span><br><span class="line"></span><br><span class="line"># 查詢的關鍵字</span><br><span class="line">my_params = &#123;&apos;ro&apos;:&apos;1&apos;, # 限定全職的工作，如果不限定則輸入0</span><br><span class="line">             &apos;keyword&apos;:&apos;資料科學&apos;, # 想要查詢的關鍵字</span><br><span class="line">             &apos;area&apos;:&apos;6001001000&apos;, # 限定在台北的工作</span><br><span class="line">             &apos;isnew&apos;:&apos;30&apos;, # 只要最近一個月有更新的過的職缺</span><br><span class="line">             &apos;mode&apos;:&apos;l&apos;&#125; # 清單的瀏覽模式</span><br><span class="line"></span><br><span class="line">url = requests.get(&apos;https://www.104.com.tw/jobs/search/?&apos; , my_params, headers = headers).url</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(url)</span><br><span class="line"></span><br><span class="line"># 網頁的設計方式是滑動到下方時，會自動加載新資料，在這裡透過程式送出Java語法幫我們執行「滑到下方」的動作</span><br><span class="line">for i in range(20): </span><br><span class="line">    driver.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight);&apos;)</span><br><span class="line">    time.sleep(0.6)</span><br><span class="line">    </span><br><span class="line"># 自動加載只會加載15次，超過之後必須要點選「手動載入」的按鈕才會繼續載入新資料（可能是防止爬蟲）</span><br><span class="line">k = 1</span><br><span class="line">while k != 0:</span><br><span class="line">    try:</span><br><span class="line">        # 手動載入新資料之後會出現新的more page，舊的就無法再使用，所以要使用最後一個物件</span><br><span class="line">        driver.find_elements_by_class_name(&quot;js-more-page&quot;,)[-1].click() </span><br><span class="line">        # 如果真的找不到，也可以直接找中文!</span><br><span class="line">        # driver.find_element_by_xpath(&quot;//*[contains(text(),&apos;手動載入&apos;)]&quot;).click()</span><br><span class="line">        print(&apos;Click 手動載入，&apos; + &apos;載入第&apos; + str(15 + k) + &apos;頁&apos;)</span><br><span class="line">        k = k+1</span><br><span class="line">        time.sleep(1) # 時間設定太短的話，來不及載入新資料就會跳錯誤</span><br><span class="line">    except:</span><br><span class="line">        k = 0</span><br><span class="line">        print(&apos;No more Job&apos;)</span><br><span class="line"></span><br><span class="line"># 透過BeautifulSoup解析資料</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &apos;html.parser&apos;)</span><br><span class="line">List = soup.findAll(&apos;a&apos;,&#123;&apos;class&apos;:&apos;js-job-link&apos;&#125;)</span><br><span class="line">print(&apos;共有 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line"></span><br><span class="line">def bind(cate):</span><br><span class="line">    k = []</span><br><span class="line">    for i in cate:</span><br><span class="line">        if len(i.text) &gt; 0:</span><br><span class="line">            k.append(i.text)</span><br><span class="line">    return str(k)</span><br><span class="line"></span><br><span class="line">JobList = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">while i &lt; len(List):</span><br><span class="line">    # print(&apos;正在處理第&apos; + str(i) + &apos;筆，共 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line">    content = List[i]</span><br><span class="line">    # 這裡用Try的原因是，有時候爬太快會遭到系統阻擋導致失敗。因此透過這個方式，當我們遇到錯誤時，會重新再爬一次資料！</span><br><span class="line">    try:</span><br><span class="line">        resp = requests.get(&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;))</span><br><span class="line">        soup2 = BeautifulSoup(resp.text,&apos;html.parser&apos;)</span><br><span class="line">        df = pd.DataFrame(</span><br><span class="line">            data = [&#123;</span><br><span class="line">                &apos;公司名稱&apos;:soup2.find(&apos;a&apos;, &#123;&apos;class&apos;:&apos;cn&apos;&#125;).text,</span><br><span class="line">                &apos;工作職稱&apos;:content.attrs[&apos;title&apos;],</span><br><span class="line">                &apos;工作內容&apos;:soup2.find(&apos;p&apos;).text,</span><br><span class="line">                &apos;職務類別&apos;:bind(soup2.findAll(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;cate&apos;&#125;)[0].findAll(&apos;span&apos;)),</span><br><span class="line">                &apos;工作待遇&apos;:soup2.find(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;salary&apos;&#125;).text.split(&apos;\n\n&apos;,2)[0].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;工作性質&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[2].text,</span><br><span class="line">                &apos;上班地點&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[3].text.split(&apos;\n\n&apos;,2)[0].split(&apos;\n&apos;,2)[1].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;管理責任&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[4].text,</span><br><span class="line">                &apos;出差外派&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[5].text,</span><br><span class="line">                &apos;上班時段&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[6].text,</span><br><span class="line">                &apos;休假制度&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[7].text,</span><br><span class="line">                &apos;可上班日&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[8].text,</span><br><span class="line">                &apos;需求人數&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[9].text,</span><br><span class="line">                &apos;接受身份&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[10].text,</span><br><span class="line">                &apos;學歷要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[12].text,</span><br><span class="line">                &apos;工作經歷&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[11].text,</span><br><span class="line">                &apos;語文條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[14].text,</span><br><span class="line">                &apos;擅長工具&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[15].text,</span><br><span class="line">                &apos;工作技能&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[16].text,</span><br><span class="line">                &apos;其他條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[17].text,</span><br><span class="line">                &apos;公司福利&apos;:soup2.select(&apos;div.content &gt; p&apos;)[1].text,</span><br><span class="line">                &apos;科系要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[13].text,</span><br><span class="line">                &apos;聯絡方式&apos;:soup2.select(&apos;div.content&apos;)[3].text.replace(&apos;\n&apos;,&apos;&apos;),</span><br><span class="line">                &apos;連結路徑&apos;:&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;)&#125;],</span><br><span class="line">            columns = [&apos;公司名稱&apos;,&apos;工作職稱&apos;,&apos;工作內容&apos;,&apos;職務類別&apos;,&apos;工作待遇&apos;,&apos;工作性質&apos;,&apos;上班地點&apos;,&apos;管理責任&apos;,&apos;出差外派&apos;,</span><br><span class="line">                       &apos;上班時段&apos;,&apos;休假制度&apos;,&apos;可上班日&apos;,&apos;需求人數&apos;,&apos;接受身份&apos;,&apos;學歷要求&apos;,&apos;工作經歷&apos;,&apos;語文條件&apos;,&apos;擅長工具&apos;,</span><br><span class="line">                       &apos;工作技能&apos;,&apos;其他條件&apos;,&apos;公司福利&apos;,&apos;科系要求&apos;,&apos;聯絡方式&apos;,&apos;連結路徑&apos;])</span><br><span class="line">        JobList = JobList.append(df, ignore_index=True)</span><br><span class="line">        i += 1</span><br><span class="line">        print(&quot;Success and Crawl Next 目前正在爬第&quot; + str(i) + &quot;個職缺資訊&quot;)</span><br><span class="line">        time.sleep(0.5) # 執行完休息0.5秒，避免造成對方主機負擔</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;Fail and Try Again!&quot;)</span><br><span class="line"></span><br><span class="line">JobList</span><br><span class="line"></span><br><span class="line">JobList.to_excel(&apos;C:/Users/TLYu0419/Desktop/JobList2.xlsx&apos;, encoding=&apos;cp950&apos;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;本文僅限於學習使用，請勿用於商業目的&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;找工作是一件很辛苦的事情，當我們在&lt;a href=&quot;https://www.104.com.tw/jobs/main/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;104人力銀行&lt;/a&gt;的網站上輸入關鍵字查詢職缺時，經常會直接跳出上百、千份的職缺。雖然有很多職缺對於求職者是好事，但這也造成了求職者的大困擾！&lt;/p&gt;
&lt;p&gt;因此我希望能透過Python的爬蟲，一次把我想要查詢的結果(工作內容、地點、薪資、要求技能、工作地點…等等資訊)抓下來，在我自己的電腦上按照我的需求進行篩選，讓我能更有效率的挑選工作並投遞履歷。&lt;/p&gt;
&lt;p&gt;另一方面，如果想了解各個產業會需要哪些工具、技能，也可以從這份資料中進一步的分析。那麼我們就開始吧！&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://TLYu0419.github.io/tags/python/"/>
    
      <category term="爬蟲(Crawl)" scheme="https://TLYu0419.github.io/tags/%E7%88%AC%E8%9F%B2-Crawl/"/>
    
      <category term="Selenium" scheme="https://TLYu0419.github.io/tags/Selenium/"/>
    
      <category term="104人力銀行" scheme="https://TLYu0419.github.io/tags/104%E4%BA%BA%E5%8A%9B%E9%8A%80%E8%A1%8C/"/>
    
      <category term="requests" scheme="https://TLYu0419.github.io/tags/requests/"/>
    
      <category term="BeautifulSoup" scheme="https://TLYu0419.github.io/tags/BeautifulSoup/"/>
    
  </entry>
  
  <entry>
    <title>爬蟲 - 爬取松果購物商品資訊</title>
    <link href="https://TLYu0419.github.io/2019/04/07/Crawl-SongGuo/"/>
    <id>https://TLYu0419.github.io/2019/04/07/Crawl-SongGuo/</id>
    <published>2019-04-07T12:53:39.000Z</published>
    <updated>2019-11-19T04:33:01.002Z</updated>
    
    <content type="html"><![CDATA[<p><em>本文章僅限於學習目的使用，請勿用於商業目的</em></p><p><a href="https://www.pcone.com.tw/" target="_blank" rel="noopener">松果購物</a>是台灣新創的電商，截至2019年1月為止，平台上有超過3,600間廠商進駐，商品數量也超過15萬件。僅用了2年的時間就開始<br>我很喜歡創辦人分享以下兩篇創業歷程的文章。有興趣的人從以下文章了解更多松果購物的故事。</p><ul><li><a href="http://tesa.today/article/1629" target="_blank" rel="noopener">松果的創業故事-第1年</a></li><li><a href="https://tesa.today/article/1909" target="_blank" rel="noopener">松果的創業故事-第2年</a></li></ul><p>在這篇文章中，我們要透過python把SongGuo上的商品資料爬下來！</p><a id="more"></a><p><em>再強調一次，請勿用於商業目的。</em><br>這邊文章的架構如下</p><ul><li>找出「男生服飾」下所有的商品連結<br>選擇「男生服飾」的原因是這類型的商品數量較少，我們在抓資料比較不會造成系統負擔。若想抓其他類型商品的人可以自行調整類型的代號</li><li>從商品連結抓出店家/商品的資訊<br>包括店家名稱 / 店家評價 / 商品名稱 / 商品銷售量…等等。具體項目會在後面提及。</li><li>轉換成抓取變動目標的語法<br>將以上抓資料的過程撰寫成簡單的函數，讓我們可以輸入類型的代號，就抓出該類型下的所有商品資訊</li></ul><p>那我們就開始吧！</p><h1>找出「男生服飾」下所有的商品連結</h1><p>我們要練習的是SongGuo上男生服飾類型，這是網頁畫面。若想要練習其他類型的商品，可以自行修改網址中最後的3碼數字。<br><img src="/2019/04/07/Crawl-SongGuo/01.JPG" alt="01"></p><p>首先我們需要載入使用到的相關套件<br><img src="/2019/04/07/Crawl-SongGuo/02.JPG" alt="02"></p><p>設定待爬網頁的網址，與使用者資訊<br><img src="/2019/04/07/Crawl-SongGuo/03.JPG" alt="03"></p><blockquote><p>需要注意的是，若沒有輸入使用者資訊，很容易被系統偵測為爬蟲系統，進而阻止後續的爬蟲作業。因此這裡需要加入這些資訊，藉以模擬真實的瀏覽網頁情境。</p></blockquote><p>接著我們可以透過這段語法找出網頁上所有商品的連結<br><img src="/2019/04/07/Crawl-SongGuo/04.JPG" alt="04"></p><blockquote><p>需要留意的是，透過輸入’a.product-list-item’的方式找出商品連結只限於用載SongGuo的網站。如果想要在其他網站爬資料，則需要檢視個別網站的網站結構。<br>具體的方法是</p><ol><li>透過Chrome網頁開啟目標網站，並在網頁中點選「滑鼠右鍵」的「檢查」功能。</li><li>Ctrl + Shift + I<br>畫面如下<br><img src="/2019/04/07/Crawl-SongGuo/05.JPG" alt="05"></li></ol></blockquote><p>從以上的畫面我們就可以看到，商品的連結就藏在各個Element的href屬性中囉！</p><h1>從商品連結抓出店家/商品的資訊</h1><p>我從中挑選第一個商品連結(/product/info/190117048588)作為範例，商品項目是<a href="https://www.pcone.com.tw/product/info/190117048588" target="_blank" rel="noopener">【瑞典】旅行折疊電熱水壺</a>，網頁畫面如下<br><img src="/2019/04/07/Crawl-SongGuo/06.JPG" alt="06"><br>這個網頁中有相當豐富的資訊，包括店家名稱、店家評價、商品名稱、商品價格…等等<br>我將從網頁中提取出以下項目的資訊</p><ul><li>店家名稱</li><li>店家商品數量</li><li>店家評價</li><li>店家出貨天數</li><li>店家回覆率</li><li>產品名稱</li><li>特價</li><li>原價</li><li>折數</li><li>商品評分</li><li>評價人數</li><li>收藏人數</li><li>提問人數</li><li>商品分類</li><li>商品標籤</li><li>連結</li></ul><p>找出資訊藏在哪個Element與屬性的方法同樣是透過Chrome中的「檢查」功能，以商品名稱作為範例的畫面如下：<br><img src="/2019/04/07/Crawl-SongGuo/07.JPG" alt="07"><br>這邊是找出以上項目的程式代碼<br><img src="/2019/04/07/Crawl-SongGuo/08.JPG" alt="08"><br><img src="/2019/04/07/Crawl-SongGuo/09.JPG" alt="09"></p><blockquote><p>其實客戶的留言也是相當重要的資訊，但礙於篇幅這裡就不多做說明，找出節點與屬性的方法是相同的，有興趣的人可以自行練習。</p></blockquote><p>透過以上的方式，我們確認用程式把商品的資料抓出來是沒問題的！接下來我們只需要把抓取「固定」目標程式語法轉換成「變動」的代號，我們就能自動抓出所有的商品資訊囉！</p><h1>轉換成抓取變動目標的語法</h1><p>簡單整理一下，我們在第一段的輸入是某個商品分類的編號，回傳的是該分類下的所有商品連結。<br>而第二段則是輸入商品的連結，自動幫我們抓出商品的各項資訊。<br>因此我們要透過迴圈，逐一地抓出(第二段語法)所有商品(第一段的產出)資訊</p><p>定義ProdList函數，輸入商品分類的編號，回傳該分類下的商品連結清單<br><img src="/2019/04/07/Crawl-SongGuo/10.JPG" alt="10"></p><p>定義Crawl_SongGuo函數，輸入商品的連結，回傳商品的資訊<br><img src="/2019/04/07/Crawl-SongGuo/11.JPG" alt="11"></p><p>結果以上兩個函數，輸入商品分類的編號，抓取商品的各項屬性，並放在df中<br><img src="/2019/04/07/Crawl-SongGuo/12.JPG" alt="12"></p><blockquote><p>這裡會輸出的商品編號的原因是，我在Crawl_SongGuo函數中刻意加入print()指令，目的是方便我們追蹤程式是否正常執行。</p></blockquote><p>系統執行的效率非常快，不到1分鐘就執行完畢了，我們來呼叫df看一下結果吧！<br><img src="/2019/04/07/Crawl-SongGuo/13.JPG" alt="13"><br>因為畫面的限制，只顯示出第一筆資料，不過我們確定資料已經以DataFrame的格式抓下來囉！<br>接著我們嘗試把資料存成excel來檢視吧！</p><h1>保存資料</h1><p><img src="/2019/04/07/Crawl-SongGuo/14.JPG" alt="14"><br><img src="/2019/04/07/Crawl-SongGuo/15.JPG" alt="15"></p><p>以上次這個爬蟲的說明，有問題可以在以下留言區提問~</p><h1>完整程式待碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入相關套件</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸入爬蟲網址與使用者資訊</span></span><br><span class="line">url = <span class="string">'https://www.pcone.com.tw/product/'</span></span><br><span class="line"><span class="comment"># 男生服飾</span></span><br><span class="line">info = <span class="string">'327'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視是否成功抓到資料</span></span><br><span class="line">resp = requests.get(url + info, headers=headers)  </span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line">a = html.find(<span class="string">'a.product-list-item'</span>)</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挑選第一筆資料作為範例</span></span><br><span class="line">resp = requests.get(<span class="string">'https://www.pcone.com.tw/'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>], headers=headers)</span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 店家名稱</span></span><br><span class="line">html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 店家商品數量</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家評價</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家出貨天數</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家回覆率</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 產品名稱</span></span><br><span class="line">html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 特價</span></span><br><span class="line">html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 原價</span></span><br><span class="line">html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 折數</span></span><br><span class="line">html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品評分</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 評價人數</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 收藏人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 提問人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 商品分類</span></span><br><span class="line">html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品標籤</span></span><br><span class="line">html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品連結</span></span><br><span class="line"><span class="string">'https://www.pcone.com.tw'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義ProdList函數，輸入商品分類編號，輸出該分類下所有商品連結</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ProdList</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(url + str(info), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    <span class="keyword">return</span>(html.find(<span class="string">'a.product-list-item'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義Crawl_SongGuo函數，輸入商品網址，輸出該商品的各項屬性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl_SongGuo</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group(), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    print(re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group())</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">            data=[&#123;</span><br><span class="line">                <span class="string">'店家名稱'</span>:html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'店家商品數量'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家評價'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家出貨天數'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家回覆率'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'產品名稱'</span>:html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'特價'</span>:html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'原價'</span>:html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'折數'</span>:html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品評分'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'評價人數'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'收藏人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'提問人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'商品分類'</span>:html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品標籤'</span>:html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'連結'</span>:<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group()&#125;],</span><br><span class="line">            columns = [<span class="string">'店家名稱'</span>, <span class="string">'店家商品數量'</span>, <span class="string">'店家評價'</span>, <span class="string">'店家出貨天數'</span>, <span class="string">'店家回覆率'</span>,  <span class="string">'產品名稱'</span>, <span class="string">'特價'</span>, <span class="string">'原價'</span>, <span class="string">'折數'</span>,</span><br><span class="line">                       <span class="string">'商品評分'</span>, <span class="string">'評價人數'</span>, <span class="string">'收藏人數'</span>,<span class="string">'提問人數'</span>, <span class="string">'商品分類'</span>, <span class="string">'商品標籤'</span>, <span class="string">'連結'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 組合以上兩個函數，輸入商品分類的編號，即自動爬出所有商品的屬性，並將資料存在df中</span></span><br><span class="line">prodlist = ProdList(<span class="number">327</span>)</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(prodlist)):</span><br><span class="line">    df = df.append(Crawl_SongGuo(prodlist[i]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視抓下來的資料</span></span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將df轉成excel並存在桌面上</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/SongGuo.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;本文章僅限於學習目的使用，請勿用於商業目的&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.pcone.com.tw/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;松果購物&lt;/a&gt;是台灣新創的電商，截至2019年1月為止，平台上有超過3,600間廠商進駐，商品數量也超過15萬件。僅用了2年的時間就開始&lt;br&gt;
我很喜歡創辦人分享以下兩篇創業歷程的文章。有興趣的人從以下文章了解更多松果購物的故事。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://tesa.today/article/1629&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;松果的創業故事-第1年&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tesa.today/article/1909&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;松果的創業故事-第2年&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在這篇文章中，我們要透過python把SongGuo上的商品資料爬下來！&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://TLYu0419.github.io/tags/python/"/>
    
      <category term="爬蟲(Crawl)" scheme="https://TLYu0419.github.io/tags/%E7%88%AC%E8%9F%B2-Crawl/"/>
    
      <category term="SongGuo" scheme="https://TLYu0419.github.io/tags/SongGuo/"/>
    
  </entry>
  
  <entry>
    <title>爬蟲 - 爬取Dcard文章</title>
    <link href="https://TLYu0419.github.io/2019/04/06/Crawl-Dcard/"/>
    <id>https://TLYu0419.github.io/2019/04/06/Crawl-Dcard/</id>
    <published>2019-04-06T10:03:39.000Z</published>
    <updated>2019-04-06T13:36:13.526Z</updated>
    
    <content type="html"><![CDATA[<p><a href="(https://www.dcard.tw/f)">Dcard</a>是非常適合練習爬蟲的網站，<br>除了Dcard台灣熱門的社群網站之外，Dcard也提供了非常便利的API讓我們能從網站上爬下文章。<br>在這篇文章中，我將展示如何透過python爬下Dcard上的文章！</p><a id="more"></a><p>這邊文章的架構如下</p><ul><li>抓取一篇Dcard的文章<br>具體項目如下：編號 / 標題 / 引言 / 內容 / 發布時間 / 更新時間…等等</li><li>一次爬100篇Dcard文章<br>透過系統提供的api，一次抓取100篇熱門文章</li><li>爬超過100篇Dcard文章<br>因為API限制一次最多100篇，在這裡我們透過簡單的迴圈一次爬1000篇文章。</li></ul><p>在這裡我們先練習爬文章內容的方法，若想進一步爬文章底下留言的人，可以參考補充資料中的範例，以下我們就開始練習吧！</p><p>補充資料：<a href="https://medium.com/pyladies-taiwan/%E7%88%AC%E8%9F%B2-%E5%BE%9Edcard%E7%B6%B2%E7%AB%99%E7%9C%8B%E7%88%AC%E8%9F%B2%E5%85%A5%E9%96%80-iii-ded52759d922" target="_blank" rel="noopener">爬蟲-從dcard網站看爬蟲入門-iii</a></p><h1>抓取一篇Dcard的文章</h1><p>我們先隨機挑選一篇Dcard上的文章作為練習，我挑選到的是這篇文章<a href="https://www.dcard.tw/f/funny/p/231030181" target="_blank" rel="noopener">警察閃光get</a>。</p><p>文章在Chrome上的畫面如下<br><img src="/2019/04/06/Crawl-Dcard/01.JPG" alt="01"><br>從網址列中可以看到這篇文章的編號是231030181，因此我們稍後會透過這個編號來爬這篇文章</p><p>首先我們先載入需要使用到的套件<br><img src="/2019/04/06/Crawl-Dcard/02.JPG" alt="02"><br>將這篇文章的編號透過quest套件讀取，並檢視抓下來資料的結構</p><p><img src="/2019/04/06/Crawl-Dcard/03.JPG" alt="03"><br>透過比對網站顯示的內容與上面輸出的資料結構後，我們可以從中發現id即為文章的編號, title是標題, conten則是內容，其他欄位的說明如下表：</p><table><thead><tr><th style="text-align:left">欄位</th><th style="text-align:center">說明</th><th style="text-align:left">備註</th></tr></thead><tbody><tr><td style="text-align:left">ID</td><td style="text-align:center">編號</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">title</td><td style="text-align:center">標題</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">content</td><td style="text-align:center">內容</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">excerpt</td><td style="text-align:center">摘要</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">createdAt</td><td style="text-align:center">發布時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">updatedAt</td><td style="text-align:center">更新時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">commentCount</td><td style="text-align:center">留言數</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">forumName</td><td style="text-align:center">分類</td><td style="text-align:left">中文</td></tr><tr><td style="text-align:left">forumAlias</td><td style="text-align:center">分類</td><td style="text-align:left">英文</td></tr><tr><td style="text-align:left">gender</td><td style="text-align:center">性別</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">likeCount</td><td style="text-align:center">心情數量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">reactions</td><td style="text-align:center">心情細項</td><td style="text-align:left">把以上心情細分為「愛心」、「哈哈」、「跪」、「森77」、「驚訝」等類型</td></tr><tr><td style="text-align:left">topics</td><td style="text-align:center">標籤</td><td style="text-align:left"></td></tr></tbody></table><blockquote><p>在上表中的心情數量是各種心情數量的加總，若想進一步分析各種心情，可以再從reactions欄位提取。</p></blockquote><p>我們來嘗試把資料轉換為DataFrame吧！</p><p><img src="/2019/04/06/Crawl-Dcard/04.JPG" alt="04"></p><p>確認可以透過程式把文章爬下來之後，我們就來寫個簡單的Crawl函數，期望只需要輸入文章的ID後，就回傳爬下來的文章內容！</p><p><img src="/2019/04/06/Crawl-Dcard/05.JPG" alt="05"></p><p>接著我們就透過Crawl來爬文章吧！<br><img src="/2019/04/06/Crawl-Dcard/06.JPG" alt="06"><br>Good!</p><p>確認函數能正常執行!</p><h1>一次爬100篇Dcard文章</h1><p>在這邊我使用dcard提供便利的API，讓我們可以直接快速爬取資料<br><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">dcard API</a><br>以下簡單說明這個網址</p><ul><li>popular參數：若設定為true，表示按照熱門程度排序，若設定為false，則按照發布時間排序</li><li>limit參數：限定在0-100的數值，表示要抓多少文章</li></ul><p><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">https://www.dcard.tw/_api/posts?popular=true&amp;limit=100</a></p><p><img src="/2019/04/06/Crawl-Dcard/07.JPG" alt="07"></p><h1>爬超過100篇Dcard文章</h1><p>由於API限制最多載入100篇文章，如果我們想要爬更多資料，可以透過before參數與迴圈進行!<br><br>before參數後面是接文章的ID，讓我們可以抓取某篇文章之前的資料<br><br>而透過迴圈，我們只需要把之前抓到最後一篇文章的ID放入before參數中，我們就可以抓到這篇文章的前100篇文章。<br><img src="/2019/04/06/Crawl-Dcard/08.JPG" alt="08"></p><h1>保存資料</h1><p>將資料轉換為excel保存到桌面<br><img src="/2019/04/06/Crawl-Dcard/09.JPG" alt="09"><br>用excel檢視抓下來的資料<br><img src="/2019/04/06/Crawl-Dcard/10.JPG" alt="10"></p><h1>完整的程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入使用的套件</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視資料結構</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line"><span class="comment"># 透過request套件抓下這個網址的資料</span></span><br><span class="line">requ = requests.get(url)</span><br><span class="line"><span class="comment"># 初步檢視抓到的資料結構</span></span><br><span class="line">requ.json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將抓下來的資料轉為DataFrame</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line">requ = requests.get(url)</span><br><span class="line">rejs = requ.json()</span><br><span class="line">pd.DataFrame(</span><br><span class="line">    data=</span><br><span class="line">    [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">      <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">      <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">      <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">      <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">      <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">      <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">      <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">      <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">      <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">      <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">      <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">      <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">    columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撰寫簡單的函數，透過輸入文章ID，就輸出文章的資料</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl</span><span class="params">(ID)</span>:</span></span><br><span class="line">    link = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + str(ID)</span><br><span class="line">    requ = requests.get(link)</span><br><span class="line">    rejs = requ.json()</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">        data=</span><br><span class="line">        [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">          <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">          <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">          <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">          <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">          <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">          <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">          <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">          <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">          <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">          <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">          <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">          <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">        columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嘗試使用撰寫出的函數，抓取編號231030181的文章</span></span><br><span class="line">Crawl(<span class="number">231030181</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次讀取100篇最熱門的文章</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100'</span></span><br><span class="line">resq = requests.get(url)</span><br><span class="line">rejs = resq.json()</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">    df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]),ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過迴圈讀取10*100篇文章，若需讀取更多資料，可以將range(10)中的數值提升</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    last = str(int(df.tail(<span class="number">1</span>).ID)) <span class="comment"># 找出爬出資料的最後一筆ID</span></span><br><span class="line">    url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100&amp;before='</span> + last</span><br><span class="line">    resq = requests.get(url)</span><br><span class="line">    rejs = resq.json()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">        df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將資料存到桌面</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/Dcard.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;(https://www.dcard.tw/f)&quot;&gt;Dcard&lt;/a&gt;是非常適合練習爬蟲的網站，&lt;br&gt;
除了Dcard台灣熱門的社群網站之外，Dcard也提供了非常便利的API讓我們能從網站上爬下文章。&lt;br&gt;
在這篇文章中，我將展示如何透過python爬下Dcard上的文章！&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://TLYu0419.github.io/tags/python/"/>
    
      <category term="爬蟲(Crawl)" scheme="https://TLYu0419.github.io/tags/%E7%88%AC%E8%9F%B2-Crawl/"/>
    
      <category term="Dcard" scheme="https://TLYu0419.github.io/tags/Dcard/"/>
    
  </entry>
  
</feed>
