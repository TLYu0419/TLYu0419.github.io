<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Facebook粉絲專頁爬蟲筆記</title>
      <link href="/2020/03/17/Crawl-Facebook-Pages/"/>
      <url>/2020/03/17/Crawl-Facebook-Pages/</url>
      <content type="html"><![CDATA[<p>先前曾發表過 <a href="https://tlyu0419.github.io/2019/05/01/Crawl-Facebook/">網路爬蟲_Facebook粉絲團貼文與留言</a> 的文章，可以透過 Selenium 來抓取 Facebook 粉絲專頁的貼文與留言，但由於 Selenium 的效率實在太慢了，因此這篇文章是記錄我嘗試用 request 抓取資料的筆記，並且於文末附上台灣的2020總統大選 3位候選人從 2019年 1 月至 2020 年 2 月間的貼文與留言資料~</p><a id="more"></a><h1>文章架構</h1><ul><li>使用時機</li><li>抓取貼文</li><li>抓取留言</li><li>後記</li></ul><h1>使用時機</h1><ul><li><p>現在許多網站都是屬於瀑布式(Waterfall)的網頁結構，相對於靜態式的網頁是一次就讀取完全部的資料，而瀑布式網頁則是將很多份的內容轉化為個別獨立的區塊，在滑鼠滾輪向下滾動時不斷的加載新資料，並將新資料附加在網頁的最底端。典型的網站有<a href="https://www.facebook.com/twherohan/" target="_blank" rel="noopener">Facebook</a>、<a href="https://www.104.com.tw/jobs/search/?keyword=%E8%B3%87%E6%96%99%E7%A7%91%E5%AD%B8&amp;order=1&amp;jobsource=2018indexpoc&amp;ro=0" target="_blank" rel="noopener">104人力銀行</a>。</p><p><img src="https://lh3.googleusercontent.com/cVJxdsY25nJinI9_OHsu4m4KBzKm7xXB1BQAP6bpvO7bj4buMJ0CiUrXT0J7e21Qi7Kf0vhGmqIROHISQhwZu9XH0kqoifKT8oYaCqhFP4Q83xZDfvtCb8Fc8tn6P8v9EKjeCGssai_A5lRqmSFVviH1xb50hY8gHoPPPJ_6fP3IUgNNtTgiyXtb2SGcb5Vagml-z8PYn8y5z0ZvjzqVBcIoyZqGYQ5ezzNr7ali3aV4_oOC-okBWzhhSnHVgbaSp5gtkBPjF3p8mdNdtp9ZzvJ3lgIjqLszGEjs_rdzWgXDfmSpjdFBcY4I8Rzycb9_fad5AjcDuXqasKQ_PF9LJdlSXOpU34Rva0yecpU_yYC8cAfD9hP7sKuQyq1VS0sfwcuc4gVHGoX7vbCcD3lcKQ8bI80OZaABqkeUy9cORoYaNtv_7zO7skCeCENWA9oB3UKDewUMzZrPO7j7iajXStiwPEWKNJAavPgItGMJQI_OAQ-jieIvcnQ7vA4S1VnefP1qQdD9_QpFcoR4fTaXAgaun5BuxKe9IvDBNItC-cqZ3a0xGIXKfSEhRkztM2jA5Ib66RgWgEsT34CgyUxY1W6FzdDc_FCcuJga_dnCfDOpOeaeGEHNcNP2wxrlZDqE7mIBtP5YP3dcfLPa_g01yb5QH2NOp8gq2npShQR4UTNzn36HkvMcAU5jtfOBfgHWBOivGkSIWvSH9DG9F7saq2DKul1wQYQT_IRrGvQmkITsZQqWPh7Y52k=w1118-h929-no" alt=""></p></li><li><p>以 Facebook 為例，由於貼文和留言的 ID 都有經過加密，此外也有許多複雜的參數需要輸入。當我們無論怎麼解都解不開時就只好開啟網路爬蟲的大殺器 Selenium，但由於 Selenium 是實際開啟一個瀏覽器來抓資料，因此效能並不會太高，而且網站出於使用者體驗、反爬蟲、廣告等等的目的會設計許多與用戶的互動機制，也會干擾我們的爬蟲工作。因此如果能單純的透過送出 request/post 來獲取資料，就不要透過 Selenium!</p><p><img src="https://lh3.googleusercontent.com/eiKIXh-EPDrU460W2-cyr8nPhi_ehV_c8iooJxylq_keH5Z9Ri3SM6O9lWFI7zIO8EyMm68aNu-ZuDyxoAD8CPlXc-mYlfEkxdPjKcHVChhg6U2CD0LW_m72vnLAQ5j00aO7FsEJm7I7nPXjHcMLypVxgUnbTwXFUTjeBNb5y-mf6V6sZyfK2XsUxcyi7Z4iRuCpuGyFCqVybHf-K9wt8FdLLslUNz3yy8PftjIm-NIEXmgttPcV6P_WLcyB_ZRmPBItFwP4XVS5qp2Nh5DBo3UEoz8Gqc0x44FaQgftWOY4seKQ2uQMZIg5iwI7qAjuxjMP7dVr4urYbAmxNDNTWL0QCnH_Z_J3Sr-d25FMBrwqCwJwi3xoluU5SOglEqdWq1dwnWDsuZE1QXzuhlE1-4jFHAjyY_hdYFgf1fNdd4qdZclIKZWKqTZfNyqAAcddWBxGxWdLF6gRewd2ah1BizOrF3TGOAJrI8klQ9xU5-w46d5iBHjuLbsK9vfKNjHODx1DvCHi-BANZNCJiJTWi94_aurriLFDlUPwe6vM_Y3V457-uxtVJskmJoCZpZrEKtE1WFnBKNm7UUh5L_rZekKX-q89hNu-9bnEdNpeyvOdBTL3cVVlCtVsBjbtnuMiipgeg6ZXM65DfqRHiMi-ZLz3t_xzxggMyl3I45_IjTqrp-iZ8ggIjCsIBp8el27VWcwtQINcrD6GGIIEMO9YkUfV8Bl59ZIoixWuFDU-ZPkQ7xEwbfetpZI=w1116-h929-no" alt=""></p></li><li><p>但是 request/post也會有缺點，因為網站在互動時會有許多參數需要發送給伺服器，比方說告訴對方的伺服器要抓哪個粉絲專頁，每次加載新貼文時也要記錄上次加載的最後一篇貼文的ID，這次要加載幾篇貼文；加載貼文的留言時也需要告訴對方的伺服器說我們要抓哪篇貼文的ID，上次加載到哪則留言以及這次要加載多少則留言…等等資訊。</p></li></ul><h1>抓取貼文</h1><ul><li>我們可以透過 Chrome 的 F12 的檢查功能追蹤網站在互動的過程中背後送出哪些 request/post來跟伺服器互動。下圖是我先開啟粉絲專頁，開啟 F12 的 Network 頁籤，並且不斷網下滑動頁面，加載更多資料後的結果<br><img src="https://lh3.googleusercontent.com/lquFOWxtMzAtvZGGTbhRikbE8PN8TVr-0OQB-f1uzsIT6AK7ccVnXCzksZ8wpmclIxYA3xrtB8KTB_LMR54876a_SSaFXo2ib7U2iR77KJlOlDN6z5nFz2CAFI8KoEBkudFf_nBQM_Ey5Ekddc81FxAfWfTtvSZHQeHFAiYVHZv05SOB4pisYJd4BwuNSZ05AIfwzXhjp_9ckhFhPlHlKCr5MGjm7m4GTrJVaFzNBMEylPhJVCbgsS9DM2sffxsQkmPgnDWwijM-pRxRsJSHbo9bYtVngUQN2fnNirpCAJUT06nKX-dK2s6VjtQDLHiR6766jyqbOz5ZV3f-epzJuG-HD5n5MwhwpZLdnZQGbYRjDYFtW70eqF8d7T04kRcIS_-jWH6eXiHtuEEm7GouKWPwJ6vGhHstAqVRxK5C0xN_sOALw59O0dk1KaKyP8aKekeWtl12V8P0rLV_ENfRwESfyRzaaMPuZupx046gPYLsJiNguwmibBAsgW-CYc3awDtftsTdGKGKvhWn_1ooh5uumpb2iE_dUdjaQStMqchtDdYfhcoXA2xNVJWWnc54PxORWgae6qUwlbszAuHiH76Dn8wEmgzZLn_iQko220tAFT3ff4TTHrVm7su4oW391rhSfqb5w9GN9nK6Ds-lQVx9r8w16uMB75n8IDHFH5AvNc7RzM5T-1zg0PYN8NsX2Bcp6Rtfy_C5iHYwsPpeeOoggb9HjQ4ijGIdgNauhcTsvxIEt5N0KPs=w1698-h929-no" alt=""></li><li>點開其中一個請求(request/post)後，首先我們可以看到這裡是透過 get 的方式來取得資料，，但是裡面卻有相當複雜的參數(如下圖)<br><img src="https://lh3.googleusercontent.com/BGtXQX5cUyobE9-22bHmY5YjaniiSG7s1HJUV3c3xYZkIXoQP1YNwoAgjRjAhElqmZ4mUShOiTCNGquMTqeXgoCr8NeCYz-5AOW1qTse7L4ZfCAHRIofWSKtb2SlzAyx_KlVyxV0ZvblhFS5hwwcp3FoOUNUUb7MQ62p-XBowpeceqtJRUDAWpqbcLl6YVX5-TaEvkeA01uxpUvLxlvyTeG43QZzc8l4rTk_KZrdnUFSg6uhdnho7Ld-djHI8TFNW3BTcvEzf8Ml8uMmzjlBMNllQDc4uamTloGAMFreDsoi8AAGgT4RVYSV--iknw2gwxNXGRVc6q6nJNZY6OHogGIjziXZuoegyosI-xLpwIcaolBPTxbUu7dSIuoSqCEeE0F22wpulf7GhDnHzX6dJb1EDuFtDdpU8YS37NzCEDz3hibyQpDkvkze62h1veJIm27itdKsT3szO-yOovzghD5KBLI0G316FCFNVKEENypCFetw8SDKgbGQ80tiegcKFw3qGkJOd3q4Afl7Mg-7OsDweRj3iYXSLclsQueYAW1qLGXP6RxZES04DqynUv-2MJgaZIcTUMEhYE3hJAoowi6QNnWaeUakL8HmInunDZlFt8jq3VkVQqsuuqPOeFpnwqlDHB-5IB5TkWqaSIcNp8fInNrTW1X2I9JovJ7v5zHp0Hq0YLa-S1uRivb2TSrVSbdllnK1NNaQc7reS5xx7aATV816cupqokN5CMFbFGry9A8KEW2Znm8=w852-h929-no" alt=""></li><li>如果你跟我一樣看到Request URL中的參數就眼花了的話…別擔心，我們可以把畫面滑到最下面，這時候我們就可以看到比較乾淨、整齊的資料了，在這裡簡單說明幾個重要的參數<ul><li>pageid: 用戶在 Facebook 的 ID</li><li>cursor<ul><li>timeline_cursor: 前次加載資料最後的貼文編號，這次會從下一篇開始繼續抓</li><li>has_next_page: 有沒有更多的貼文</li></ul></li><li>unit_count: 這次要加載幾篇貼文<br><img src="https://lh3.googleusercontent.com/jX2Tdmpmcfp83g9JoxhTgE2daBeAbHYFJzjmsRhZdCYjFSt1LE4rbJX54wR3T_W5F1hS5Y3EkwFmWdIU5c5QVKbDRYtdaRP-nCwY2CKUcgJE1DDULskChYoCEtqg602oBZ_Aln-flFCxxl_CK7OGew9A1y-rGVT5fKHIsl7pR6XUi-GnWnZLFqwUhTHnrwuNJvRoHq40Edpzgb3UTjXOwk7qiae6vl2_k71lJpgWWsqBCmJS7CrUAmGUaNISXZmbsluTBbmG__KHNardGSIHsllOaymXCFearms1WUrtk_bGyQtluIkRLw8vCkldS37PD70SRuUHPR3O027Y4wCXwtq-WmdfAQcZjDG6WI2cCrS8z3Gr1EGe8veZN4wsszmzok9b8iLmElaeMrOcRxlUqNH2RkSABSyzwnrjzvZSXwKqBwgzczW2w5hqGqRRRo_pxusVgjvgdgtMiA3Bu97o42eyS2lM5TWt0W4mFsuvYH-5kB7DwQd9b_I0HGxIIcUA1traf0dU3K9ptA6bVmADBonxIqhh5Di11Azt-k-spp9RIExlq3Dn6uRtheqR6sm9w05u9l-WhP6W2rxOD_p8FRLvp1fVVpbZS4rqo441velV1RAPEXSeEkM7YkN7ASPIih3oWhjCgDtzU1HgyUHQ7GEk8_0ZJHZS9LRbxldlrnAY77xQ9tCrEEATzvpJ1pjTwfVbLcs8Vak_b5Cz8uhwon8m7d4XH0rmoMuy0PR_U8tiDAlvKYGBuSo=w854-h929-no" alt=""></li></ul></li><li>接著透過反覆 get 資料，並且更新 timeline_cursor 的參數，我們就可以一直抓到貼文的資料囉!</li></ul><h1>抓取留言</h1><ul><li>抓取留言的方式也與上面同樣是透過 Chrome 的 F12 的檢查功能追蹤網站在互動的過程中背後送出哪些 request/post來跟伺服器互動。下圖是我先開啟一則貼文，開啟 F12 的 Network 頁籤，並且不斷點擊「查看更多留言」後的結果<br><img src="https://lh3.googleusercontent.com/3kBse9YvxO25CYuamJQdL48fDv7evBfZhQTrPh9rfXpoVy8vN0cjVB3GXVpou2OyqDbgglLGo2ltC04A0V3sE2nS7HzuykxnI6q673iVgABBsMX9Fgk_OyMUqAVn0zskSuS2DvTOJQ6IfACmmpR9dRHxASCKXQliFwpkpn2AMgAVoswMoRFJMw_6RkGZbAXwiN-hr_Tw8ortF1M5bdjcMquReIs1aSPewaFs3Az3eBygrFlBb33ZqdDrJ_KdpEg74HN-oIfBlelAxjBwJnOtFTGfZIPpxZS1GYzeqEFjlUvFcuLni9PyxnNp2M9v0dWWwTBz4kButXWh8_Xyw5saBN0_z8jBnkxXIPQRZKZ9tx9lm1Uccy-ts2hja4YzXWYCr4klSwgdOLgD4tpAMt0isE7I0JGzlMJCAMY7PdQW_sc-5v8bJkH3J3W2JTyhLxJzLL3aKa8uV2V8BMwbjP-D-xIrqGBq4iZaPeEzAcx0zC6znWx94w8lu3ioZwUfJHXN51YLmlYswCwCpeo3r9CkDpgHdDLaWywC8r_wV0LWAn5tmYjay1vagdJ9ktQtv-areKBpebjOzAuWy4sLrJo65RYeaTkn-Zs9A87G8rTcvU_dPdn16pGfbhmCTBlpA7qruN4gLskaBisal3aPZYNOH_DlwFpUB_kRYW4j_l5tVqv1cRILClcgTyZWOaBZhzYnCq1ZCgPqqeh8vcujW_xh8P0ARhUkD7P5YL9YeqlY5T074JrZdhPFjEA=w1696-h929-no" alt=""></li><li>點開其中一個請求(request/post)後，跟抓貼文不同的是這裡是透過 post 的方式來取得資料(如下圖)<br><img src="https://lh3.googleusercontent.com/nKPUa9qKvfQTzzyHWtaWfKnOwzROswD85miOeKIMcnvVM1H1O0IHltMxm7J0z19gR46mAIIofhLkmXrLZlewtwU9lDYhzUbIfKpmCG-x0LAE0s01_4GXp6-_w3qGfLQJlHgxN0nRTlhOFIels2sToMgokJYnVYK_lpRol9LSJe8dNKbMP_RNfZVNgUdZn9fU-yIO0hyI85XF1hrP4e8PqdRxgguPoMFGavHbkW3b0-sA_vLLZkhGcPY3_Bs2J80PlQRzKJzMce6l_D__EnqebQARPKepGKRMSwOy79qU5sTTPiWx3qCv3Pmlsta8mdboJgiZh_dir87htEYk7_YCQ6UhAHOvuYmt7Nnv1OmbuA8a2Ejrc-ZYQiknLlSshQUAhkhAFkDuJdiFhHv04KrCIEizP51u4eEZC2lV3cYdbxwrthbtbVbe0gCz-SEUwNlGOdYWXExYBL_k8accHQGDznwdvZTU84KnKAzoNimx1ENDzWeg2vcuOrDcdWUSh-LjcdON5MCiY9EW0eRscADxtrB6ODstEcNSW3-ALAUcOIzK_OupcY67mTaQChwpyjCgL3m6tLB4NJwE-tv1lczbleOeZarUfX1QOsPPcnV4pdpMjN3WO5eTaeIdeg7j7lGBJYBGnJhTQ-jK3m_Pnf4mEXknAOhYKxNKx6Pu6ssuW0RZ-1IYG-vCDTzPkj5aN914FY0Um0eu_sN5YvFxMODI5dCvLLCeecHlm1CxgjYngVLkhlX-VPnLno0=w847-h929-no" alt=""></li><li>我們同樣把畫面滑到最下面，看到裡面也有相當多複雜的參數，簡單說明幾個重要的參數如下<ul><li>variable<ul><li>after： 前次加載資料最後的留言編號，這次會從下一篇開始繼續抓</li><li>feedbackID: 留言的ID，如果要抓取這個留言(Comment)的回覆(Reply)，會需要記錄下這個資料</li><li>first: 要抓多少筆資料，最多可以設置50</li><li>viewOption: 留言的排序方式，有相關程度、最新到最舊與所有留言，預設是相關程度，有需要其他排序方式的人，可以自行透過 F12 觀察要輸入的值是什麼</li></ul></li><li>docid: 設定成畫面中的數值即可，沒有放會抓不到資料<br><img src="https://lh3.googleusercontent.com/HwVy6hgv1cIaV-bsqpggVKIbioHOAAvFHDYRA5nwaRSCERE18fthrIEliwvdPMpnF8iyQCdUt6HsC0nT3mbFyXVwBm9tFULzN0oD7O2oSjrMLBUr7LKlKzl_GoGOCIVeEl6ct9jKo8eO78hSiFgr698oPBQyO6TQxUC3qN6PZNxVaT-DKWjQWenLodkrblduc94EXnwEMqT98Xja0nprsFSm_FJKQd9ctweJcrYqm5Ndv2fCzAcIeavK0o23ng_BBiTBXi6GFTsEQo2AjfIBvp5huTmG2ULDcvLqJFzFCJwOtCoEcfq_vM-qP8KOyju7KB35IYOsVNY9Q-o4jjFyGZ_ullXxkPOcr-eiWOcdo8_7HycA_4gL_EY4phA1549igwtI6T1Bt1xYWZnEcLJQgSdufO-VkP2HwZvLTH9-DxBMFSEXHUKXdDILfsCUwrNKreEoprgJwxHwve7sH4mQ8DgIepMhcmxiy-vXxq6hzy52gBFoA4Uqah6Aku2zTctdYCCH5g5c7I0VX8wIWeNKqZYbZ977iJFyMY4LJPr1I99cJJg2du2UQV-21EENhTFN90xbKUaS2IpLa-8xXGJlMr157V2SJSIqsojRfSJ42yXoxFfHAva9iaSPvW8K0maS8s8dTMze-7hwp825PMM_ft9c2_BlbhBKbGDbsVj1nKRFjFuLr9C0CZ2y7fx29gC3C2byE4eVP9wTtchw-XVm5-4_1uLukfOz2oqXjQrkdl2MkgtxxVVj4wM=w849-h929-no" alt=""></li></ul></li><li>接著透過反覆 post 資料，並且更新 after 的參數，我們就可以抓到留言的資料囉!</li></ul><h1>後記</h1><ul><li>當我們成功從對方的伺服器抓到資料後，我們取得的會是 json 格式的資料，屆時還需要再利用 json.loads, BeautifulSoup 與 正則表達式 慢慢剖析取得的資料才可以將資料整理成DataFrame!</li><li>以下是我抓取台灣2020總統大選的3位候選人(民進黨的蔡英文、國名黨的韓國瑜以及親民黨的宋楚瑜)的粉絲頁資料，時間範圍是 2019-01-01 至 2020-03-09 的貼文和留言資料。共計有 1,700 篇貼文與 355 萬則留言，欄位的說明如下<ul><li><p>貼文</p><ul><li>NAME：姓名</li><li>PAGEID：粉絲專頁ID</li><li>POSTID：貼文ID</li><li>TIME：發文時間</li><li>CONTENT：貼文內容</li><li>TYPENAME：貼文類型</li><li>COMMENT_COUNT：留言數(包含回應留言的回覆)</li><li>DISPLAY_COMMENTS：留言數(單純回應貼文的留言)</li><li>SHARE_COUNT：分享數</li><li>FEEDBACKTARGETID：貼文ID</li><li>ANGER：怒心情數</li><li>HAHA：哈哈心情數</li><li>LIKE：讚心情數</li><li>LOVE：大心心情數</li><li>SORRY：嗚心情數</li><li>WOW：哇心情數</li><li>UPDATETIME：抓取資料的時間<br><img src="https://lh3.googleusercontent.com/SDCISaMhAgQaA2Z1E8eLyZKBOiHS8CyZbOeDDAPvKvcsDtGLt-A0islLxu8c38oYoNkPvO8uCjRKSQnxQILMlmhAtTCShOWVMomEdz3gQunhMgS_rsXYQxASvMJj4KcUq1Mvm0bZK_e-o7i0y0iFGvqpscKydrgIqiCE1QeYkCqmPH34F7GdQqNwfqVm2wz6UYuMhsgYNNZO0Pc3s9zJwQzQ4ZlV1Egcf25awcIbSJq57OZP_94EpR6VgJ2XmSdktjsHtvK86FYWQMiTTVDnLizLBhaICuFK75yK9rWGkhysY0tyNpZ_fZovIgTEoNGV1mUHGr3l8stJSq1yLei8Cu6GkUp6Vd3o_yhYPaoBJwc5R_5h7IZiECIuzSJ_Fz5cuBetNhxZRhN6ZWtwZw-QHh65J_3obaYUfi-E1tEiWtXDySY1_iewisn3D_P7yweBfUOVEC-OKE6tgQu5qRJIaqUYLSD_vo8I90Wung51YOHDCBVsKjr7HtwcP5HxGblCAKBN_iD-7coXIkjcX3nNUa0R1MWT83MzO2T7S5yECzJBzC4YV7SCg0eDTTUzTQDKPAyFmIkl8fVVZNuwJVnOxkOTsZ9x1QL39Pac3whhEnxOOopY62x58iGFsMGcpFp-QG41l2zB2k47Db5R2gJrNnW_cf5clq7Ky_4DULAiGlTb5tj7uLGNzFf69PB9wy5KBo9J2JFtlYQT6q-F7rSJCSNOyNYZ_8BWK52coliIbTgQW06dk5jVHbo=w1847-h822-no" alt=""></li></ul></li><li><p>留言</p><ul><li>NAME：留言人姓名</li><li>AUTHOR：留言人ID</li><li>TIME：留言時間</li><li>TEXT：留言內容</li><li>TYPENAME：留言類型</li><li>DISPLAY_COMMENTS:留言下的回覆數量</li><li>FEEDBACKTARGETID：該則留言的ID</li><li>REACTORS：心情互動數量(Like, Wow, Sad…)</li><li>COMMENTID:該則留言的ID</li><li>POSTID:貼文的ID</li><li>SHARE_FBID：該則留言的ID</li><li>UPDATETIME：抓取資料的時間<br><img src="https://lh3.googleusercontent.com/Khu49E5W45JG8gvzz1YD5An01Rf_iq9kl8FHLFt0Xa3Hxk5LZmtZzVtFjYLT_0CC8RsfUs0pbwWNzGz6IJ-bjH-XcJQ2EF1A0hXE_4upfhRxSGq-o1ZK25z7Hnw5JwYvft6cCi-VOIQZaUMHnN8PvHLzDr7Ghv8dk1O8Q61joi5p3MdyUpo9ISrKNPBr8_D98Jl8grYgDeeGyR-wt_jfWabZstR9dKuCmb8COQC5pVvgmliev0oT_lvQ2lbRTWYvgbm9IxI2E-JhToDdTMdDAsuW-5v_QePy82g4jKst40Yw-311LVoBnSMAd2UH6zs5kXNa2CelLUiSpI_AQCuoR7ivb8mkBq7tE7UDnvOhduzNetayY9G-u_adgYhJqKVJuANKM_mV_sMzVQcQuvnAI-jNCMiqWJSD7b5Ne1xg9RY14YEBynh09H5G7TXkP8PmejVDwyRR4HK9box9mdvZBVam7bGvozedrSpDRynCoE2NOr9-rmjeeBiwBUOekJv4HF9Yl_2KTkFBG6GY95onp6cHkn-2U09cf1Tajq0sqrMNfBTxXG96wW2OxzeDRFeW5YbeY-dvWr6-AsGy30tS9wn3txHTZ9c3Yjb8IyBv1ODXz4x4iqkP9yrktH4lQHVvh5qhTozfbZC6-y7U8kYG5eY1VKEHxlnBxFiUiKwKQ27P2nzt7fgiJC_ZKccIF1ObEQDgcdZhqbXkhrPd2YFFXiOY5kkt6JerrDR5r6Em9nJnHkler9PH5FA=w1845-h830-no" alt=""></li></ul></li></ul></li><li>由於抓取這些資料比較敏感，就不提供寫好的程式了，有需要的人可以直接在下載我抓好的資料，有其他需求的人再請來信跟我聯繫。<ul><li>由於 Excel 有約100萬筆資料的限制，因此我將資料存成 pickle 的格式，有需要的人再透過pd.read_pickle 的方式讀取資料即可!<ul><li><a href="https://drive.google.com/file/d/1iWHXZmab-D46rQDuiSGcLeBBTrnHfavH/view?usp=sharing" target="_blank" rel="noopener">貼文資料</a></li><li><a href="https://drive.google.com/file/d/13URXpiEEHefjIt10-rAAoGTt9Jcg6tp_/view?usp=sharing" target="_blank" rel="noopener">留言資料</a></li></ul></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Facebook </tag>
            
            <tag> requests </tag>
            
            <tag> Web Crawler </tag>
            
            <tag> 總統大選 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>台灣政治人物 Facebook 粉絲專頁清單</title>
      <link href="/2020/02/27/Crawl-PageBoard/"/>
      <url>/2020/02/27/Crawl-PageBoard/</url>
      <content type="html"><![CDATA[<p>最近想要做Facebook上有關政治的社群網絡分析，所以寫了一個 Facebook粉絲專頁的網絡爬蟲程式。但是當我寫完程式後又出現了下個問題，就是「政治人物有這麽多，要怎麼收集這些人的粉絲頁連結清單呢?」</p><a id="more"></a><h1>文章架構</h1><ul><li>確認目標網站</li><li>觀察網站結構</li><li>開發爬蟲程式</li><li>後記</li></ul><h1>確認目標網站</h1><ul><li><p>經過一番搜索後，我發現 <a href="http://page.board.tw/rank.php?tagid=24" target="_blank" rel="noopener">FB專頁儀表板</a> 這個網站已經幫我們整理好了這些資料，上面共有 742 位政治人物粉絲專頁的名稱、連結、討論人數、變化趨勢與標籤，因此我們就不用從頭開始手動整理資料囉!</p><p><img src="https://lh3.googleusercontent.com/JNckJySxn8TVQha91Sb-kSDm6b4_fejRXYyqJBtKcTynzgbVczC0F1xH-_d-Yjt81gkigrFXFyLyXOrgLgcHynx8I8UDdMz4a1qxdOKut3B6M5LNS3gavs7LIbTy8JELgmas_nyg9lqpA3qWERkDYxorItvcrb8OQO0RkcfdIFUUJbXoivDgsGg3dNbxe9i_lScmtrYeIHGZ-rGYLHOrgZJ1La6MZi-gHq7D7qUJkHRXelTpxXN3q-tpduYkvm3WQk4u8dxghVjvzdiIyfr1b5E0BKCM2Q3HIMzpUBQyKH65VL6GwcYf0ASvfgB8wjTPttrxQrpFklgRxsOh0IIBXriqXQiPri4_WyzdJ4mUC23gervKE_SRaqVT88s9tOJWRWg0vy5EPQ03VhPrEsxLgmYOu8RtoiT6NRyJX2NwbzbNM0OQI03-kcBbgQMa53v-L7fwskUT757P0ffFX5rPtsFf38zcGMCKNsv1pdUHdqbWEm6DKPCCrsCFGF94-Jdk6NfiLUNbEy2pvjhwdA7ZT9BV1jyZurbJ6khpCcXLTSK_s_QRtQLd6ChqoAIgolCPjLOSNZE-pwMnikF3PGNzkzDDeLrMEBG-P6muC6v5TAhipbrnm0pFTv9I_mNWwVI3r-NQAN0LrPPv2qayubCF06oW-NlBmulGmQjjcf57qmzXMVi8UxJT5JOs4bGrngEXA5E0XaPmRQjZT-_s54TPEdl_woR07uYUEvv04uWv__udrwYI=w1358-h978-no" alt=""></p></li><li><p>那麼下一個問題就是我們要怎麼透過網路爬蟲把表格中的資料抓下來囉!</p></li></ul><h1>觀察網站結構</h1><ul><li><p>載入使用套件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure></li><li><p>送出 request 請求資料</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://page.board.tw/rank.php?tagid=24'</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line">soup = BeautifulSoup(resp.text)</span><br><span class="line">soup</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/ED9wGezykG9ACLoWe4ZRSkwFsS86RbtxQFcyp-M6ekL9CT_2cczxghBCKp_YSn5BFi6wU4FNeDQO9hM2mbVytDtlXmNHIj5EA0VCbnhdEnaEdOVBMcUxzX7_--ByJtwhOmoi9geUz1wSlrim7kuQun2Rh5U1PO3sZBy-qFu3VB1yHKVFcTYHQAKOaQCZnG3B0PRFy-aGm7Elr0_0APM40bMz52Q1bcpzO1mW7zCUf83_WIVfpQPzW1V-_JRS3mQE4hBEvj4OVL3VLUYskS-9QhgxedyCkIqhbU8PnWvh_Y22RF7u3TWUThHh-f9E1ZYK5H29o6LKYd48Z97z_c6Ik5GShYnXLMdDVk0gFzwpsUiM8ogeKGsZtcOiOXDCM1sfFvnI8KUhWnvvOc0nLgl2Okuc-FaMFPEciZalXL2hO70KC-3BA8whw7EGoo2K_h75GXO94KvLQX8NSi47u1pVl58rMq1m8-OXSEF7RneUfzaG0nBHngFqcosJTBYE-rcUyPRZxxfRnar29GfCGU2JIuh1GaxJgMEjRBgkANvsk3_4mLcPRSXJgoBXSykDZ_D97UuF6tlT5CBpqeXLeYzxMd61DezK2F2wjOxForG_2q3LClTelLSpobMV3ohl5skTCdAcfEf4SQUQCWKV3u78uid1VkLbhZLm7_e9QjRXDj2U8lq4f2Xehd5tArIVI8ZLqQ3Y4WYiJp4BsC6Ewlet_7DrD84We1UI9nFZRh4e8sGF9F1R=w1852-h764-no" alt=""></p><ul><li>結果發現網站的結構非常非常單純，資料也都放在 tr, td, a 等等 element 中，因此我們只需要逐一取出 element 中的資料就可以抓到我們要的資料囉!</li></ul></li></ul><h1>開發爬蟲程式</h1><ul><li><p>因為網站的結構很單純，這個爬蟲程式非常快就寫完了，具體語法如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'ui container inside'</span>&#125;).find_all(<span class="string">'tr'</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ndf = pd.DataFrame(data = [&#123;<span class="string">'名稱'</span>:i.find(<span class="string">'a'</span>).text,</span><br><span class="line">                                    <span class="string">'連結'</span>:i.find(<span class="string">'a'</span>)[<span class="string">'href'</span>],</span><br><span class="line">                                    <span class="string">'討論人數'</span>:i.find_all(<span class="string">'a'</span>)[<span class="number">1</span>].text,</span><br><span class="line">                                    <span class="string">'變化'</span>:i.find(<span class="string">'div'</span>).text,</span><br><span class="line">                                    <span class="string">'標籤'</span>: <span class="string">', '</span>.join([i.text <span class="keyword">for</span> i <span class="keyword">in</span> i.find_all(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'ui teal tag label'</span>&#125;)])&#125;],</span><br><span class="line">                           columns = [<span class="string">'名稱'</span>, <span class="string">'連結'</span>, <span class="string">'討論人數'</span>, <span class="string">'變化'</span>, <span class="string">'標籤'</span>])</span><br><span class="line">        df.append(ndf)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">df = pd.concat(df, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/nB1_aowklDYVNF2K9GLQARx32UGE18JWg-sRzFNpwYWTqAdn5IvZo4LkKfM4LL60KcOaahZt-W0UHxaf6Ju3qwAzq6HmmTblOsi8fmOqqBKGWV6DPHltOQX0QAkrue3QBcga3B_S2yArBoxy4xHJdeOn0UAZRA2LlHNR9gxHGgBTwEahixO4phEc3R33sOtFKpLbNHD_WGvXrl1dG2x5wHBvAwxhFFHfxdKKA77_4bwjfiNn8H2VfKqNQHG5NotNw6-owLsjaajBlx8GnyzQfyMNX03emXqo2t0M_Sb7q5iSY74yAXCfIbfyXweOTGA88VxBngNYIDQTSDvI_l3HiFrvN4umf2fq3t_6y49tmUAmQzrFNResJYgtlu9Z_qV35UZft2d2LTcPJXDaeEsoNQKivNU4A57PjW-4bXH0-n57qHWafejy2511gbMsy5pX01nwWCzTkWAMds89j_LGMWOjf8sHZtWEALFNEyKlLUAkb7LXhclFFT0H1lnZsvgzIFozfbur5gnoIbWfvYf7nzZLueQTS0zS9N8cleBCnWP81Nxkvv7DgLU6yGwpzHylis6rXSjjSSfoZzadMOEiwh2HISw-Y6jhaPK6YDq2r-oqUUVnLWZOzeWSs6-Tcr2HeXzoFq_gwL4a3EaRA-Ll01wVZULSOtplvIIzTEWMVZEe0JWs_4D_fp_HIQh-kZG8hf_rRRtz9QoNiUFnYo-c-X_Pyr46BAVRPFF9RboRYseeMrbN=w1199-h705-no" alt=""></p></li></ul><h1>保存資料</h1><ul><li><p>將資料存在 Google Drive 中的 Colab Notebooks 資料夾</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path = <span class="string">'/content/drive/My Drive/Colab Notebooks'</span></span><br><span class="line">os.listdir(path)</span><br><span class="line">df.to_excel(path+<span class="string">'fanspages.xlsx'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/Lgi4vg4DHo5gB6-fP_DnJAGc4tTlp8UudTWo5sKA8ozcQyPV9EOkgAKAjyhtOi9VSnroXNGz35yr4pxpaa8bkBhPhq1Z3aAS-yYNW45zigeVXvwf28hmL2B6Jjrsk8S-_B2QQ6INi3riUERnyKar4fh5tAV1-zvh63OrAXPrVDCq7SGuMeYp82oPGuzYOZQhj-nz6FFDwdjVwUs5As0aLsV3B4GyqLB3mydA_7vzeF6agrl5kezDZyHrY2xvv3GXUKnOdBNm6CICPR3VZqWBN9H-f2A2CxaTU36q395ol_ph7mTqaTb8CYvAgzxkg98FvecZwijF8iZyN7ioiJkNot9iewqAkow3aMxwMm-Ot8fshcS32ce6-yBwFAJpSynzh4Cv8jXJ6VqO2HcyKd4EZKfINAwdDdfLRF5sD_utpUbSLJiS4DuqkIk_FGU0OQj2abQYg8b021th3hbeMUZoDVelGh_K3ySrzlT9WI8yWu-BCWfpbf5jDP_Mn4uDH9bLkpd1OGAedbkJxTCLxXE6lpKkRL3D-BedCGEJllXvo4Nq2N7K9vWIEwx-gi36try6aJUUD2ovFxLsQ6dflftK9kIn3vF49PhsWnJFRxtdyuIOFpqFRTF0S0F1xmDLFXtdHLpzBg868xBfTy3s-pSfTU5uRtte52kuhzWyROipcML6YiF4at0Ix-cBq7hCJstPVzLH4fRCvJnitfr--GELa0pnCYt0w5hem7z0eo1zfR_iNNsE=w933-h978-no" alt=""></p></li></ul><h1>後記</h1><ul><li>沒錯，就是這麼簡單就完成了這次的爬蟲程式~</li><li>有需要這份程式或檔案的人可以在這邊找到<ul><li><a href="https://colab.research.google.com/drive/11xN8QftibAGf_jdXj7BWf4h40yhXnLlM" target="_blank" rel="noopener">專頁儀表板爬蟲程式.ipynb</a></li><li><a href="https://drive.google.com/file/d/1-6BL2EDs1LpE-aiWtV32RffauMGHAoHW/view?usp=sharing" target="_blank" rel="noopener">政治人物粉絲專頁清單.xlsx</a></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Web Crawler </tag>
            
            <tag> FB專頁儀表板 </tag>
            
            <tag> colab </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何抓取 Google Trends 近1月熱門關鍵字</title>
      <link href="/2020/02/18/Crawl-GoogleTrends/"/>
      <url>/2020/02/18/Crawl-GoogleTrends/</url>
      <content type="html"><![CDATA[<p><a href="https://trends.google.com.tw/trends/?geo=TW" target="_blank" rel="noopener">Google Trends</a> 是提供我們查詢關鍵字搜索次數的網站，如果我們適當的加入時間與空間的維度進行分析，還會發現許多有趣的事情!想對 Google Trends 有更多了解的人可以參考 <a href="https://growthmarketing.tw/google-trends/" target="_blank" rel="noopener">Google趨勢－你一定要會用的四大實用功能！</a>在這篇文章中我將示範怎麼抓取近一個月的熱門關鍵字與被查詢的次數!</p><a id="more"></a><ul><li>我們直接來看最終的成果<br><img src="https://lh3.googleusercontent.com/XPlul5dcUZBSkRoue3UjvUhLOUCf-Yhr6EqX3wEDLlo3HDp3WNyneHkuxj8fpbP0k8UGhRVYRNvNy3OM8wULyafiBvU9beY0ntJepD52lqy7LrXfjVBfHNLjNN0gunFlRvpXVOeJRdIfgtVnqaFqQ1b5ulUBinBza_XZBGmSS52QkaLrot6s7vWFs1Lte6Qhijj0gDUW_5dSVQf8xQ_Zlp0bpy9hCBQmuEYrZcGiXTUn52gyRWs-ANecQBIGh-WsDGHhfqnjB8q9sL2RVQ_8a22Xtv5YsLjy_IZ64jXpi40dylQq8emegRG617DSGFS5MtDIZkH_TgvD3VM7s2DHgweqsQgrFkRRO4SZ4S48o4lB20vxtbzy44vDiU2JpQui3LbvH0By3qyni9pw7p6wmH-9OooqpHUZutZsisCt2eVmK3If0dTeZXqd23yUaWvg2aXXH9ywE7VfCsaMzyC3A4noJ2NIB59oupZuoFiBEUpRMYc2xCwUo8vGxu2OWpeTvAyRK4-hKpK-vLtcHTq53NZSa-GGo5hhalkyCt2frVAn-TiUqfhq5vkHM9yuxPvm6oprhzuaMVsVNZfEhOVfk5H8lOw8fuOEg2cIJqkGlorWw49SuxNQ_qEffw0ZsVGhkhoMrQEX5lx5V0rVR1GPL2qf-xZ-IrvjY478rRlPoDTr4pt0Z-gJ31AAkfEJuOkky0ac35_WNqjVjiPyQYAC_jQOZdmJHol2gawivTiCcvlZOWZE=w1785-h978-no" alt=""></li></ul><h1>文章架構</h1><ul><li>觀察網站架構</li><li>開發爬蟲程式</li><li>後記</li></ul><h1>觀察網站架構</h1><ul><li>首先開啟網頁並開啟 F12 的檢查功能</li><li>點選右上角螢光筆標註的按鈕把雜訊都清空<br><img src="https://lh3.googleusercontent.com/PMJrh9Cg1a7vqk8It5OxT9NUZxeVRWkRRc6ZyZzgxJIP1KVhSQhuUuH-TfKzmEXVv4qYvTELCgi9GLVlYkbl4d_ZLGoVslmB1Kojf-Iqx9ysu87fgmojjHiA4YJis8VJGvGLLlh-Bj7OaTEB3pu2pRxrhzf6xRD2eKwicqGfb9NVCyCu8gu29xPNFEs4PNj8g0bSVfEEMc63g6fEzrCyDz5R0at_ACa6416LwbhoQrgeuOs-HC0QOXctWXyZomAL0_Ycny-S_BetUbimc4dv3efgJo4AtajqE-jnWKgCoVQ2JzOQIc1CPh17sO4q8Shd6YH7boKMyNNMG7YDxzSE-4h50lK-Z05NLHtZcjCd1ToaP3zXlaqbtMxQvE9S8W5visfYDhbO7u6OWqPQppzcrGvnGr9MNkQChkLl-IEewmZ_9QUHbOlmO0GIkXcYaTu-Tn4hXJEKicQPjCBJkZDBNKyMLzM5YWOwBqy-J0G6Vg30t5TLLc62DhVmyAPdFBu-wC3qA_oLIrva9iNen0hmG4mcHehlLcK87yDBxLEi8_v3Z_qbI7W89seZyeveBB4jmaoaybpWCXY74uzsL3Y8JuzxMf4FM8X2ksVAI1FVM_donWYvJ54Z4xQQeVNLZ4NRIFBUx41LJkLa8-chewyYxHHQI_5GlJQxKs_VwtkgUe1GbtpbfmiX73xli1AZHeU9yJUx9ueg9EepqDTk5jLTTbZe6Ps8aN67CTZjvl3SHhgY_R-U=w958-h523-no" alt=""></li><li>接著點選左下角的「載入更多」，並觀察有點的網站活動<br><img src="https://lh3.googleusercontent.com/k2RQaMPgHaY-9WU0PeOY3aV8ynXmKd7w-eSLwcs2DnGqPX86znmLTeHz9HQijVhq-vgsqImggkUcirYLW6Hb8epYvz4bzJCAI16HsLZRJO8wMVO6e7GS8S82r2HBR2IPLrFKzHZxL4cF6i5MKch3vuo-OFcni6y9A19jkV6hLfhu9SfI_Bjvj5fGr0IGE-5eMbzKoKVP4Ho1DhLO3SNMCwuK_VZcvg97BqDAfveTFS30DaEN5DnvVGoSVI6S_fs8KpEzymOUBy1TfK_IkhFME7N5_9N-NQPkLxivw8627diAhQRdqGvNstrD_urZ8EWLTzD-35oxxrNbnicJlBGAvYPTviOSa7fe2kJNKPmQpRxMrLxncXRcWyPaI3zfYb2GZEn1BzM34iac9PSoUIuL4BS5bnRbfdyxo_F6CjQv3Zu8fP7V0Q7Q2kmvs_9ZxOkxE7pId6Kmqo27kWBjj6dwGTKUrGx_ZRbnfuZ4Q8ID3Pu0gGbcK3Pd0S09mOkFmXB7W0bNy4D5pXAzDrIaI689RgCpbzfWZ_rw8US0-Q99rmWut_s4DLdWbOjO5gM2myuY8CkytwNBhcbJKImQn4sI3L2oOBn59ATjDegihP263L_lINzBbo83scCRUWCYcix1UVtr626U9gFTjwtP7e0feCoa0wpKSxV59HRZ62dfMrFl37Py50Z0sp2s1CFaPByGJz7dgF61X0aF3sYtZap0NeFBjc1enV4yjwJ58aoMFJNp8kC1=w958-h524-no" alt=""></li><li>點選後就會發現加載更多的資訊都在右邊送出的 request 中，而我們只需要解析這個 requeest 的回傳結果就可以抓到這些熱門關鍵字的資訊囉!<ul><li>有些比較複雜的網站會送出相當多的 request / post 來干擾我們的爬蟲，遇到這種狀況時可以善用 ctrl + F 的功能來搜索我們需要的資訊</li></ul></li><li>以下是我多按幾次載入更多，網頁送出的 request，聰明的你以下就會發現我們只有ed=後面帶入的日期在變動而已，因此我們只需要寫個簡單的迴圈相信就可以抓到我們需要的資料囉!<blockquote><ol><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200216&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200216&amp;geo=TW&amp;ns=15</a></li><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200215&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200215&amp;geo=TW&amp;ns=15</a></li><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200214&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200214&amp;geo=TW&amp;ns=15</a></li><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200213&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200213&amp;geo=TW&amp;ns=15</a></li><li>…</li></ol></blockquote></li></ul><h1>開發爬蟲程式</h1><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">import pandas as pd</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import datetime</span><br></pre></td></tr></table></figure><h2 id="先嘗試送出一個-request-並解析回傳的資料"><a class="header-anchor" href="#先嘗試送出一個-request-並解析回傳的資料">¶</a>先嘗試送出一個 request 並解析回傳的資料</h2><ul><li><p>送出request嘗試抓取資料</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 如果連接掛掉可以自己把ed中的參數改成今天的日期~</span><br><span class="line">url = &apos;https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200119&amp;geo=TW&amp;ns=15&apos;</span><br><span class="line">resp = requests.get(url)</span><br><span class="line">resp.text</span><br></pre></td></tr></table></figure></li><li><p>送出 request 後會收到以下資料，前面有一段干擾的文字 「)]}’,\n」，而後面就是 json 格式的資料了!<br><img src="https://lh3.googleusercontent.com/pCb4Vf_u_fuZ0ppJuH8ERypJJCczItrPwlyxmZEqwRbGgADWjAuKY34gbqUGAds0H5Nhjm3gAY8yKEsSmLM0FHuWlR4SrNUTKth_cZrdQW5rWpZQJ2sKoZKOLjqVHopEAlFSUXRvDnqaaIMD_CH29gbRqJUlACx0AABUfgTq-FiRYjxeSBr4KbQJuS2wHdzJSv-gK4JTbvTmitMsFqKRp0YI4kv00zC115ZrGyCtFmjEopebN1IBDHcDkWC91fD2JDSGr-eRPOrJLBVH02KwaJPfUlfxED0_sXPICsewdML-BJckvWowh_zUoGYXRdt1sodZAPYENe6vo_bOuQRs3vx88EcDECGsjGMGI5ABv860auDCDUg-WvWONDC-zsNlD2a_n4jFj4t3AXV79j5VWm0InjX-s3ksuZZQTEokKHEHvOh9pEMyp2FHCPn2rBrsiCcU3h-dxEOEP0Q-AXx7d6YISMdEdDwlXWhgoLb3xvmOW05y9lU1xU-ZMR3LK9AB8kNG1ysIbJ1WlrnipZRZ1elKLT2adsxznZKRz0aIIs1mD3kMPe_xFsxt5sKjEX4Kw4M8JYE04Sj_kj6ZOg1Vcx5z1IHb8x8f-Gj2E3JLSWz1a5He6QXMH0pDVeegB1Hv6fBXRE8Zx7e6AHaF5PZSdFXJdIgrSSJO1NU_8QHdGIZyoDigrVVsWQDro28oGioSiYxgJmbwl1rd1qCATGKlFyvPES6s1v9XJQhKvtU9L5SHZJoM=w885-h59-no" alt=""></p></li><li><p>因此我們可以用一下的方式把資料整理成 DataFrame，中間還有一些簡單的提取資料的過程，因為比較簡單我就不多說明了!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(json.loads(re.sub(r&apos;\)\]\&#125;\&apos;,\n&apos;, &apos;&apos;, resp.text))[&apos;default&apos;][&apos;trendingSearchesDays&apos;][0][&apos;trendingSearches&apos;])</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/yI8UzXTPawvb8fMg5F89vVCxFjJDduWFjEoVJfhv7ZE6zthcO8cRDgCTgxDu2YTsI4NhcIECV0fVqyQt2JMR4Y-gMiXuo36KMMhfNFAsOdzwNMeGc66hUGXy4Nr0P2W73lfkNMbxk_IW1PMfjeREWpke4K3ubdoVyOQwaL3Cp87ZXRV30bDrhYPQw2Xh40V6l2K-kPqn_dymg97MlWZG3MP-IBoXtYQb4iPosXnCG03Vkn39bofWmZIg6gCwz5pA6AiJB8q_47UzjUvYy6_T2JRoogbhgFie9YA-PRwKq07PfCR87VEc-L9TvfqYysvhfAzoIAE4Ds5zQHwlOW8uesePDokM9UtJvpbbvRTuu427jU9eT89DeVD_uu3DQs0S2Wzk6Dy7ENBdoqPCYUaNIDyIA_sVvHeth1gfko_EIzxy1bCcOsrbboifHLdc-_rb40V3b4JOnImYUDjYjgrbJNGApYFpIK9NLjVVXK9eQ_Di14wLVewN3D4p3ugaIBrRJwVxAH1lSV5NhXhtZumBcAj1SGUa5n0gxbREURY_KGeHxgF8k4EE6apbsdC-jAgxLTVV6JcRTvushWqUmO8JXzdCAr7EcJDo2nmydpGrTAxs9usqsTX0Ha_HJlCOmZPfwcTyw5NsBFeAnRt0JD2YLFUd0RvkTDKls0mq9Jl4odxw4As_lA8P9scplDo77Xl0AT90rr5-i-fytBSuBkAGlU4hF7r_jO1rRm0gguPE7N3OVO0D=w1854-h706-no" alt=""></p></li></ul><h2 id="透過迴圈抓取近一月的熱門關鍵字"><a class="header-anchor" href="#透過迴圈抓取近一月的熱門關鍵字">¶</a>透過迴圈抓取近一月的熱門關鍵字</h2><ul><li><p>因為我們在前面發現只要改日期我們就能抓到資料，因此在這裡我先設定兩個日期分別是今天與前29天。接著就透過迴圈逐一的去解析資料成 DataFrame 並 appand 在一起就可以了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">enddt = datetime.datetime.today()</span><br><span class="line">startdt = enddt - datetime.timedelta(days=29)</span><br><span class="line">df = []</span><br><span class="line">for i in pd.date_range(start=datetime.datetime.strftime(startdt,&apos;%Y%m%d&apos;), end=datetime.datetime.strftime(enddt,&apos;%Y%m%d&apos;), freq=&apos;1D&apos;):</span><br><span class="line">      url = &apos;https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=&#123;&#125;&amp;geo=TW&amp;ns=15&apos;.format(datetime.datetime.strftime(i, &apos;%Y%m%d&apos;))</span><br><span class="line">      print(url)</span><br><span class="line">      resp = requests.get(url)</span><br><span class="line">      ndf=[]</span><br><span class="line">      ndf = pd.DataFrame(json.loads(re.sub(r&apos;\)\]\&#125;\&apos;,\n&apos;, &apos;&apos;, resp.text))[&apos;default&apos;][&apos;trendingSearchesDays&apos;][0][&apos;trendingSearches&apos;])</span><br><span class="line">      ndf[&apos;date&apos;] = datetime.datetime.strftime(i, &apos;%Y-%m-%d&apos;)</span><br><span class="line">      df.append(ndf)</span><br><span class="line">df = pd.concat(df, ignore_index=True)</span><br><span class="line">df[&apos;title&apos;] = df[&apos;title&apos;].apply(lambda x: x[&apos;query&apos;])</span><br><span class="line">df</span><br></pre></td></tr></table></figure></li><li><p>執行完就會看到近一個月每天的熱門關鍵詞與查詢次數囉!<br><img src="https://lh3.googleusercontent.com/62JSXkDW_YJjCI-jXB3rGTxKTRdapXBbACDu4Fzmg4u-8kkONHm9MD9-FozEFK95299xnZn-vnScTrM2f3Od_BJfI2SYERFfog8RwnIxRPS2lqKTZ1EjUsNNy_W4jFGnYwHdMGL9nrQMQm9wIVi91ihrWjY3WWD4PlPLzgTQge8F3oqzqpGrTfcjlU53R7iPzcjKLPuWJT5aseeQkcAjewZ47rtn3WTGP8u5EDufP-ug_AIfCpULIWbK-TXD7QzysmpbtEZcvN-Bxp6E5bCcN_szIYDghL473CaVYBnKskxNJWGNhip1UYi5rWfT29-4q0YyyeB4-zgieCPRa6iPQ54cZhun7MVfK_UEY5iXL3hgQaECv8lrIxPEWMb3VJBSV8yBRxdsBGjIQSosr_mKTn0D-THRyqJW00FGc6KGBJzJ7H0gnsi-Lmhf25gVjA3a0223Gggdvtyo3pdjMgTRLnCW_jHwTprWByr5Lwk0tzehJXBIfdy5YL7xxj7i338NbglHd7M7mDmGiMQeTyCrGhmBtnK5JzWavjQRe3IFYCVPLzXHg_oNQjObG4rRNlBRqklEjUb-XL256XVChOTkTxKUWOdiluM07YCA00DQpqD2TpacIv5ClqPiX0URRlz4wRdVuf_9xyeY3VPPH5DxVx7YFpDq5XUtrc9pc4eeU7fnPjC3MC2b9L42WBX6MwCsFRii3TBuOoL1WNqZIfA54KVgMeDG_CSwxhooxOyFglqXwMtZ=w1830-h743-no" alt=""></p></li><li><p>最後再一段保存資料的語法就可以把資料保存在 Google Drive 囉!<br><img src="https://lh3.googleusercontent.com/zV4IrTuvOQV0wcZ9mJVREcULr0zS6AA7ZYIbsowu3pUaujpumJvtO9BCoasJ0fyaHgkjTLfKWFWkCWZ2-2vueus1OC0UVXfCMuHDEkYZ_9vBmI7CjGEzka5_Ws5OfQ7gv62JTkYEf-nGzHccpE--etSmXRMZPUXHjELaeE7ogtTouO6mbNmy0nwuuORwI_jRKxvwWILPqIBd5x0eOAlbA0gYxHYMdxkm9bXJJRg5SVvvXCMSAvt7wr12vYacYvAZXpIVSufC_fUL8qLMRcMJYnffDL8Mf-1GSNBJSX9GK20wjG0xR9_GD81R8lPsRpp_urPbIWUkdgyhk6qklq9BlG74cegCQ2SWWjgHS956VcwBcvgbXihRxtuglo-741GmE4r9hnnUm78AVcFCMEX8nybxKKvw43_gyfLxCsC94YsKyM7YVbglCR-1Ln_24uxw1bAKCUoJMY4WKoOjnT_kgxXayql8EJqeAezpPWa5cNuZLA2NHjqBgIWMk-7Js_2nu4MpVvUiimAukSv8XsfdoG0gIVKC6USa7wsUlAgpCEqYEYFH74F0Nr5xfOdGV3GkN4wKxeQKrT27yL6Kf9Qhnr4hmcOpS9mJ22NQYmkxVouWMGXagW11UBNXU1b4LchW6OtQC3Df8jkfIuTXZ9T4z5jfUL7bPeFLfB-fJe_VBw2fMp1uv2j94mIY9Gzjx6IJYYc1lWlLyys0zxkn51Ln-z39cvwsR-onIQBtgHRdC1vbVOsv=w1522-h544-no" alt=""></p></li></ul><h1>後記</h1><ul><li>Google Trends 提供我們的資料是近一個月，每天 20 個熱門關鍵字，沒有提供更多的資料了!<br>(如果一開始有注意到的話這篇文章可能就不會誕生了XDDD)</li><li>有需要的人可以在這邊線上執行/下載程式：<a href="https://colab.research.google.com/drive/13Ssk8v_eWF-EwNFUuwjAySg2HBld6i5s" target="_blank" rel="noopener">GoogleTrends.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Web Crawler </tag>
            
            <tag> Google Trends </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何建構代理IP的清單(Proxy Pool)</title>
      <link href="/2020/02/07/WebCrawler-ProxyPool/"/>
      <url>/2020/02/07/WebCrawler-ProxyPool/</url>
      <content type="html"><![CDATA[<p>當我們在進行網路爬蟲時，或多或少都會碰到網站設置的反爬蟲機制，其中簡單的是檢查headers(瀏覽器參數)，複雜些的則會「鎖定大量訪問的IP」、「圖形驗證碼」、「登入」…等等，不過既然有「反爬蟲機制」，當然也就會有「反反爬蟲機制」囉！</p><p>今天我要介紹的就是如何建構代理IP的清單( Proxy Pool )，透過代理IP來爬資料我們就可以避開「鎖定大量訪問的IP」的反爬蟲機制！</p><a id="more"></a><h1>文章架構</h1><ul><li>尋找提供免費的代理IP的網站</li><li>抓取代理IP清單</li><li>測試IP的有效性並建構 IP 清單</li><li>後記</li></ul><h1>尋找提供免費的代理IP的網站</h1><ul><li><p>網路上提供免費代理IP的網站有很多，我們簡單在 Google 上搜索「免費代理IP」後就可以發現許多網站了，我找到的網站並經過測試還不錯的網站分別是 <a href="http://free-proxy.cz/zh/proxylist/country/US/https/ping/all" target="_blank" rel="noopener">FREE-PROXY</a> 與 <a href="https://www.us-proxy.org/" target="_blank" rel="noopener">US-PROXY</a></p><blockquote><p>提供一個使用心得是，在使用時要盡量避免使用位在中國的代理IP，會這樣建議的原因是大陸有防火牆的限制，訪問國外的網站時容易被阻擋或比較不方便，為了避免這些問題，建議直接找美國的代理IP來使用！</p></blockquote></li><li><p><a href="http://free-proxy.cz/zh/proxylist/country/US/https/ping/all" target="_blank" rel="noopener">free-proxy網站畫面</a><br><img src="https://lh3.googleusercontent.com/XpGOGzFPJSsJCsI-MzGD_ORk0vn5NwA87omrsJVE_8HVRYoG87U3zoM-XKhmFOxIpEZQwmiBbBXNpgxwNv0tLixnBEU-Bo-h4E0Ok34onfxN9oy5O7DXxQ9nWhRaOb8ci1XvUPzE-ClXI2M-JPP1EgihddnAgri9Pb63_226MgvgsgQuyTnMZfWurLu196MauyPrMX8AcKTjQuapdowKcnQEdY3zvWIDWUuNrQLvFE3G46ZuVaseGforOsmMEW1pfmilDlHBNuOxwDcVQIgkN4HQ5Jk8jCulT3dIt9d3FuojR_pYV75193kNoMtjAddILBrLIJM7MfL9sdfWhSsE-51DF6rvZCitwTpLfBgtnwtcQeJ62fqDQ-xU9Fn6PX5xYEsb0SZ-wlQurHnO8tPGjaD7yhvp-0C1e2Tx42vfSj-Y7vRQ3iL-d4rz35dkhW4K5Qom0scT2g7MY8ps5M6mSZwamcp64k0h_7hyPaI3rmN3ripo5J3KV7pVioeCdAjsoIhHiGc3fbp6Jg6RjhfGA6OfDq_vWdU3w2M8weE6wzYGnyhBDJR8jR3EgqXgDiwd0XVYfIzOtjMrd4Ee4pRSwjp-8IXFNKmivla0firDAV66rZjhR6r1GEGL98QLQKbYqrtOWeUMS0c-4yzmuPDd2N1n97AgKWHIKmQz4VexSa3D9e8yvN5-gnd8xrO4=s1843-w1843-h978-no" alt=""></p></li><li><p><a href="https://www.us-proxy.org/" target="_blank" rel="noopener">us-proxy網站畫面</a><br><img src="https://lh3.googleusercontent.com/HF1yxLSPCmhAmznlUHSYZvtCSJxq3j1iFUCkUAcV-bwrg68VdWjTJ8-yftiPk6ORvTRLqPXcc5OLZlIrazmEbn7xanaR-rThHD3FLGbyicPlzrQFPyJdSDp4MKxs7SFnQUYMPkZLgmtUvxhqUjZ3L4tDkgS8SsGlFmyRctMhDswo54WVildO-8uKLSZqr4LfADtvcHtZyN3wCN4lMCve2HRQNJ0gXksenMO1zEZRrkVQRn-JBGoKVWfJr-hWyRbHW8Y5FJ-lYw4jFBsh-BuSMMOMxnY8gopsKMKMuXuRawq8dlyXcr51XE5oMdgET-NAYeOHDiIPoPnyfzSUgYyGxTk09YmJaxwEa9vizZbUxGFuPjQbjG2o8HEMTbrJvRjy2t4nb8tmZo_TV-poF02atWOgOD_0crjuNh6KN-EDZQtIzngiUIKYiFymIJVDqNttoxrP4ZrD0nradgiZyRizE2LM_vqvxlcNi4Am99QqXQXIkOCwkwXu_GVnORYQDKR-cyBLjxAEEnAfxtkN1eJkw_AUYJMc3USJ9OjssS-fb6LuW0fz_NoDPt2nvY_sQgGgYWV2JvYzRko1xhyTX6VXIcT0FtALdSTI-0AhmOmYK2qnJYgGu2ptcgU_cMGP96_JddKo3X7U9nCQv0W-LpYNmLCbUmdp2sgkHDN-3xUpX_CaMyLfIBqlWSNRR4PH=s1847-w1847-h978-no" alt=""></p></li><li><p>在網站的畫面中我們會看到表格中有許多的 IP地址 和 通道(Port)，稍後我們就是要抓取表格中的這兩個資訊！</p></li></ul><h1>抓取代理IP清單</h1><ul><li><p>在這邊測試的網站是 <a href="http://free-proxy.cz/zh/proxylist/country/US/https/ping/all" target="_blank" rel="noopener">FREE-PROXY</a>，考量讓大家能方便執行這個程式，我是在 Google 的 <a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank" rel="noopener">Colab</a> 上進行開發，文末也會附上完整的程式，在此大家可以先看流程即可！</p></li><li><p>安裝與載入套件</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">!apt-get update</span><br><span class="line">!apt install chromium-chromedriver</span><br><span class="line">!pip install selenium</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure></li><li><p>設置Selenium參數</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">options.add_argument(<span class="string">'--no-sandbox'</span>)</span><br><span class="line">options.add_argument(<span class="string">'--disable-dev-shm-usage'</span>)</span><br><span class="line">options.add_argument(<span class="string">"--disable-notifications"</span>)</span><br><span class="line">driver = webdriver.Chrome(options=options)</span><br></pre></td></tr></table></figure></li><li><p>收集IP清單</p><ul><li>在此先用迴圈的方式讀取1-5個分頁表格中的IP</li><li>由於IP的格式是由四組1-3個數字組合而成，並於中間用「.」隔開，因此可以透過正則表達式來抓取資料</li><li>具體的語法如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">IPPool = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    <span class="comment"># 用迴圈逐一打開分頁</span></span><br><span class="line">    url = <span class="string">'http://free-proxy.cz/zh/proxylist/country/US/https/ping/all/&#123;&#125;'</span>.format(i)</span><br><span class="line">    print(<span class="string">'Dealing with &#123;&#125;'</span>.format(url))</span><br><span class="line">    driver.get(url)</span><br><span class="line">    soup = BeautifulSoup(driver.page_source)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> soup.select(<span class="string">'tbody &gt; tr'</span>):</span><br><span class="line">        <span class="comment"># 用正則表達式抓取IP</span></span><br><span class="line">        <span class="keyword">if</span> re.findall(<span class="string">'[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;'</span>, str(j)):</span><br><span class="line">            IP = re.findall(<span class="string">'[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;'</span>, str(j))[<span class="number">0</span>]</span><br><span class="line">            Port = re.findall(<span class="string">'class="fport" style=""&gt;(.*?)&lt;/span&gt;'</span>, str(j))[<span class="number">0</span>]</span><br><span class="line">            IPPool.append(pd.DataFrame([&#123;<span class="string">'IP'</span>:IP, <span class="string">'Port'</span>:Port&#125;]))</span><br><span class="line">    print(<span class="string">'There are &#123;&#125; IPs in Pool'</span>.format(len(IPPool)))</span><br><span class="line">IPPool = pd.concat(IPPool, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">IPPool</span><br></pre></td></tr></table></figure></li></ul></li><li><p>執行完程式會看到以下的畫面<br><img src="https://lh3.googleusercontent.com/CGeuFhe8iXsa5pjHhebVPdhvuXLxFZEME5K55ltXIrW7R0cs_5vNtqLOyFAodq3xKjnQ2Jru9e8LyUwOEdL0HB2EInDmFSmINibDbxAqDYLAxrVFkC5VQE0oHn4YGnG_E_dtiilFno68oAUETCGl5fvA3_w0tGc_xYmZt63w99k43z3_ezcdVtI00NIA1eWo5NciUw1ZTyGP9P5Booa20aesNixxjxelc7mvo85Piou_trbzxiZXF9kyeeo6MNZEMHZrmSViih8gtlddX2cX8sHgjWK75M1-JFtNfIHYzKIy9ixVbSdk4BKgIMNZSFEKYA3pgkxlAeVevp3Kb5DcdZ685Lql2hIbM8k1ITlMeZjPrDMMIX4wlr51RvgGbwa5ExjU4oJRPVSULgXqZQm6ZETPzh7w8cgFgFhxhMQoUKhh6g4hbH_aLFYxRaogGOBgClRHzvyf7_BG2mS5OkR1nRsoaUG_IQeS08jA_UvwAozD61Qr8FVwo4MgES4XCx7TJWyfkHWFfqaKwFDs2Z1yTjp8CNnTMqf32_agdCp_Tl5wetd3oWePWmCirxvkYbizsTf3Mdh4tjYsV7la2mVeHAhrlLZPxaYcIw9TGC4G9NuD5ro5AmAankdTleDbhHnobISJSac3e9D-XENBbUFmjw0gG5_baxw9TquXqvwXLb6Lh3yVKar3ZyY=w702-h588-no" alt=""></p></li></ul><h1>測試代理IP的有效性</h1><ul><li><p>雖然看起來我們好像拿到 104 個 IP 了，但別高興得太早，因為當中絕大多數的IP都無法使用!!(畢竟來源是免費的xDD</p></li><li><p>因此我們在這裡需要再多一個步驟用來確認這些IP的有效性，而測試的方式就是我們在 request 的時候透過這些IP和Port來訪問網站，當我們訪問成功後我們才把IP保留下來，反之則丟掉</p></li><li><p>具體的語法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ActIps = []</span><br><span class="line">  <span class="keyword">for</span> IP, Port <span class="keyword">in</span> zip(IPPool[<span class="string">'IP'</span>],IPPool[<span class="string">'Port'</span>]):</span><br><span class="line">      proxy = &#123;<span class="string">'http'</span>:<span class="string">'http://'</span>+ IP + <span class="string">':'</span> + Port,</span><br><span class="line">               <span class="string">'https'</span>:<span class="string">'https://'</span>+ IP + <span class="string">':'</span> + Port&#125; </span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          <span class="comment"># 隨機找的一篇新聞即可</span></span><br><span class="line">          url = <span class="string">'https://www.chinatimes.com/realtimenews/20200205004069-260408'</span></span><br><span class="line">          resp = requests.get(url, proxies=proxy, timeout=<span class="number">2</span>)</span><br><span class="line">          <span class="keyword">if</span> str(resp.status_code) == <span class="string">'200'</span>:</span><br><span class="line">              ActIps.append(pd.DataFrame([&#123;<span class="string">'IP'</span>:IP, <span class="string">'Port'</span>:Port&#125;]))</span><br><span class="line">              print(<span class="string">'Succed: &#123;&#125;:&#123;&#125;'</span>.format(IP, Port))</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              print(<span class="string">'Failed: &#123;&#125;:&#123;&#125;'</span>.format(IP, Port))</span><br><span class="line">      <span class="keyword">except</span>:</span><br><span class="line">              print(<span class="string">'Failed: &#123;&#125;:&#123;&#125;'</span>.format(IP, Port))</span><br><span class="line">  ActIps = pd.concat(ActIps, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">  ActIps</span><br></pre></td></tr></table></figure></li><li><p>執行完會看到如下畫面，如畫面中的內容，大多數都是無效的IP，最後我們就從 104 個 IP 中找到 11 個有效的 IP 囉!<br><img src="https://lh3.googleusercontent.com/mpJ3CTXL5OYoijGnd95bm8wFJut98UpcDT_gvwSuVrs9-ydMWReaKtgZZ-vza355xH7SndLwguEMpUite5HkGrUx-WxZPTzfIElrDmEAvyDvGQH8EdOCCDhgzKypMwX_2W_o9v-IMGoV3xooKL3QA-hII3HO-iO88n9ZprIrEdg7ib0oWAKk668XfTHbo0DAWGz2p7DrAJ9tMDzzHvc7tzhcRnRr7QU3caDLaqk6PeSwOppEyl-fRYDKiFrzD5HEzj9GFISTDsHoi5GX4cJBgCi4PvOgN1aCJNcjqlGn6Pn-YEuc2d6pYOX6YEWem_BbaLux-qEKRxtBM6AG9oCGcQVw8VUH1tP4H9cSceHtDoQTQgIqoNrepE-CJTXWX64bpayXQE1umVpWheXIeA_zbHl8YC0rs8x52HEkBrZoIoQ3ZisQF-JS2hQstQXrDFef8hOtTZtKOXk3fh-ihIzR-QGSFXhkJzdw4zhrjo9Vxp_mJzmgL5Az8il9KFsY0kbLhJAY4G9LhR6hkMvE_1NsPgdxwLUyAzwZzlTs3CagQppyxehkJCx67SzWn2L3VGbshqSC9q8_OvZbc-IRpyvgjdH9mCBe_eFb4f90KyUByZDudiviIb00qInmNFfjPaP3Q0QJuQ30QyvIeXGC2T7LDLIGlOPisbxlsEfYOOlg1vbRnoL2k4UI2l8=w394-h820-no" alt=""></p></li></ul><h1>後續</h1><ul><li>之前在透過 Facebook 的 graphql/api 爬貼文時，會發現每個 IP 大約開啟 1,000 則貼文後就會被封鎖 IP 了。</li><li>當資料量小的時候，我們當然可以透過手動的 開啟/關閉 手機的飛航模式，讓手機重新抓基地台的訊號，藉此達到替換 IP 的目的，但如果我們需要抓上萬則留言的話，就會需要透過代理 IP 的方式來爬資料了!</li><li>有問題歡迎在底下留言，而程式可以直接在這邊執行/下載：<a href="https://colab.research.google.com/drive/1PgALsr6iArpUE4NHt4GJw2TkiGH7we-m" target="_blank" rel="noopener">GetProxy.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Proxy pool </tag>
            
            <tag> Colab </tag>
            
            <tag> 代理IP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>中文斷詞的新詞偵測技術</title>
      <link href="/2020/01/05/NewWordDetection/"/>
      <url>/2020/01/05/NewWordDetection/</url>
      <content type="html"><![CDATA[<p>談到自然語言處理(NLP)的中文斷詞技術，我們通常都會直接聯想並使用 Jieba 套件，網路上也有許多人分享了 Jieba 套件的功能介紹與使用方法。但是當我在使用是卻感到非常困惑，困惑的地方在於，隨著時代的演進會不斷產生新的詞彙，那麼我們透過「過去定義好的字典，適合用來分析未來的資料嗎?」</p><a id="more"></a><h1>文章架構</h1><ul><li>一定要新詞偵測嗎?</li><li>資料來源與說明</li><li>資料前處理</li><li>新詞偵測</li><li>展示成果</li></ul><h1>一定要新詞偵測嗎?</h1><p>在做文字探勘的分析時，首先的任務就是需要將非結構化的文字資料整理成結構化的形態，其中英文因為有空格作為字跟字之間的界限，因此可以很輕鬆的進行斷字。在中文的文章中雖然有標點符號可以切分成不同的句子，但句子內並沒有空格能把句子斷成不同的詞，因此怎麼把句子斷成詞就是一門很大的學問。</p><p>Jieba 就是幫我們把句子斷詞的套件，而斷詞的方法是透過事先定義的詞典來匹配文章，因此詞典的好壞很直接的影響了斷詞的成果，我們就直接來看看 Jieba 預設的斷詞結果吧!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">sent = <span class="string">'蔡英文強調，希望其他產業也能台積電一樣，把台灣當作研發與生產的基地'</span></span><br><span class="line"><span class="string">' / '</span>.join(jieba.lcut(sent))</span><br></pre></td></tr></table></figure><blockquote><p>‘蔡 / 英文 / 強調 / ， / 希望 / 其他 / 產業 / 也 / 能 / 台積 / 電一樣 / ， / 把 / 台灣 / 當作 / 研發 / 與 / 生產 / 的 / 基地’</p></blockquote><p>如上所示，原本是「蔡英文」被誤斷成「蔡」和「英文」，可想而知，這會讓我們在後續的分析成果中出現英文時沒辦法區辨出到底指的是語言的「英文」，還是總統的蔡「英文」。同樣的「台積電」也被斷成了「台積」和「電一樣」(?)</p><p>我們再多看幾個例子就會發現，斷詞的結果仍有許多可以再改善的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sent = <span class="string">'法人預期，本季台積電營運表現可望淡季不淡，預估比去年第四季的營收高峰季減在5%以內。'</span></span><br><span class="line"><span class="string">' / '</span>.join(jieba.lcut(sent))</span><br></pre></td></tr></table></figure><blockquote><p>‘法人 / 預期 / ， / 本季 / 台積 / 電營 / 運表現 / 可望 / 淡季 / 不淡 / ， / 預估 / 比 / 去年 / 第四季 / 的 / 營收 / 高峰 / 季減 / 在 / 5% / 以內 / 。’</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sent = <span class="string">'法人預估台積公司今年營運將呈現高成長，營收將年成長15%至20%，並續創歷史新高紀錄。'</span></span><br><span class="line"><span class="string">' / '</span>.join(jieba.lcut(sent))</span><br></pre></td></tr></table></figure><blockquote><p>‘法人 / 預估 / 台積 / 公司 / 今年 / 營運將 / 呈現 / 高成 / 長 / ， / 營收 / 將年 / 成長 / 15% / 至 / 20% / ， / 並續 / 創歷史 / 新高 / 紀錄 / 。’</p></blockquote><p>如果我們想要透過文字探勘來獲得有價值的資訊，那一定需要立基在好的斷詞結果之上，而 Jieba 則是透過比對詞典的方式來進行斷詞的。但是除了最一開始曾提到「詞」會有時間的影響因素之外，在不同的產業、場域(現實/網路社群…等等)也都會有各自的專有名詞，而且這些因素彼此間還會彼此交互影響，這些因素都會讓情況變得相當複雜。</p><p>那麼字典到底要怎麼新增與維護呢?最直接的方式當然就是透過人工的方式維護字典，例如當我們看到「蔡英文」不在詞庫中時，我們就把這個詞增加進字典中，透過不斷地循環來增加詞典的豐富程度，但是這樣的缺點是需要花費大量的人力成本。而看完這篇文章的你們，以後就可以透過統計的方式來幫助我們找出那些字可以組合成詞，讓我們可以輸入透過大量的語料庫，就自動得到一份專屬的詞典!</p><h1>資料來源與說明</h1><h2 id="資料來源"><a class="header-anchor" href="#資料來源">¶</a>資料來源</h2><p>收集資料最快的方式當然就是透過網路爬蟲的技術在網路上收集資料囉! 這篇文章中的範例資料是我在 <a href="https://www.chinatimes.com/" target="_blank" rel="noopener">中時新聞網</a> 上爬取有關台積電的新聞資料，有興趣的人可以點擊此處直接下載 <a href="https://drive.google.com/file/d/1rw0QrW6_LnNM6trH1p7tbLpBx-cmmycd/view?usp=sharing" target="_blank" rel="noopener">台積電新聞資料</a> 。至於爬蟲的方法或想要爬其他關鍵詞新聞的人請參考 <a href="https://tlyu0419.github.io/2019/10/30/Crawl-ChinaTimes/">如何透過爬蟲抓取中時新聞</a> 這篇文章。</p><h2 id="資料說明"><a class="header-anchor" href="#資料說明">¶</a>資料說明</h2><p>基本上我把截至昨天為止在中時新聞網上面有關台積電的新聞都抓下來了，時間範圍是 2009年11月 至 2020年1月 的資料，共計 15,455 則新聞，包含以下欄位資訊</p><ul><li>TITLE：新聞標題</li><li>TIME：新聞發佈時間</li><li>CATEGORY：新聞類型</li><li>DESCRIPTION：新聞摘要</li><li>CONTENT：新聞內文</li><li>KEYWORDS：新聞關鍵詞</li><li>FROM：新聞網站</li><li>LINK：新聞網址</li></ul><p><img src="https://lh3.googleusercontent.com/QrO9iVtj3wRRD5XdFthCmWSBgt9M6RL8F7Vybl5znE51klcQ76gXvCIXPXjVVawWFEdOkqrTXrFQgrP3Elaxsu6AFrdP6i9EHlod0cfMWvfZ4074I6Y72c_CL6b50EnezeG1HhoB7EiXjJYfUgS1Q-KOAL1V0n3g_QCb4IA6n2BmvchzK3PfIvj1GEOfouEPNqnx_cXNg6WD0gQsoMd6eQ8FhPtGuNIrpQk3xaPbBYrGf-eePIu1VdfVxZCt8-JvfI4W3k6fURWm_1l9oYqCraa4kyGc42hIT3ZgqunP88RhQixMiqncpl30Wtitsf8e_gTRvee6KRO-ZlnNkPd3XA6UjL70yf_GJfzO7Elzfv3bo4Augg-VKeEOI36Q8zvNwTshZHFH1eXsCXhOAZNBUJoCe5M3sMa1XPGGTevder5rvAgbiJVyskWbQvTtzRHlcH_Zs3SM75Vdhwoc6DjExPl-OKqNCZrP5W21dKAF0ztB-ANhDvyfvOfOhj0Snf7Jqma3ZS1LHf-68BmLbYVJCvhjwSrBwO1Q2UfuMgYm78CY2zOM2SZzq2m9o3vwZT2JF42jjSQpSVwM_1twCoNAKU_3eQomMDZ9Exe1Z2HYUn4eF_f-fxRBskgg53Dql9xOgmRKORdsaO3Na0FImqGVi4Sln3JNxDRAgVqPrVwYEVfEWo6klfNdnuGnb6mL=s1739-w1739-h978-no" alt=""></p><h1>資料前處理</h1><h2 id="只保留中文字"><a class="header-anchor" href="#只保留中文字">¶</a>只保留中文字</h2><p>因為這裡是想要找出中文的新詞，因此在這裡先將新聞內文中的英文、數字、標點符號、emoji…等等的資料濾除，讓我們的語料庫只單純的保留下中文的文字，而中文字在 Unicode 的編碼落在「\u4e00」與「\u9fa5」之間，因此我們在這裡透過以下的正則表達式來保留我們需要的中文資料</p><h2 id="按照年月串接新聞"><a class="header-anchor" href="#按照年月串接新聞">¶</a>按照年月串接新聞</h2><p>由於在新詞偵測時會需要使用到大量的運算資源，考量記憶體的負擔與運算時間的原因，在這裡我將新聞按照年月拆分，並將相同年月的新聞串接起來當做同一篇文章進行分析。因此在後續新詞偵測的語料庫中，共有 123 篇文章(123個月份的資料)</p><h1>新詞偵測演算法</h1><p>這裡使用的演算法的概念、說明參考自 <a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="noopener">互联网时代的社会语言学：基于SNS的文本数据挖掘</a>，說明如下</p><h2 id="提取所有候選詞"><a class="header-anchor" href="#提取所有候選詞">¶</a>提取所有候選詞</h2><ul><li>我們的目的是希望在沒有預先定義好字典的情況下，透過統計的方法得到一份字典，因此會先透過窮舉的方式列出所有可能的詞。通常會先設定詞最長的長度(建議設定為6)後再進行搜索。</li><li>以「天氣預報說周五會下雨」為例<ul><li>長度2： 天氣 / 氣預 / 預報 / 報說 / 說周 / …</li><li>長度3： 天氣預 / 氣預報 / 預報說 / 報說周 / …</li><li>長度4： 天氣預報 / 氣預報說 / 預報說周 / 報說周五 / …</li><li>…</li></ul></li></ul><h2 id="檢測指標1：詞頻"><a class="header-anchor" href="#檢測指標1：詞頻">¶</a>檢測指標1：詞頻</h2><ul><li>如同以上的範例，透過窮舉的方式我們當然可以把所有的詞都找出來，但同時也會找到許多錯誤的詞，如長度2中的「氣預」、「報說」、「說周」等等，因此我們可以透過統計詞頻的方式作為第一個檢測指標。</li><li>可以預見的是，當我們文章的長度夠長時，「正確的詞」出現的次數會遠高於「錯誤的詞」，因此我們可以將出現次數太少的詞濾除。</li></ul><h2 id="檢測指標2：內部凝固度-pmi"><a class="header-anchor" href="#檢測指標2：內部凝固度-pmi">¶</a>檢測指標2：內部凝固度(PMI)</h2><blockquote><ul><li>以「電影院」為例，如果「電影」和「院」兩個詞是完全獨立的詞的話 「電影院」在文章中的出現機率，應該會與 出現「電影」的機率和 出現「院」的機率乘積的值 兩者會接近，這時候表示兩者可能是偶然拼接到一起的。</li><li>但如果「電影院」出現機率遠高於「電影」的機率 乘上 「院」的機率的話，表示兩個詞並不獨立，也就是這兩個詞可能是一個詞。</li></ul></blockquote><h2 id="檢測指標3：外部自由度-entropy"><a class="header-anchor" href="#檢測指標3：外部自由度-entropy">¶</a>檢測指標3：外部自由度(Entropy)</h2><blockquote><ul><li>除了看內部內部凝固度之外，還需要進一步看外部的表現。如果一個文本片段能夠算作一個詞的話，它應該能夠靈活地出現在各種不同的環境中，具有非常豐富的左鄰字集合和右鄰字集合。</li><li>以「被子」和「輩子」為例，我們可以說「買被子」、「蓋被子」、「進被子」、「好被子」、「這被子」等等，在「被子」前面加各種字；但「輩子」的用法卻非常固定，除了「一輩子」、「這輩子」、「上輩子」、「下輩子」，基本上「輩子」前面不能加別的字了。「輩子」這個文本片段前面可以出現的字太有限，以至於直覺上我們可能會認為，「輩子」並不單獨成詞，真正成詞的其實是「一輩子」、「這輩子」之類的整體。</li></ul></blockquote><h2 id="綜合評分"><a class="header-anchor" href="#綜合評分">¶</a>綜合評分</h2><blockquote><ul><li>這裡透過簡單的加總凝固度和自由度的得分來幫各個詞評分，接著我們就可以按照這個評分對資料集排序，並且自行決定合適的閾值(Threshold)，來選擇要選幾分以上的詞作為我們的專屬字典。</li><li>實際在使用時，可以再自行調整這個綜合評分應該如何加權計算。</li></ul></blockquote><h1>新詞偵測實作</h1><ul><li><p>在網路上已經有人將以上的演算法轉換成具體的程式碼，可以參考以下網站 <a href="https://zhuanlan.zhihu.com/p/45745963" target="_blank" rel="noopener">python简单实现新词发现</a> ，我在使用時僅有稍微調整這份程式碼就可以使用了。</p></li><li><p>以下我們分別列出分數最高與最低的 50個詞彙，要強調的是這是我們沒有預先定義好字典的情況下透過統計方法自動計算得出的!</p><ul><li>最高分的 50個詞<blockquote><p>醞釀 / 泡沫 / 犧牲 / 垃圾 / 槓桿 / 貢獻 / 余湘 / 蟄伏 / 駕駛 / 妙禪 / 禿鷹 / 夥伴 / 烘焙 / 泡麵 / 苗栗 / 辣椒 / 邏輯 / 肌肉 / 呼籲 / 野村 / 尺寸 / 犯罪 / 挖礦 / 漏洞 / 麒麟 / 蓬勃 / 敬鵬 / 古蹟 / 咖啡 / 徘徊 / 干擾 / 梅姬 / 螞蟻 / 卜蜂 / 硫酸 / 搜索 / 什麼 / 灌溉 / 鳳凰 / 侵蝕 / 陰霾 / 掌握 / 餐飲 / 烘托 / 紓困 / 瑕疵 / 霹靂 / 暑假 / 魔咒 / 雜誌</p></blockquote></li><li>最低分的 50個詞<blockquote><p>復五日線 / 轉機題 / 逆勢收 / 能源教育 / 規劃未 / 中東緊張 / 負債表 / 險試產 / 目前多方取 / 決議每 / 台北時間 / 受制歐美 / 關鍵地位 / 也會是不錯 / 等類股拉回 / 多芬第九號 / 開發案 / 信心指數 / 去年度財報 / 長黎方 / 全年毛 / 統一及長榮 / 兩地 / 貿易戰休兵 / 低接 / 除息前 / 貨市場上則 / 統一超 / 後市不 / 鎳價 / 主要受惠 / 李秀利表示 / 度因工廠而 / 援極紫外 / 與股后方 / 上市櫃市值 / 外銷訂 / 年以來 / 交大 / 創歷年同 / 鞋與 / 本周納入 / 魏哲家表示 / 受惠於國際 / 無限可能 / 果砍 / 重啟協 / 運底部已 / 小幅成長 / 統初選</p></blockquote></li></ul></li><li><p>觀察一下應該就會發現高分的詞的結果都很符合我們的常識與邏輯，至於原本被錯誤斷詞的「蔡英文」與「台積電」也有成功斷出來(只是藏在後面一些些的排序)；而最低分的 50 個詞中則出現許多不合理的詞彙。我們可以把分數當做是否成詞的信心程度，在使用時還需要選擇一個合適閾值，並保留閾值以上的詞彙來當做字典。</p></li></ul><h1>年度關鍵詞</h1><p>以下是我透過 tf-idf 方法計算得出的年度關鍵字，如果對於 tf-idf 還不熟悉的人可以參考 <a href="https://zhuanlan.zhihu.com/p/31197209" target="_blank" rel="noopener">机器学习：生动理解TF-IDF算法</a>，考量篇幅的原因我就不對資料預處理的過程進行說明了</p><ul><li>2020<blockquote><p>表示、去年、產能、資金、開高、不過、營收、第一、走高、晶片、投顧、高點、華為、貿易、處理器、加上、全球、美元、法人、預期、台灣、設備、表現、三星、帶動、美國、買超、產業、早盤、技術、今年、指出、行情、成長、漲幅、電子、製程、投資、半導體、億元、大漲、新高、持續、市場、奈米、指數、外資、股價、台股、台積電</p></blockquote></li><li>2019<blockquote><p>客戶、去年、漲幅、上漲、以及、明年、貿易戰、營運、美股、帶動、全球、加上、震盪、投資、影響、大立光、蘋果、資金、產業、晶片、指出、目前、美元、公司、台灣、製程、技術、法人、表示、預期、成長、電子、持續、半導體、表現、新高、美國、華為、買超、今年、早盤、營收、奈米、市場、億元、指數、外資、股價、台股、台積電</p></blockquote></li><li>2018<blockquote><p>美股、大陸、全球、股王、賣超、產業、獲利、投資、去年、半導體、震盪、下跌、買超、蘋果、影響、開高、指出、族群、權值、鴻海、預期、奈米、加上、早盤、股票、上漲、新高、美元、公司、大立光、暫報、表示、目前、美國、方面、台灣、成長、法人、持續、電子、表現、營收、今年、外資、股價、指數、市場、億元、台股、台積電</p></blockquote></li><li>2017<blockquote><p>企業、帶動、國際、今日、賣超、產業、整理、影響、漲幅、獲利、奈米、預期、開高、加上、半導體、震盪、族群、暫報、美元、投資、指出、買超、上漲、權值、公司、目前、蘋果、美國、萬點、股票、成長、法人、持續、表示、方面、台灣、鴻海、大立光、新高、營收、電子、表現、今年、股價、外資、市場、指數、億元、台股、台積電</p></blockquote></li><li>2016<blockquote><p>族群、震盪、經濟、大陸、加上、可望、整理、全球、獲利、資金、奈米、蘋果、今日、投資、去年、漲幅、賣超、權值、美元、上漲、金融、國際、公司、鴻海、暫報、美國、股票、指出、預期、目前、台灣、買超、持續、大立光、方面、成長、表示、新高、法人、表現、電子、營收、股價、今年、指數、外資、市場、億元、台股、台積電</p></blockquote></li></ul><p>雖然還有2009-2016的資料，但就不在這裡繼續列出來了。從結果來看2019年爆發的美中貿易戰確實有被偵測出來。同理，我們可以透過分析這些關鍵字來了解台積電在各個年度中發生了些什麼事情。或者藉以監測公司的營運狀況、風險…等等。</p><p>今天的分享到這邊，我將台積電的新聞放入這份新詞偵測的演算法後，得到的結果如下，有需要這份字典的人也可以在這裡下載 <a href="https://drive.google.com/file/d/1jHyeq6jBQp3YMdNcbWZ2Kk2-RpCkvvfE/view?usp=sharing" target="_blank" rel="noopener">台積電專屬字典.xlsx</a><br><img src="https://lh3.googleusercontent.com/PQF2PmC-umwOwqeOuZfEg8tNyBFvTw2ibXq7B0JsE6VPYQFbUW2phD-oG2GiDYE5JFIjmwaku4NMKCBx_tPh6BaqjJTzTupcCVL_J90zG8-eiEUdc1mDEokcyVtpeLRkNXvR2fFfb0iOZQ25Fw2grPsl5EoXv50EZlvyUZziX-xDfLefyBq6oN_r1IZVQwoDErjelva5OI1yCrc8kNtRWHVncuSKaaNtkSHLeL-mNuj7qjAg9Xhnp4_ovSXOIpYGHw7xouEsyYL0xS_2j_pYRUAHvmhiwU9hbij8mlqIXAeGUpvjECpVAFgGdywjnmrbqg6WrTxmPY8z_IdEScKVJ0sCBDBApn6RUBXxQSrLwr6j4R6-c0awNKnzi04gDssFV0RkIJueZmRNbAAUhGpnCW4ZGJLezFyNtLdfUhmyS2NsevOSdPKGyQLvngcde2zPCHNOqi9beLUF1UkFaVoeVJPbfRsM3XFxRuY9NKdnM1No_PA2AMypeOKsmnxviRH193T43GLnBHZ5d8CQw7yjzKdJv2eOJzeqzS1DZtwvSoGhsaUYCZqDdXC1xnLtpJwJcbXxfXsxdT508IORqnHHEuPrNIRXd4O2W7ZRUHG8fIeSLoe_A7m2bOiwv1ybMekCw0FV-YUZKSJ3_XUQn0TNFZIIZLuSTBGOwV4R9JNuCMrX1qekELJP7ro=w1056-h978-no" alt=""></p><h1>附件</h1><ul><li><a href="https://drive.google.com/file/d/1rw0QrW6_LnNM6trH1p7tbLpBx-cmmycd/view?usp=sharing" target="_blank" rel="noopener">台積電新聞資料_2009-2020</a></li><li><a href="https://drive.google.com/file/d/1jHyeq6jBQp3YMdNcbWZ2Kk2-RpCkvvfE/view?usp=sharing" target="_blank" rel="noopener">台積電專屬字典.xlsx</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 新詞偵測 </tag>
            
            <tag> 中文斷詞技術 </tag>
            
            <tag> Entropy </tag>
            
            <tag> PMI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>透過網路爬蟲抓取1111人力銀行的公司資料</title>
      <link href="/2019/12/29/Crawl-CompList1111/"/>
      <url>/2019/12/29/Crawl-CompList1111/</url>
      <content type="html"><![CDATA[<p>先前曾發表過 <a href="https://tlyu0419.github.io/2019/04/18/Crawl-JobList104">爬蟲_104人力銀行工作清單</a> 的爬蟲貼文。當時是透過 Selenium 來抓取104人力銀行的「職缺」清單，而這篇文章則嘗試直接用 requests 爬取 1111人力銀行 在全台的「公司」清單，包含了公司的名稱、統編、資本額、簡介、職缺數量…等等內容。最終抓到 1.1萬筆公司的訊息，並且我會將程式碼與抓到的資料放在文末的連結，有需要的人可以自行取用。</p><a id="more"></a><h1>最終成果</h1><ul><li>按照慣例我們直接先看最後的成果，這樣會讓我們更清楚的知道要做些什麼</li><li>目標是將左邊的公司清單的資訊轉換成右邊的表格，具體會包含以下資訊<ul><li>編號(1111人力銀行給這間公司的編號)</li><li>公司名稱</li><li>關鍵詞</li><li>公司簡介</li><li>產品服務</li><li>法定福利</li><li>公司福利</li><li>職缺數量</li><li>聯絡地址</li><li>行業別</li><li>行業說明</li><li>資本額</li><li>成立時間</li><li>統一編號</li><li>員工人數</li><li>公司電話</li><li>公司傳真</li><li>網站位址</li><li>相關連結</li><li>連結網址</li><li>更新時間</li></ul></li></ul><p><img src="https://lh3.googleusercontent.com/SyD93pdVXMVzztxyDaSSpDTVH-MxGheXLvlSyKHAkoZI4pxj1WlbqLgB2Bd9IOY97MZ347L0G1zttYG4RhuhPbmKsVr0R_Q80gmCMYAxQOBAgfJQGFtjNIVKVbAgl53cunP7b3ZsXvi8SXMGUmLx836OgCtgEz0Ud09I8FY4jT6blNEaHvGR1_1KHisvgpaCsRYLPQvJuvGRpR5HOEbQ20bP431wLGVC7joJlF6jyk5EF-LK3xtHqba-2g_alqwlhOiDhXxIt23uqSGQKHE0WDLRlgoGAzkSuhBsQ1Skx20euttJBmZGDkTdzcw9KlMmRRghkIDiKHAD9TFuoZjJlhjzbGej6CcswdoIGFD14crawL9YiR6eCBajCxxdRTq44MnAphbMxb3EKKtSkZLpjTjoNdCL93vXyF5D2FAmnckepdw1x8kjc1Pf2fjYKG8auo8CcMEvrMGkutyBdaUi28rWeFZnfVDDnTstRne1CZ7JQKo6-oLWaDR80vkCvoLYUvAOSbxbKFdwcV3wTQ7s3nzBKKmLi2dA1YIgTleZw7SVpMJABc0Be-xH3XFIDEBPT_WRebTSK0M3kxsmVvLFpKIr_PKQtWpZj2rhKTMs9gAjuu2Kzwz9wv1o9t6d8dDHYDNiMpuuvl7HZiBuqRAMLXX2GvRSnNK_Z9CLGWVLuYksOtQcSZXkX6c=w1739-h978-no" alt=""></p><h1>爬蟲流程</h1><p>這次的爬蟲會經過的流程如下</p><ol><li>載入套件</li><li>偽裝成瀏覽器瀏覽資訊</li><li>收集公司清單</li><li>爬取個別公司資料</li><li>保存資料</li></ol><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p><img src="https://lh3.googleusercontent.com/1Ap_T16OA0P1CBJoC-h32JFe1MjhmsA87CV7Dn8kf2-10P2WykSP5-p3Y4_dyLiCqkGgb9Zw6RV9S88hENJF-StCrsOFKWBuIq1njBasbXj-N1Z3UeXqEUENiR1YG8S_VVmgbW4C7XMnhwaRr0Iq9JmzryW-8wP2Oi9T1_GROTLeE0ITRz1k4Fy6JbCxfSloqN5vK9Zs2Bj63oqpwqKK6eMxqEOsHgl82LhrSiRdVgW0327tHTJR5ku9ri2oY8mbxv-RFi24IsLGB7vRSF06vrKVYdRo9saC4sEV_OoSGAeojtdtPynJD5nf86b55hgzSCEmAOo_rbmbS4iYIDvKkpcqKZXqZOA70b1_lUtPoT-pDTKgklVncZT79hxGfmOLrVtdLfYMXkhiQUBt8Pzn2zvWfpV-IFiLe1YiehzfNLJtbU4yLmE6aBhy_angvYR5fb9fm0v5H2dDAgVZKttkzTtnlG76XGSr_qcVK4Rca2CxYGTRXSs4KCXMz_fWC2RwuaTK1pL-oDgsdmakO5VVvT6sruf2NekkPhlkfgHeuVR6S2ugoe1e2Nd-6Iqd-1NFFokxa1mHqguU02VtiXEYXLNTb88ROk_2TMwV8hpxG7BFlnYSg7nwNBaHrR7MdOjXL4iiLvCR9FnqTU8PoFeAlHJiThqz9ir7PGfjS9ijYvM_xpE80AfgQU4=w1820-h155-no" alt=""></p><h2 id="偽裝成瀏覽器瀏覽資訊"><a class="header-anchor" href="#偽裝成瀏覽器瀏覽資訊">¶</a>偽裝成瀏覽器瀏覽資訊</h2><ul><li>有些網站會偵測我們是使用什麼瀏覽器來查詢資料，如果是沒有瀏覽器的查詢有相當高的機會是用程式在做網路爬蟲</li><li>因此我們在爬蟲的時候建議養成加上瀏覽器資訊的好習慣!<br><img src="https://lh3.googleusercontent.com/SEzOhqCkMkmcloJ4-wirqfazDe6M-MxHx7iszsuZK3vtUJpQlJoDogH6C13XAYk_yK73jjSeIAazjat-GK557Ka1wMUflS_kxLYbuwj5ShTDPqvfUm3vmHPvVEz1j5TYwPGltgkLzwSy4FVMEBtKJ02IC6ySnLBTY1Q4HvYj8vkamR4sVq7XldEWsGAanRt-sYe1icZ0AIlyh-WNY2Kb1-OHDK05anB--HJck88PeftheLCPMxNVuniO5zN36eUC-hAh7hZ5zLiExwFb_5y6ucMkYCPq6gI0nHWNUvUtYkdk2fyeRl3xwQhcLvhKCQnXJ89rXHnOHPLG-jZ2yCzooyDyChLmGMp2_oYn_OB47smE8IdrukZoU8O_l6QLOyMgE7sQzM729BJxlMsC7aci84w9JfNgcvjxVgEofA9GOfqquBjajpSqWZBgnZLSIK0GobAYpLjD_-Qtg2SLhA-hc7cNaenUYl0-F7Xh-w7ErhwNw2Os1X1ssgFf9sW7vn2s3l1auvvexSoFx2c1to1TFqdhN1CVLP_oxRJxJwoRgZzdFt9ZBxW9YgjHI_k-jqP1ddLnF2Ulzr84v0Y9F79Pqvbzhs61CnGFvepHh-XiWHCDgXszq6eIghthqK-1dXQSIcbhlYgEIy9TqjvG-_pumihI9iNt7tx_gEkzKJ5_oaY1VLSBPFShZ_A=w1819-h65-no" alt=""></li></ul><h2 id="收集公司清單"><a class="header-anchor" href="#收集公司清單">¶</a>收集公司清單</h2><ul><li>這邊可以依需求調整要抓幾個分頁的資料，因為我想抓全台的資料，所以直接設定抓200個分頁<br><img src="https://lh3.googleusercontent.com/69HgjwzpY9cHuZass9lqLxVYS9UXcnyym-aFtOglqdHrEfuPhXdoZRXh_v127l0d2LCE0Wzi0Fjp2Ke8n6GrDSjOxLuPSaXw0zR1xyrd8nsRA2sem992NrgjwP__DxTZIfJErnMZXdSfpalYq65JWvW4ZQz_A3fTP8IUE2FLajZlKwbkoxshcvI2WtMR8r27_8VZkQAgEqOwR9Y1MVFOS5E9nKP1E0PqiT5pUZF0vydn6PXdW3eMbVIHUxpwPa1XN21jbfiuSQwlk7YTtja-I6YDFWyMDKkSRpJ4yefFFMGfefnzjzubjIbUfJ8_GzITWub27HQ2pNra4w48tuEhFOVUR5E1Yy-IeAkBa04EHP5MttYjCu2Fr3-maVpujMIE6SslLhFBFGtk7djGXEVVKUz8BMoFvkAFvDYXbwgepZPrOwTPL9Py1SidSzj_B5shfg3WyYbGCMtj45piEp9PB--JruYToyE3MRwbWo5VRjMABMIIkBBvkBxDz4CQaVX-_9uRXQAYP3lXecrBEv_-tQ1Z_Vv5hULqn5KnN_ykTLm9UiyKnhtQ3UZ7KjqbcvhwwkYDuS8m9gzMlYhZ-VG3-Yv7SrPaNsMvKQo-v_a-_BOXOWyQeK1-wVozCsZvq9A9VWDctZxoCURpV8J7AbxL_3QzHtEPjNeHDChmlInjF4Xveadt865ud8o=w1820-h297-no" alt=""></li></ul><h2 id="爬取個別公司資料"><a class="header-anchor" href="#爬取個別公司資料">¶</a>爬取個別公司資料</h2><ul><li>網頁中有許多資訊，而我們抓的資料可以分成以下幾個區塊<ul><li>基本資料</li><li>公司簡介</li><li>產品服務</li><li>福利制度</li><li>職缺數量</li></ul></li></ul><h3 id="基本資料區"><a class="header-anchor" href="#基本資料區">¶</a>基本資料區</h3><p><img src="https://lh3.googleusercontent.com/jMIQYJ9rhX7b5YymoMGIh4oByrjEcpbtDfnGsrf_7M62Bf7V4Pifu9bwlr_9YwDH6rCuQSeiniBqP4t7sXfnQLne74_izfuyYoogs8C7OzXCHgBmTazN3nd7rW84NWShgfnPSfogiapWS2bDUkZWUB1ifd4G4S4KPWFjlHLfye_nPUTtR0S4SXcCC9I34VVdTXVncCo-I0XQLda_Nav7UMPECc8OolB_aAMkfbVsVqvfZUQuhhTf9FkDwqdJPxMfbeocLavddxi1Zt-HAzes0ijyrFWO2LLzGnJzWBQqr-EjkWgeoqBAqMVV3tFzlILYGsgF3M7XE78LXosDCyqD6w1Kaytxzf8pZL1roaWIV83-mtC0aMnd0s50-YSscqoJ0R3QVZNNSqVSSUC4Zlj3BKSp5v0kzcYTJjQZVJCHfsxWjlrfLKg0KjwdBzfc6wzV19nn_Fs18Wv3bb0hpsB5EV4-qrf6BiY5lUo-Gs050_ZyuSHjWOPCbC7YJfukPA0ld6gu6TOCUndqt1FpS5rvrNphNRHMO_6fHLwqF3MWM5x2kI9uYzD9qFSZtk4SQdnoD3FNSU_wAOTgteTsgz4bTl_SOp4Slc2E2JAf-OoLS6UvvdidXolMEjpJECrC49T3bkNdXD6teLctwDytdi7yly-iqcgQBos3LONhjI9XljIkPK7WMUOanI8=w1815-h849-no" alt=""></p><h3 id="公司簡介"><a class="header-anchor" href="#公司簡介">¶</a>公司簡介</h3><p><img src="https://lh3.googleusercontent.com/vN-ay9lbeBF4f3K5RM1ChkwbeZjvxtoa8Eoi-AeOcrq9Ep6EHtvgfSpKD70kPP3K2y8ms60zmEwo0cIMrrikUuq6g29vBC3YkvMfJ1OZUv1mpHcRt_tuQiEVaJ1Jfv_I-bsH0mEPWo5AYxo3oFSHikjwPbjf-hZvaJFjmSh2qH7GboFZTT5I4E30lE8LEw68SCZK0L5RFY7img0PIUCEDtdaV5Lf-kCrfjJuczpdhYUHbiMGW-wBzY5IvdxaM0YllQPcqZCOmA4YdNX8uu9aO2b1M5izAi3GtmIWcVIOSOs_czdTIh1ulbx_yMdCh81Oz6hMVfcrJyZcaa7jig_74TyFxSNed5ST-0q4qQnCslkbrOfRXAaFfuFAprjaFxAkedL_Esgcm3qJ0NWlDoxOyED-2hBSUQ6vxoA_9yq6r9P5ONGFSMmckkPRxOF4fHclHkLTnnozNA3T3GKQg1J-_pgy3ZDlQLqJhx_O6nQyXVmM0b7uUhvq0X1GLqbUzNLLXRg_4ZOCPUXlKJziMgPdhggOVRc33Kq0uSmuRKI1PnpeVIQMLSb6GXjI7oICg--e-munZhKb-j8b3KvQJlrEEPpfum5BF8z0k5cX-b5Pw-GpFz-gUkRmc9mDO8xaI9iGzIUnkmj0zw2pxPRnTYLODCykfd22f-l63vonRWnXUL7-eBiQqWZJ6xE=w1820-h82-no" alt=""></p><h3 id="產品服務"><a class="header-anchor" href="#產品服務">¶</a>產品服務</h3><p><img src="https://lh3.googleusercontent.com/O9-4A6uX3HKy6S9Y6o3_5enoHxiqzl20dLKAEOCWNB3sktAZ1NhFW3-D93TMH00LhKwL9ZykCVMy1asw0uPuq5158uzHRpxC9HFVoU-DyRxuEZm5FKHUSZizesy6p2srzPMVpwgJFApb4bZsXvG-TZVwE66L_CfEgzWDhkNWujfp-ldXWEpsf99OKD3AS8VNZE1fZKAyM8UfrJX31occncdpETTCTpMHJU09a5L7ev8c8dEryf9opo-Fiia6GcbISZTiymNk_I1sSJrIFK7cgd_rnLQTd4aygBQX_a7FQnCrTRuYwtrknf42iFiCWYe9MleiDMlmCJysR34cC6W17RANr6WrFw4WkSt_yJqiGg_0ugoeVOAiY7Ryilzw-eQp99OKZHOQHU4EMhLrhIsCSN0T1vLy87-G4KyO4aRYpuT7AAT4XuJHT90-VGt03_Yd3Hxak2wT6i2MEMgtXdIiXFJbZ4DbTPkQcV6jILB98CxZcmbN1Y-S0AqNWdhL4_cdf8mW3uEMXbjl42MElNFhj82EsLqGDny38quNQ_YSh-renX0VMwW-sM0JZ8gxkXxJuPpj_0wUI7KVzVEqjjhDumbOV_bjbXauYdTQYmoISj-543YjAmzMj9LwtqNB8TN_hEhYlGMrSZAO23OYOQATg8ICIFzKVaEopkADa1cVlFc9RlJPH0jwo3U=w1814-h81-no" alt=""></p><h3 id="員工福利"><a class="header-anchor" href="#員工福利">¶</a>員工福利</h3><p><img src="https://lh3.googleusercontent.com/eDqaijATxVUqJjWvJtAeCHhzoMTC7J-yRcbDa7LMf6Kxt3J7KtiiqrowPlf3R3ptFtdSM0U5PLzk7IiZsn7GzavzSyqtV23tNKIQWFCutNawgmpuon1u3QfHlyxxDhKaz1wuPDUgAyokYv0EGY5ry45T7W54LLzN3tyy7YUrh5vdLr8bKQ4iZRuTbuBkOfM7j32zR_LHghKlFb9the_MmymQlvAnsAzUOFK6k_7ii8xBJolt2UXW0Qg4ZTpcd94O9azkAuIpp4hPTy_dyfTBJIeeb8KRFgGOn76SlF6fPoNXZX18BDzJm1rQ85UhF54kCe93dTwoChVQgrgQfIufcFVNAtS2o-MUpRWR9IiAeMWRMifITGJrPH00nBqFMr-8HW_RyeKTbMfzDL6Eyq9HVOtWkE5eaDOPSjhn2AyR4-tgAOFu5EzgkYPo1YL6M9sH93UvVXX3GVrRZfrkPrXU6tH9_iAw4EZgxoChYY9iapnrnjWmH8ADasSYxYYi24yP5S755fJhAM-GDwwvFprm70d5ji4Zc8nsUXnAl1MiETtKPj6lJg4d24EohCBR2EKLJo1l7LtL1EkUx6SnufR_SquaNIKwxnii54055Z6dZAXFG_SzV3BJjz7DCc9ckzCGO8JQ_7zxlWgy00HBE_wiy9DwUEHoBlfuVU4P4V9qw_-fUJPE0R4Q4O8=w1810-h222-no" alt=""></p><h3 id="職缺數量"><a class="header-anchor" href="#職缺數量">¶</a>職缺數量</h3><ul><li>因為職缺數量是在另外一個網址送 requests 查詢回來的，所以在這邊另外寫一個爬蟲抓職缺數量的資料<br><img src="https://lh3.googleusercontent.com/PJRzhqtZFIp3J7yxKeRlVAUD-rkF1XquMCJjd7Ox26xTG-vcM443vxmnjJoY2uoe61o8thMkrAF6Df2NaCnrdhxxxdDZU6VHg0ZgEBnT-9Qhdy2vgK7Jk9KDzg823MWyWI42YvvRpaVGS7sEa-M3stNNTYqEFCffOuXxz0P41OAddSC0oowlH858eFV05x_tJ2TDCRmRGZzURk317IJlmeEa0ammt8pfaNvOfHXUOK-tYJ95slGb-TZQ1y8UtqUsU8lc6sUO7xdfhl8ZklvbMu-Eqn_htxKiDyU6IaNk1A1_kf9zcmCOL9uEuMuFVJ5_lvK8vr1t-Ny68Za1OyvRkwJwB5E8P3q2D501NMtSrUP-TDHM5_pVYCa8yFZ_ybdVUzs2F_UkU8PL9M31nz2yteSZLBXfZxXJqRD3h-EZw1YAk3U7X8FRKjnl_Npzn757gWRH4XVgRWbRTA-WpUxHtkIaxZhi5XSyrp8QGEiupNiqe0dVFuJZxSWr5sqzt8CPgwAXCgwJ1HrN910E9r1Px7UkTU5atHcOpjvMhi3qCDvVpZS4Gc71kPn_8x1a9iFcOqqjPBlS-sTbWLs6n4Yohsg7yCT5wK3wxKhMeLiM-yFc-zlMwTerZyPHrBwAEmo-0KsiOpXT_k3EYBHzbAfkRRYfT-yILoP88jVTiGP-WvP3SNIl3V3I-IQ=w1819-h177-no" alt=""></li></ul><h3 id="組合函數"><a class="header-anchor" href="#組合函數">¶</a>組合函數</h3><ul><li>把以上函數組合起來應用，讓我們可以輸入網址就輕鬆整理成表格<br><img src="https://lh3.googleusercontent.com/8eawvb0XTNKq_mo6z85ayEfi42wFDkHo-3_lfBfkNtC8uk5mhDiTcc9FMArDAVXOyGIHANPeiBjlNLjnINQ9ypgW6EpZnS7FlczzOzyg8MqFyP5RbtA6mfkj2uTaMgeGMO5hRNBo6KQ0CvSoh8bRu_IJdAE7dWRgV1F2wPZ-sAFnDrtDzxoiQJ9euDhGSSxkutqt1URnCFX0-8JladAqfFV_G1au7OFBn1SnQkG3RmEaytfIlyvpbLmsNt8zypiOSSwMGKg6btMdYktpwya70HhMrN8ovOj9WkqIgS39K-Y4YdxwVG8qnZFAaQy0vE-CRSpiqASXNumA5up8zMAg9yX7otl57zFXKRN0u4Fp7ncqEtcqRSo1dQVVc1Hlg3ZpRJ9W5PxZr0ECsSU3BR0yqW0r86zWwl29OczfsK-P8KOI3-6YD1ZF3XJOaGScawGZRbC5VlWM6cAilXukXdYoR4DiomUAfHgLWKDoAg2wq4lzZnStgypNpFxlOM3sL67uUjljk6qx5SbOLaWpqnYRNmpNIJwi62E2eIvnXJBDM7acXbP10qNyUPleQcz6TvpxHaLGOrJFnfauWRBCpWy3liQ3OlyihIhOwry8ac4Veiz6Lyzf-e8ZduosI4ICwRC1QbqRGsBd796HLQ2h4A27L-J4Y2SLewQQqiTpCuxhj38X3nWDzyb_FQg=w1841-h857-no" alt=""></li></ul><h3 id="執行爬蟲"><a class="header-anchor" href="#執行爬蟲">¶</a>執行爬蟲</h3><ul><li>將前面收集到的公司清單逐一透過組合好的函數內爬蟲</li><li>實測抓 1.1 萬則公司的資料要5個小時左右，如果資料量大的話建議引入多線程的方式來抓資料(加速爬資料的效率)<br><img src="https://lh3.googleusercontent.com/5gcpxpdR30GEJUEnmUgCQjLJHarDCUDbJKqKzgSIpLzvy0fTBvNVNbXCJkutNt6srzT2W6MatzmyJYMl5O-iRB3ku-t2zFxK2d0J7LLAf9tgP98-N9ZvKrbXdmJdyCdHtou2_I5SeQxPBtA0qQgBCyHf1GLotlaV9JrV3Gz-iR9s51x_N4Cdhg1b3lbZf7mu1HQ0On_7jqQ6D91cRqtNmtNMKhB4JGEIrhTu0t98kgYYz-mlqF-rJHVOwh4jBt0hlu-2jCXwA-BdKgWpnRxX7aI9XG3HaSXRw6r3WzQ1PA_UiMzeO4iwCjrk2UrtMODcrvXqxDO0ovyi_UtGUREPAS3Ms1QyFh_09qou7kx64iX7fO-RYqURq8PL35OMmW26yTw32MI3EMOVD6q4MpITcN7l6EbA8DMafpM43FyXzxjqVI9kLnKOxrt0VNht1gJU85eDGZS-aVrE9sMpNWgC6-Rau8Z558AXUetCPt2stzh1jGyHTPMfMT_BfRZwD_oC9qEwEvHb9XgRWBC41RZAGclQ70-6Hoe6vELVCTsUUL3w3l_zcO-Il2_CYHsri7q-Behw2hTSXtfpG-9wLDD0Gx0qq_WhrJgehqtxtRKeLsO7DObxMkGfNm-nD3Db0t1KLr0qUu_1611bEutBtpwloau2vjXrOlEnxhz1hQqijvRCtXvGDPzJyV8=w1819-h344-no" alt=""></li></ul><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><ul><li>先跟 Google Drive 空間串接，再將資料存在 Google Drive 上</li><li>資料放在 Google Drive 上的 Colab Notebooks 資料夾中<br><img src="https://lh3.googleusercontent.com/XFSD8XDUc-6MHpYrKAn7_MS1Sll0yGOtakm8yO8WzfAZWx4G5QmIehLgWLRDgpBS0TCSoG3PPtpe-xE7sKUY9nCHODD7zlSMd2LcF5cm-pYGMZSmGtm_CR3Dikm4Fu39dFhp5n-A1MhhYReAHxMMInc_weKBJzo1gxuS-X0E2vhgSTpD15iiuYViTJ-TgZrEAvlgSqUTWKzCWE40x-KkwK2s5QClL_JsZ_qC41Pm2Eo1FBhEIz_wEXWuipalVajINAjSm8jgiY3U5Y2gntGfPOu9mdChcUS4I2H1K9jjy8kKSOtkeeg9pF8qj5dtH_-B-Z75Rvdz8KhSpqwqKYFSgo4ejY0lkBrnYioQHLThWurN2BRUu9iuzh2F-FPpKQ51MdWHezEmeQgNgyhd9VUY0KmyiqDXIaM9m_DeSQ2l-nN6-TqEhRbUthCSxgjUwAVz9YpZWogbQSHrrME4cv61GjxFUDGzmMtobLQVw1AqmlukDxei1ppIntKZJ2YtEhXQzTehyEGwYxqJKem6Obw24tv8mQA_7_wVmSau5stZpy7B1DIxZ5ysAwLQwPnpDWIHuDGtW4nhrPihswZg8Jr3V_UMstWM0QXv2ufBHcarYXU6RNQAPswS-ewWvjM-2kjjO5-tsxPJ-UmaUz1akGBGwzCMjKIYQ8rYnhdmGnlDXHKUkM-TcGCjli0=w1827-h239-no" alt=""></li></ul><h1>程式碼與檔案</h1><ul><li><a href="https://colab.research.google.com/drive/17GzHUJfFuYxVE7m66Zk3zpU8wXlAimQb" target="_blank" rel="noopener">1111CompList.ipynb</a></li><li><a href="https://drive.google.com/file/d/1scDcy9lIWDK2DHaN4TqOHFgeDeeClbyZ/view?usp=sharing" target="_blank" rel="noopener">1111CompList.xlsx</a></li></ul><h1>後記</h1><ul><li>其實這個網站並沒有特別的反爬蟲機制，最花時間的地方在於怎麼樣在網站的 html 結構中找到我們要的資料，不過花一些時間找一下就可以找到了，並沒有太難的地方。</li><li>因為 1111人力銀行 的搜尋引擎有限制 150 個分頁的限制，所以如果你想抓的是職缺的資料，建議可以按照縣市/區域切割後再分批次抓資料。另外因為我目前只有抓公司的資料(約 1.1萬筆)，如果要抓更大量的資料時(例如想抓職缺清單)，可以加入多線程的機制，後續有機會再跟大家分享。</li><li>當我們有這些資料後，我們可以進行以下分析，如果你還有想到其他可以進行的分析項目也歡迎在底下留言跟我說<ul><li>各縣市的職缺數量、就業需求</li><li>從職缺數量了解各公司的業務狀況</li><li>公司名稱與模糊名稱(關鍵詞)</li><li>…</li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> 1111人力銀行 </tag>
            
            <tag> python </tag>
            
            <tag> 網路爬蟲 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何爬取 GooglePlay 的APP留言與評分</title>
      <link href="/2019/12/26/Crawl-GooglePlay/"/>
      <url>/2019/12/26/Crawl-GooglePlay/</url>
      <content type="html"><![CDATA[<p>隨著行動裝置越來越普及，各家公司紛紛推出了自己的手機 APP 來降低跟客戶間的距離，如果我們想要了解客戶對於 APP 的想法，我們當然可以透過發放問卷的方式來收集客戶想法，但這種方式的缺點是會需要花費較長的時間。</p><p>另一種方式是我們可以直接到 Google Play 的 APP 頁面抓取客戶的留言，如果再搭配一些文字分析的技巧，我們就能更快速、即時的了解 APP 的優缺點。</p><p>在這篇文章我將示範怎麼透過 Selenium 幫我們自動抓取Google Play 上 Foodpanda 的客戶評分與留言等資料</p><a id="more"></a><h1>最終成果</h1><p>老樣子我們先看最後的成果，我們最終會將左邊的客戶留言轉成格式化的 excel 表格<br><img src="https://lh3.googleusercontent.com/Qx862wPuasoREv2KBhW77KXhaNphIO3q0jgOaKfOyNNZwRbhaqF9PodY_biXwg4C8aQU1C6P3_hdJ8EbulOM50IUWlc2o743L2UTaenjmwFnZ3WKnkXTatim9LZTMkaQ42njQ8yuMqvPnxI-gtpbMl2pnFsfEOWB_Lckg7XdZeGQi8x_SADH7HShZQE_C6GsHC8J6su1zeaePL1mfAqvPnTOaKPv2DZRYO-AcRFOhtdRKadaInOGj1_s4wY2qPnwW4D433R8Ec0LU-xgHaCNcvBS-5gsgzODlIVhhg9gsY1Ziy6vybtEO0dIXWEAltizfPG0BSX8SDaQhlYjPjDz6lFb1EpNaqAXcZjm9yg69ZFg1stpZcCq0MF3HnOnlpgTp7aPaLRVJA6u7ZJ8Kz3bzJdsfkLeycWaFPxCtVcjUgSISgshA0THJnI1whko7K9BCnLjrxj9sirqT_waymA-ZIXu-4Zm4yCuBMeNKc5Tkbsdsda6xHEGs13gWliLSEJx_Uz18_3COMV73H9o9LAlP0KgE_4FM8OVySFTuuubrNaMNi7PRgHAO9SHlYF-36dsCr2ybizL_vPDk8LYiFWOhsEO8zt036jf8pz9DED47tvb7fEKIZo7w19IptHvAztqR0qRLaodfFWTA3uwJ89THJkwh4HCx111gcxIpH0ggSy2JhOr7-HcCJk=w1739-h978-no" alt=""></p><h1>爬蟲流程</h1><p>這次的爬蟲會經過的流程如果</p><ol><li>安裝套件</li><li>載入套件</li><li>選擇想爬的頁面</li><li>加載更多留言</li><li>檢視完整留言</li><li>解析資料</li><li>保存資料</li></ol><h2 id="安裝套件"><a class="header-anchor" href="#安裝套件">¶</a>安裝套件</h2><p>這次的爬蟲會需要使用Selenium，安裝的方法如下<br><img src="https://lh3.googleusercontent.com/m6PjNPESUqsilH1DNfaURaID1HaNb3anLF-uaT-XCTgycg9oLqB2lQZ_NcmQKFRfpuHMl7YUCHtH3BH0i0HNLKMkJyFfPGu5t1fPk1PIkdTxHDSSDV4NZQ4-hhEEMc8LAxQ_G-hdgHP6HBY9nssAUL7y6_J-ku-V1ascTZKYCXo3-edYw9sB7zwkSm10CWC5aWp4hnmArynEN_d1aleig5mEznwNf7EaZBEYcSlpsc69z8hNhDBUEOJe9aWwy-xKEsqJpblp0uYN8Il1HSxJ0FgO4nhipl2ccOBk6CWAFntPTERNwzLqSwTD6kRGvQ33k0MtMQYXX6LLD6LzNsrxZnqCV02aq0L4-esbKSiHUZXaaF3tUiAWS-hlgl8VDtgk5AYnbFQdaOKf4pUm-r-tezaLx1nLy-HmF8v26t0PpynQ5gU7eJFqzHRxMxdarxC3yCocA4Y3J_a9vDIorMZS5B0lgS4noUWs3gx0AKx0JjBdJ31PQogVQ35f4bmaCTY8YWRsL2eHD7sN7tlWffRt0b6evN7TzTz-GFm_lcX-oanvBYrEmRDvXnk_pm2eMfzw7NGVB43FHSLLnRYuZL5Z-wTd6jwNN8raTLE9uYIPNSMPnWFoNcJCq3JrjfmLJFyaHNS8JzK_KvJuTJJMTaay6ZPkO5YmrLqXco13aeXfAjh2f7fCiU3bnWI=w1509-h90-no" alt=""></p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p>載入需要使用到的套件<br><img src="https://lh3.googleusercontent.com/Q-aLaTmFZBwybQc6pPcSivze-ZG2E522Cw_BvGeZVASDwhkGgMjZ8n7vqYrmaHJcvMnvCs-6ZA7fk8_sQ5AVl3iHlAaw_TRphrbOxKXpe2PY-BDG2LUdm9DSCANE94tcYbVmKM9bzp3QDEI4BgDwpAYM2A4qx6WEV-EKA9Kekn5lq9ei2nOW8XS5YaaDIYl69IX8W3m1jlGzL-3lHfGd9naur1ZO0UyWMI1VRv3NHq7vqtIksvFqzjq7lMNlvJdM_Snz3Zn7WT2uUGhaIKnM8v1_1WP4epC8mBQ288J4Beb0jYtE7_vNNBHkl7yKz07DMHTek6TqGQQ-qcVkUIGXwoMnZax3WeopWI8TpEzLi-_UnyDaZig34D03FNsnJ1W01OEIkG11VBFaGXcQiqCb7_EkTCIOxK9w5EEe_UQSzOOXFlf4ZnEXz74DCpB2tDZtkFS6tsyb__l-r1hTpHYI97fFfurMwKZsmHbfSqQPrT4CBesCe59L6h8omWRKztlo4wfgcwU2n3U-Zl5Gejr8PWdJ0yZw3qhla7lcFtl6PqGJPjRYVlIwJJjj7QcqjhSrbkW_AZKptRJ0hEif0OaePX6-Y2X4invdY2BhNZH-iJjwxNXWDRW0iILf6C7oOX-c4vCEjXUkJTaklQQQk_c3Dge55Oz7whEuSn5TG8h1uscvQKE7eMSy7_4=w1515-h145-no" alt=""></p><h2 id="選擇想爬的頁面"><a class="header-anchor" href="#選擇想爬的頁面">¶</a>選擇想爬的頁面</h2><p>在此以 <a href="https://play.google.com/store/apps/details?id=com.global.foodpanda.android&amp;hl=zh_TW&amp;showAllReviews=true" target="_blank" rel="noopener">Foodpanda APP</a> 頁面作為範例，想爬其他網站的話自己更改連結即可!</p><p><img src="https://lh3.googleusercontent.com/iK91grOv8nJJe4FDiQ4EDMeSB0Dg-yDioEz-LMSjaQbXukez5NMFgmabHlK1SBvUlwKvKnr8mUE8CKP0ZSz7bwuuOW8Nu6E_2uy6UTaUX8RTtsAiq_A3wWQMzYJ7sL3S_FKRWQW7FB1JrOtpGXLMExfVf5jnqSYbMvY-WNx5yLG_pnuGQrGyaXFsDARcOB1JeKt-AqiXnHOTdLBk6kZU7HoTsbJF1o8SyX60weQT1UJxGCqeNri_2wAsvezEPBk42dAr3WYFW2mEXjYMPShYIYRlL1eXZt5BaY1FB3QGj4dTgQRX0v9tPoinE15TLs9Zo8batCY0ypPWmLcnms-W6PVpgpbHwxQeBfgJmrgvF9QC8cOM1maLW0QSUbMLvZyIq72_3TVpd89mKmxdTfgpCHeoY0ks2vsnGTt2pfdk5Ax_hNsEgno4Kq08qZR-V1fZS7SkRvyErn-GwJX6ZZNAtEvWAw3PSsJb8Lx-aiXZusEo8NfPMvtwpISPWtsK11D93FgoBt9SxljPYKXlJZcgaLa3imS0gxW-Z281lyrTlo4u5OR2IdGRYKZ0Y6AwOBuN2gv8OjYAwgjIaE7Cv2zouISx2nzW5I2y9NZFRV4HADtw8N_y_KCNsPuZBjixl17-ycpZ561DvcgYxNrx9fVcX9nf_He07Bl93q_8uFNbeEpB5nfBVREcmGg=w1739-h978-no" alt=""></p><p><img src="https://lh3.googleusercontent.com/CPmCuDKJAP0lKEsfhV3lyzFbmMqwGFYpTF7BKQQhfSeR9W_711fAYqdc5kw_1Oq4eDf2EPaHk0tD9rH8HkEBdzIC9X34COP-rMNcQSHhk4zKiOkcJv2of6XIdSJkDiVD4vaxVT0lNB0vYiRk4GfYzPTxXtSnIrKw_u1fsQbByIkHpaMwJw8mBX9yAidHabYigCr9M0dYxJdQEU1d2MXY1VkVyRCqdm_KFa6RPtfFKny59VtyVp673hP2h9-J8xLS6f2T36d_F3IgDSIRlwi_lcKTtYfqn5jT4ZcHxNKNOlSjS56xmSZhIKFE4h4c4LdKVMbaV4sUPJK1YVA5xYGyXK6AvCLeai6p5-kCUz2lzvZ-_gaVPx2Kg_aWZmMJ8xkZ7k23mIxi09HD3cciqPRbkaNSFz0MVjRQ_b5bS1sVruCWL8tOicebg1PxU6TlWPWLLUUhf-Gykca93JeTqb0HFyGt1_JJQOtwoyytn7mijt9NOq6r0-7cFcFWfkjaRenMtTaChMaIZdwb7ginzBERQ_-FRLcEKCkLSnxKokFZlCJz14VF2ma3c1Df1MTnj5NofROUZytFsc_FrASzcaofbDMYg73rshXlStpVZbWJnOg8-s6SOlfw0zmTfTZsO6YC2iRSyv2G6pCrmC_K4d7oLTcMbjeYlhzQjQTipbr0_PNeAbwWJUhYwBc=w1507-h295-no" alt=""></p><h2 id="加載更多留言"><a class="header-anchor" href="#加載更多留言">¶</a>加載更多留言</h2><ul><li>透過觀察的方式可以發現，當網頁滾動到最底部的時候會自動加載更多留言，但是每加載5次會需要點擊一次「顯示更多內容」</li><li>另外需要留意第一次與第二次之後的「顯示更多內容」是不同的按鈕，所以在這裡使用了一個 try 函數</li><li>在這裡我設定做 10個點擊「顯示更多內容」，實際上可以依據資料量的大小自行增加或減少<br><img src="https://lh3.googleusercontent.com/UvNjhu7xwKq7RLikJcYCBBGrb4Wm39RkvznDeFyie1BpQ2KrIde4jTHHRP8jqzBUfZ4X8-V9fUSAbboYHamony1jA0D7fvReTQsvyhTcPHXLOIscas7VIRktvVYiakbEs7D-K_nGKCxkJ-GaT3q7P_B70W3sg7bb0DWDXGRI3m8V0ea4go5v13Il0haW4xmrPSaRMj-sAnvruTXHCTBo8gp_P1MXWefqC3xv0FUqN1jCenbQD4eKsUYvcrN-KCzzMgfYEBxfdY8hCCvLegs-lXF3o80OOLXdsYaFPIa0U9Ft0inFzrFKlSrNl8-10JNns6vjXaGXYDarC0QvE6XQPd1y3YbgpUKByYUPmpD5w5I7JpaObmr0HuoW-M8OZEWcVVF3RfrhcSZrxoN0n5_Tl1lqIuFT2P6DnPtFbJpjeqUftuGA2NBsy-C9nM8BkbZwaey5YZ0IU0QOI1PsH_2-ErjGKvpR3xmeT7JOUBjtB4SQNtizVOYEndFA12f34rY2NFj_v6tOMjAXGTiGREBulOoyLeaPk5VSm8IKO9iCJEQxBxEABU-yMFmd-M5QTYz1SemeQH5vxDQe_Y4dsc8UavCTklEOY-wpMYXLIaIf3icU47NvSCzrSYcOEwlHL0kSa_i7Ppo_7MnXgW1TIcCvD2DsY-KMw6cmyBdp1C55HSmkDUP_70n6c4c=w1514-h242-no" alt=""></li></ul><h2 id="檢視完整留言"><a class="header-anchor" href="#檢視完整留言">¶</a>檢視完整留言</h2><ul><li>對於長度較長的留言需要另外點擊「完整留言」的按鈕才會顯示完整留言，顯示後我們才能抓到資料!</li><li>這裡我找的是頁面中有沒有「完整留言」的按鈕可以點擊，有的話就點擊將留言展開</li></ul><p><img src="https://lh3.googleusercontent.com/YKSV2CFs-TGvnbnBzWP1PvJczTioKJzUyJA_x7eQqli6G3pYL2W2rTrtwyGfc2wYprI_dLKngQroECIGbDtZPiWu9xnTzI9_w28iPGlVZfmJIgVgaHH6mRjzkB6ADzwF43KaOxdFAUK4mW6xsF0wimz8VD4w8UEeGGkuv6-AixwNZ98WVpQpEu5caBwnENIg01XJb2bzxJrpCJjCFN3U2qCkrqizcTSAmsGYuEgLekX8IE2TomJiONQ-oHB_6FRNb5Wx9WjwOlC1_xwaRKrZvAQyzKhXrASiOdHHHLKRsh_VZqPGZrEYyaq76XaXob4fqQPzmyBSZ3hYaEMi_2cdYPL0ms4OU33l8_AxQhfS0mlOrdQ6vM04vXwJBczOGDTbmTmlBUp5qWQiLDxf4InOn05_dz-G1AWFiEtA9lVQtX-ElErhQcc3glerWarN5e9UE3zxMf-xp3Qgr0HXqj2ynRM4mb8YONNUnJYkJbWZ-kdNeofsgeC6t9BOQjk28K1KtP4sYABZwOKkDg_W_X1L_m2EIOfW0Y16pLvTjBAzBnf5xTFkN6lsa0lLn77za9aYZOwL4sim_nT2bSLrdimF5Ay5i8x58Mrx4e-B8MrDxCGTwhPS9V7ZcAuxSunO8yavq_1T4z9lrkCWSFCE-ry0uMoXUnE96y5LHu15D2lFsMTUR4HEzYuaRGM=w1739-h978-no" alt=""></p><p><img src="https://lh3.googleusercontent.com/QJzwab2aj4SjQsxIbBOoMslSEMT7MA2PCbOg_aE92OI2q84TG0hg6rF6DeM01CbGFdiEG7zwpvLIAjh0JdoqJNeifckNUfjG_vQ6rknESmueaIAk1zEnSJj639PUXUsoTW7KRZyJM9WxMXj7KE4RiWWpaEmTrTMphtNJdANc5lv_zoQXz-1NNldLQPY1yBy7JZJdXuOyJj_L4K02YiAlpLpFKKddjFAFOB_jaVi__THoBy89psEiRd-sNCtTkTiXomMljFFsIeDIRaI5hccd-qo79IbbHg4mEoFd31LsRVdY99fGL7ymlnfwRFu0zehLhxQH6AcBiR1eARxoaw0OzI2M6KB8f_r7HdnmPMchveH4j8XJNbfChP_b5muiC1U00Todp6ETwhUu2e63CO5W9tCM9iwnwPsXTCNqZU29pf5QgiAQ3er6GFDRtzmHS6wIYfspnYBy0LKkC7-NotkOpV7zWBu_zfS4RcGwKZcX1XOmqI72Rgn9G9p8Vt5v5UaVFOQQobNwxv9NKeGo0meUavK9k7XdhqZiKM6XWjd6S5kyMXeT3f1e63VzkYPt2btXNyoyosKpE4bSJPh8f4O2pHXcmBv8N1HvbREh489KD2R7PvHZliJE6VT_cNIdJ6gNl-LgSWwkCeNLfiy23ic2hNSDvH1kyLhsVbNhBZuosHL14Yl853lTM1Y=w1512-h168-no" alt=""></p><h2 id="解析資料"><a class="header-anchor" href="#解析資料">¶</a>解析資料</h2><ul><li>當我們把所有資料加載完成後，我們的資料是html的半結構化的網頁資料</li><li>因此我們在這裡需要透過BeautifulSoup來幫我們把資料轉成DataFrame</li></ul><p><img src="https://lh3.googleusercontent.com/8kbpnn8SQruf0Xfl4IfWlcyLFat45mArTP1BpKHfYvfORfxBETYjBBNSZwfldIfr6bXfYUS8vuI_DT1qGK3bnR4fbbUAnK0PPby4q7YhbiA65iwb5GkAtjJkF0_kIfCG6aH_RmHPO3MHciwm5HL7l5_MzrA36P5s1RVpIsG86SgBHbbNxM7E0aDHBe6Bao50Vq0T4dcjBvbA3gntftpCo_lUpI5718IyNwbZew4LP7SssN1wNdGCoQfY7k4EA-teS7Ob3F_tRem0CoSSL_t3q30CKDHy0iNwV6Lkxfv-0qXU7GYQpwh58G7HSdBffew3zoZcyAg5hhU1v5lmKeKvY0rPnnh9ilhOJ5JgsVhV4mv7mhbf8g6jrRV1HW2I_ahu6uvcgl-8EbRqApN3xbtR-J47V7qBMRLozhHyBP3ylvB-1RgvLSRB7nIRZqYJLmDXAjc8b2HZ5HvtKJ3qtnA5Hq0Ja3A6bE48m3yBARewfD-AQ5QWktrB0MfhNSpnMGSAgDc_pzkLy1DqTrQaZt56pQURsSxOuud09Wga_tjG6WDxBCFZxu9p93hqN4b44kd-Qdq9a51LT9FjOlkDb95tXsZU7OoqmQoZIVgtYCGl2ywm5EBMxjZ-DnShlB2OypWegn-Jzrs5Z8rGYGDqXEk7caC6VqvpPSmAJXJqG6Ju0EJQf71L_bBG84g=w1510-h478-no" alt=""></p><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><ul><li>在辛苦(?)的抓完資料後，最後就只剩下把資料保存下來囉!</li><li>先跟 Google Drive 空間串接，再將資料存在 Google Drive 上</li><li>資料放在 Google Drive 上的 Colab Notebooks 資料夾中<br><img src="https://lh3.googleusercontent.com/1RY9JXc87whTGNVpHY8GNm77RarneoCpsxulPb5vj6wfXeVyWIK7fZBDibHwqNOzCts_l01rF4cRfW4DT_PgQw5TTiy2GJzPkBOHVUF5Se406acjFQeu-5aIdJaSPN2BfZUe9IcH8jsejbAkBIKV-bnLT3iSwDcQDrEWWPShM-aY6Ik-ZCVgg0ru5gEzl69wz9jxslwry5qIlm1bqfFgsr9thO8KxVdWK7izA4PkHv9KtRW09_wmPr7XxfomxanTKhe2l65vpe_oTs36w03dz8huj3tKWHo9ZafJ2N4gSqdPfiabN47tKetfHlgRY48nV99oVeIQTMSehsIm9hS7ppdygWz_h04W5oR1-q4mvIREU3wwbpojD_9ae87F4aTIiLEVHKi9TOvJ9RXkgKI-z-pShLKrrEgMcSdb0XxHMXWEQNqLSFZ3u39GRG2yJMDMmF0Fr_Tj3kj7RCqRCezX0c4OsgR81NMV6Wt5CdB6NvbxbQX2HOxfc0txXze0hdGjgwaTsBf7dFYWFCw3iKBVqlVbdE0bywKFqky1i2GnxVBWOIgTijG-B48BHy_tDhWpvxB3bUnYGtfotQsMV6HExX2T4iii8EdAfpV8zHdHEAvY3yOuN_lDUfz7Iw67VZQjVx9M7M5i3tQPYJLoOCf-s8xNk9V_XCzwwCNsYsUVfZNXK8-4CzdzGGg=w1508-h67-no" alt=""><br><img src="https://lh3.googleusercontent.com/vbM-BfMd3-qRvcXEo0ROQj_xubD6sDgxRenFirWnqQyMZKTCDiOH0R3r6ygLNNzgs32iUgHYy-TotV2HlaI240JJMV4NN1dcHZvRVQtVeQx8iQsiO5yEZ3P9wwXN1PKit9q_yR0i_RcL4MR5UAa3o0VuEqPqHKKhAg7orRZu0S6_C0lFtJFPfyTRNqsiR33iFkWvZDPzOuvjjzQXWqCue7ufVALDV_B0qU_6Nx8dkNcbPJ_wLI-DEriNX8zqTVa8hU9D7rZ-zaMjNO3BtNpK7uW2x1bi-xK97eHBmK1qENnups7kIvUb4ukyKZC2t88w3thrO25vwlxgc-oyZvAXB_23CiUeE7pbGvBwypdfAqg76WDm31-RZMb_HrlhB0LaR2cXaj3Kp7JUjZNNMce4dH5WppTuVI_j5BXikdkaW_igKv0MO6i-s9Ikuuq7Dt6pJxzgjL3nWsypQAMwhGiSzoSFzYVJmLyHz85WTBbWxp5xQcX5b4YCOj3qk0I9B3vbClvj5utdmYl8XKjtGrNL8HMhwY0FqjdMXirhfFmftH4XL-bip9fU6xM__1DT0ofUgh-xo-io1hfsqWz8VutTDixxv9ry-SXwvAIOliMD9WKH2rzsa4iRo6KhpU5RvsJI7ZI8136j3AlyNMdGjY07Jx7ghy72QrGZwWMmn1JOUf36mAy_eWPUE4k=w1507-h85-no" alt=""></li></ul><p><img src="https://lh3.googleusercontent.com/XqGbKWIZbCLvd8iGRR41WLodgwts4h9B7NEEWBh_Mg2Xo1k07Fm8FP5i-K_si7_fq1-9EuMtyTa8a-UC7_jupFpdtg8ePU2NSZgjtxya5QV115eKFcUrQeytCRqteC-XJ13otanJFZOkbm7LwG8tt0gRrSF7qVaWaOwcG2XTLVdVrJvvAnxzrK2f1G3eQEz7gZ9zLOyZMU_X9dnMh-Tx5I7ajmYgOFs1ulghQH5S54eLD_hu62mUyGOg8qc06xH7ms1K2TTNnoifjshDMfOzU8eTbfakyoV9Wt3SiZlplLxpQ-1Hxps4SAeNrHZB2MdP3J14rR5Vjz20EWscSY6t2Z1QESIYbBS8RZO9qZG3g83WnVGA9r195OiKTXKX70wVjlPGSzz8cLPE05QXpOM9ZcL_YZAgZc5eKtI8fjcFUsyiEeUyfbVluEdYNScPOizNLsm4ovhkuzROfmTwBAo_we3qysHbqHgvvRTYOq5PKV4MjiwD0Ytmi4cvrccpt_YRzMvDCfxKWmxwmbiaEhTZNVy7nQ4JH0xxDM3akaIfu34voRjUovMfalsns28I4gK-a11Ux96saA9E1VY5AG4PY_Gy3xXKXEIYBl6dznEFW7IBJt3Wtk1x18tmlJNpW7bsnI9qBuqIyNyM-gm0NtffNhxKoV2C0ayEpLnJ0tD-AKkzPOfbGOCJWvE=w1920-h878-no" alt=""></p><h1>後記</h1><ul><li>我將程式碼都放在Colab上了，如果你/妳有想要抓取的網頁，可以直接打開這份 <a href="https://drive.google.com/file/d/1_jgNnCa8SYXJK-XO3VzG6TdvZ4JMK11u/view?usp=sharing" target="_blank" rel="noopener">GooglePlay_Crawler.ipynb</a> 執行程式!</li><li>這裡抓到的資料也挺適合用來訓練情感評分模型，因為客戶在留言的同時會一併評分。並將訓練出的模型套用在其他沒有標註的資料上(如 Facebook 的留言)</li><li>如果有問題的話歡迎在底下留言或寫信給我，最後祝大家聖誕節快樂囉🙂</li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 網路爬蟲 </tag>
            
            <tag> Selenium </tag>
            
            <tag> 評分 </tag>
            
            <tag> GooglePlay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何透過爬蟲抓取中時新聞</title>
      <link href="/2019/10/30/Crawl-ChinaTimes/"/>
      <url>/2019/10/30/Crawl-ChinaTimes/</url>
      <content type="html"><![CDATA[<p>以前我們在收集新聞資料時，是透過複製、貼上的方式逐一的把將新聞的標題、時間、內文…等等資訊從網頁上複製進 Excel 的表格當中。但是當我們需要收集上千、上萬篇新聞的時候這種方法就顯得不太可行。</p><p>那我們要怎麼解決呢?其實只需要引入網路爬蟲的技術，就能透過電腦幫我們自動瀏覽、收集指定網頁的資訊。今天我們要透過 python 撰寫網路爬蟲，練習在中時電子報的網站上爬取新聞的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊。</p><a id="more"></a><h1>先看結果</h1><ul><li>下圖是我們將新聞爬下來之後的結果，我們會將上面提到的新聞標題、時間、內文等等資訊整理成Dataframe</li><li>有需要下載的人也可以在與Google Drive連結，將資料下載到自己的電腦中<br><img src="https://lh3.googleusercontent.com/N8gNwNU6T7-ZGiKIyCee4wq87LsHDPv23QWgou6AzpqOm-8bkkeuCG_DB44Jt6mUv3NYx0VCH5mKMRZhvI0wsGznuwTlllNLwem5faa94G8i8GO_W8xAHNNVvLY6eYNxBDfMaXEwkFOtGSeTxrd1pqF-bFKWWvnPQGUPBOYHA2CZxA657AYw1sjGIXQMMwqZCFRmzviU_7bTPuHPV0vYR71S-5sKPk6ZL23Suv5RcV5s3z1o3JbOJI7Ou6TBSLOcxskn7Y-i3t8vly1DiQAF85SaRB_8SI5hI8uO1FJnqVUuKpGKcSOkdFB3gIVV9gMxNjFlVN7m4Xt3idCZYm3x_nQ2yz-CnpAA-tXlU5-5Jjjc4aQEJHEKB01xj4vgkjb5Anc9vqA498XJsGMFT5ypGgra3wx97mfpxtFS08QViz_ogQ5DLLHQkRx8jVWqoWMUtbQIyXcCx5YwCJESJGEX7J6tNu-MDVIa2La5UiSpqC7gPFHyiLpe5N42p4aMkXOoIjQ0T5SGomzVF-kFdqfzB4NlPd8rwR0WZID4Ay-OgOg68LCwMPjDPWChbMJvTU85znuWSlCYre-WLkuB3c6nCzDVuYNPQUbaQmVurZyQgZwRP--JD-3x4HZm-OqK20zzlQRYtaaQvs6RmwEq1bYyfuSquLn1mCHztBruzxkyLSD9c9fpYonA96M6C0LcijLlJZ4pV8HMAsosApx1znI-YN6qAc-lDBW6HwHGfM8q2zus0RnY=w1840-h413-no" alt="新聞爬蟲結果.jpg"></li></ul><h1>抓取一篇新聞</h1><h2 id="解析網站結構"><a class="header-anchor" href="#解析網站結構">¶</a>解析網站結構</h2><ul><li><p>在開始爬蟲之前，我們需要知道怎麼從複雜的網站結構中找到我們需要的資訊的位置，接著才能透過爬蟲自動幫我們爬取資料。</p></li><li><p>那麼在上面提到的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊又放在網站結構的哪裡呢?</p><ul><li>以中時電子報的新聞 <a href="https://www.chinatimes.com/realtimenews/20191030001004-260408" target="_blank" rel="noopener">英發動大選歐盟換領導人 歐英新火花受矚目</a> 作為範例，打開這個連結後會看到新聞網頁(左半)，接著按下 F12 或 點擊 滑鼠右鍵 接著選「檢查」，我們就可以看到網站的架構囉(右半)!<br><img src="https://lh3.googleusercontent.com/6bTMD0hNVHYTNZwkXc0WzpT_Y46ZlJILp2M9XeTOEPlFJDD0ktrZVC6KcQpXFfvhtE_nO4lCka4FnMMWWX0MVLiDZK-lkOpOmliJfjSRgt5vUobV3Dm5QC46fmRt18MPGRO5cvY5BHsp4RwZIo-ppTaUmIwkpD5dPopTHGn7ybFo8Ijm8tpCIiYbOZEc4g9q0VEmVtYKSEb8KPh6RpawkDMqF-s-nMNUqJxUplMBZG02IKibjLeeaeEOv-gWcQfoF4dp0rHJrraeruZLJB15kYDJ2GJTfF7IuYFRtgcAvw318WLdPFeSy_L6GtAmktI4ufK-hHUpYsVGmY--ESlAjVRIAYwkXDK6TqTdP8ZFYKgEEpe5xBYu2mksYmryl0Hi5iVARC0dSSAsRJ_nClYp6QbFsSEHKo-vQkITGSwLcCvCTvpD1Qwx8h5db-CEuUj-FjFwNXLL6vgIYme5zTmIoKH019D-pwUpomkgCUKx5A9vV5uBGYpxGljfKsOYwczquCYPl3kjzHL93S7vBZK5MhOspgqzl1l1gA0FOYwI22OAHpG97M-BND-GZKxjx_GLQ2GreG7R7PQ22WdarBZlBNpWN9BuS1C6NPCrGFzyzP9esLGdCU41k_UqLLNOnkH7mVIXNRUhBKXpYBLJZiul9PFrSh3916NQ8i0_lvfn4NSIDARnaaHFj0CkRavqBY31GW81zY5SwBEO9Yn_zqq7sqAuQr9T4mpl82eaJ694C-pf8mJo=w1739-h978-no" alt="網站結構.jpg"></li></ul></li><li><p>其實我們看到左邊這麽漂亮的版面格式都是來自於右邊的網頁結構中，也因此我們需要的資訊都能夠在右邊複雜的程式碼找到</p><ul><li>以新聞標題為例，我們就可以複製左邊的新聞標題，並在右邊的視窗中使用搜尋功能(CTRL + F)，就能夠找到我們要的資訊位置囉!如下圖，當我搜尋新聞標題之後，就會發現標題藏在名稱為 script 且type 為 “application/ld+json” 的元素(element)當中</li><li>另外我們還發現新聞的發佈時間、新聞類型、連結等等資訊也都在這個元素裡面<br><img src="https://lh3.googleusercontent.com/MY8vc8gGbLzMt02Ai9FI9hhTZTfu_hMmGBAz-0n1ZkwSuoAe_Wh3TUjQBkx0sPq609thcEwP1tKPUD_VuNmaIJPBfc0OeaA0REe2ob0jZgNmwTVNvwRt78FhaZUqjnYVazz3pdDxZT4bC1TQJVsmqeLf6FFnoY1mA_dgS56rHSn2dLTSmajL31dkl0CRYEM_CVTWHQZ38cb91t16DTX_aPWCsXN9j0Y-1AtW5IlcXAeXda3hIAkGCkm-ojFlB66U0fFjnUXDcIW3XBSnppC8K__Knjpn9ZjywDNxAwmX2t8QUmtVhuazbgczbStaqZmeAhAdXAZcuaOkgcMh1eV9HppzG0BiRxXdIj5NSxDJKs2nNk8AKaWhcH1NJM5IVAm7K4X7NtrPImVblqUB--L7vQHS331EuQvamcJk20Y4w9f9sQp-DE6hqox-wBDa8o5XqVsuz1LTvkqXgjJLvUShArXqXAYE7gyITn39V4ufg1G4zkuZ2chPh-Xq4VCQAHxEDHCJW1uHWL8Ag-4AApvZbQyr4tAYpbNFrgPLb3whGoxvv6hMW74zkqMuelDCwI1vXNvuc-xMbohRPjHMcQEO3unumYZK-XPxjjkgwWI7cmC2t6rO_27GD1hYAo2nSImdlxccYDHZ1qtKrQzXRE2cAFvJ_MI1434LsvLLAR6abs6U3n6McDmQvwbIqrO9Heb8TRM_8Tmy89jwZ5LRpxGd1XvW8yR_aHGFOYmLb14IZtxK6Dhl=w1418-h978-no" alt=""></li></ul></li><li><p>經過一番找尋後，我們會找到我們要的資訊分別在以下的位置當中</p><ul><li>標題： soup.find(‘h1’, attrs={‘class’:‘article-title’}).text</li><li>時間： soup.find(‘meta’, attrs={‘property’:‘article:published_time’})[‘content’]</li><li>類型： soup.find(‘meta’,attrs={‘property’:‘article:section’})[‘content’]</li><li>摘要： soup.find(‘meta’,attrs={‘name’:‘description’})[‘content’]</li><li>內文： soup.find(‘div’,attrs={‘class’:‘article-body’}).text</li><li>關鍵詞： soup.find(‘meta’,{‘name’:‘news_keywords’})[‘content’]</li><li>新聞源： soup.find(‘meta’,{‘name’:‘publisher’})[‘content’]</li><li>連結： soup.find(‘meta’, {‘property’:‘og:url’})[‘content’]</li></ul></li></ul><h2 id="撰寫爬蟲函數"><a class="header-anchor" href="#撰寫爬蟲函數">¶</a>撰寫爬蟲函數</h2><ul><li>盤點完我們需要的資訊的位置後，我們就可以把這些資料放進 DataFrame 囉!</li><li>在這裡我寫了一個簡單的 GetNews_chinatimes 函數，讓我們只要輸入新聞網址，就回傳資料表供後續使用<br><img src="https://lh3.googleusercontent.com/1O-4wLHHQ78GCOzmgAK71p3mHasOtfBVEwI45doSG2T_N0_z5ZP1d2U143osp7oL1lFi8d5UWC53A0WtMokhuleNfLwtTAilQzKaQUP-zbHqqFVgfUN5zraUyEG57MifDVCdRlcqg3xlm40LAz3TtlCO40RjaF_S-BUvKRq2fO3UPVldfFoLuF1wPZW8jGFys-JR9roOd-tygiUHXf8RaESWszJvoKyyxhDJFrttGltTRlSmFpm5SFX5iwCUcyL3cvHXthXt3j-YfR3gYP2ndx3CJE63tfzHOAubKNrC23TQbezeOec0AhSXtIZRd-zOUDZduqLQcfBi53eRm5bfmVc4ZpgcOnqFq2H1VUDIQZUoJiG_pF95ps1GdY_uyW4NqS9GchwQfVTj9JJ5_EEL1dBS8sf7YMyuh_m8rfzi0B6Aab9-5yn17yB5nghjaPmhpG7yIDJU84m4p7lEwYBJduJSHABsyUjhm95QsShyzTS16_nLRoiSDc8B_LJMZlo0tQvhoZa8jq_QjQWzmkGl2d_yjL2EJrsfIqRAB5Y_G1xOKvAzjHyxcUDVJJ3dk9Keh9YVkxFRarsPLqdVevH_aguIrPUO7eMI9pr4DC-RA1nYyeY5yN7GO3GQog4igKFOX5oE2gV05QFszZuVB1P-O6EyA7PB9eJ6CkRjPQF7KXRW-JDgI0w5yyzKrqnye5udEhag5z7_xGgiAj6Eh-e9qUc7G3qhLRAl9hHj-IxqNqOuyfBi=w1861-h526-no" alt=""></li></ul><h1>搜尋特定關鍵詞的新聞清單</h1><h2 id="解析網站結構-v2"><a class="header-anchor" href="#解析網站結構-v2">¶</a>解析網站結構</h2><ul><li>雖然上面已經寫好 GetNews_chinatimes 函數，但仍不能滿足實務上的需求，因為我們不會只需要一篇新聞，我們想要抓取的是大量的新聞!</li><li>因此我們需要運用網站上的搜尋引擎功能，幫我們搜尋特定關鍵詞的新聞，並將回傳結果中的新聞連結都抓下來，而要抓下來一樣得先找到新聞連結放在網站結構中的位置<ul><li>這裡我搜尋的關鍵詞是<a href="https://www.chinatimes.com/search/%E7%BE%8E%E5%9C%8B?chdtv" target="_blank" rel="noopener">「美國」</a>(左半)，並透過 F12 的檢查功能找出連結藏在 h3 元素下的 a 元素當中(右半)<br><img src="https://lh3.googleusercontent.com/JwEFSUZGq6nPNiZidVByMG6eyVPUt9U8uJVARkd4D9t7h9VZTPLbZOfTgsbmLh4TOBLrM08XevDHbc7UW29fX0tMW4tpds9Q0TTSKEiJYzA8zgeB3h1CbsqN2TfXc7SzL9sPB2YrUMzf-D5jvX25JsOCHSOW2yNhFQyc0-0343asVUDJCeOBibIiARPuxz3QLGG-zPAQ6tUHtx3j-9VmOa0nL5dRmKb2c_-uF--GGKv7utloxgiDnoD0OqdZbhfxrVSTCzHftAjAbVg6JWKYSWwqcaHe8_0fsRBRPP2NxfM7HSvzZWKvsahkZscTiilyGwblN0fODArJxZjike1xCCizpocBTbkjBv05A_LXn3-XqcPwdtC1RV2R_a7sPLNKDPF1boy3X40qG7nMWSoYTQPCJ4oT9As4ZMnHNJ5w5wQXlmfA4g-cKGesxVVtLJ-r9cN3ycz9NmpqAgF6H9xVjL2oXmQF8Orr-OHKpHT1hMHHV0BTL4FjhkNN2BQq9bgwKx8LsEcIy-D2MLIdy0o9PqoMjSsowEE6cAD3S4I4622KG_0ja10qSAtvI0hfxztS5vRPl-rVE2kT3yC8M1ssu2afnmJjdsM9xhGNwlnsZAUwFpAHejP1AdTXTZLprAo4DgbKLxxOxnb8qjfHQv6l00Jcf1iSys-gGF1FfKjXDtDl-sftztM0glqXTPjdEOJOXoUpAVNSru1PULNZ-IssPuoQm2L3nMpUTdhOB8_5N9VWK9rS=w1787-h978-no" alt=""></li></ul></li></ul><h2 id="撰寫爬蟲函數-v2"><a class="header-anchor" href="#撰寫爬蟲函數-v2">¶</a>撰寫爬蟲函數</h2><ul><li>要抓取連結的方式非常簡單，只需要找尋 h3 下 a 的 ‘href’ 屬性就可以了!<br><img src="https://lh3.googleusercontent.com/KgL3_3_7QDzaPngbVCTwN33x3ShFgiaZJM_ZERhfc35Nnr7eEwuD3Tf64TVsSLD0f27EnvzgiyFjWrGeNEUgWm2zLKFGHsFv9z6yyI2FEQQBspN96T-klIgEpVeAHpmot9Ci8HKLXlOX2gkHiko6dd2Qw_CC7fW9w52j9sJqTIxP8vNpKzo_fcRdWEalvvuZlNhpBmzuo-cbUBpqA-6_MkMLpdGqh7poC8HB-o6lvP1Hcpd8EatSamX6cZ-ppb8KP533NfXmgRvLB95vKxrYuWmB3rWHxdfqZ3l_xTsJuLx2qo_8QBLSqS4tBE13O_JZrfetHV0FxGvEp_Vw_ILl02Jvg3tC6meXahi0H8p2XY_lbRbQTYuoXDkNn2tkNYGncFCfniCS3Ek-viClaEhdMfmtzygwzPpR9PFlATipEHlFlj8MbeOhNHRwpqC_6w37JtNNDRGPygtMagEbatvoYQrvnJkO9_ldZZXXmhoWOCO4PY9aeeUHlajpHelhwVdnYH5VBwFtccblV2w467-BdxCDbzNQxU4ZiNnzakpCaYrx8h61MrcB9rw0uZEvDKOVyHTUYaOxWhxzkvg5GM9SwJJ2vEDsFImlDqXHpJ7uYeY0lBQXSSBMS_wfFpbwvlxxj2lpLISpHadsYi1c8z2aOa_wDhqw1Wb3GXzP4I-vNVnJMUPW_dcGECrnbA7vX54YrQo6tgpG3N9RQ2QERQYqIqtqT1hN12C4d2evRlXY0bAV-8LK=w1789-h978-no" alt=""></li></ul><h1>開啟多線程功能</h1><ul><li>只要組合以上兩個函數，其實就已經可以實現自動化的爬蟲作業了，具體方式其實就是逐一的把新聞連結清單放入 for 迴圈中，並透過 GetNews_chinatimes 函數解析新聞就可以了</li><li>這樣的方式在少量資料的時候還可以，但當我們需要抓上千、萬篇新聞時，「逐一」這件事請就顯得非常沒有效率。那有什麼辦法解決呢? 答案是我們可以運用多核心多線程的功能同時抓取多篇新聞內容，只需要運用 tomorrow 套件就能讓開啟多線程變得非常簡單，使用方式如下<br><img src="https://lh3.googleusercontent.com/Ru1GOLmFbwajDukJ7y5iu2Gi7YeDkqau6Kug_ctByP_hMNUkv6H3F4uukxOpEuyzHdYQ_DWne_DuMbnoRsZqtQncplZGBLkQH7HYiudDjee3ssrGYS1m_HnLdOMuB4UuEL5a1E4TlT7Q4EBBVqaqud5w5C_5jTUrse12z5Uego3h_Dk4nCZsHzYucNQSuPMzG7eSrWI6mk0ZaXbV5D7wwHavziP9vgElg1-B8htkNbJtoDZtuR_SHSMIAZlD3U0myd6x-tIgWQSevfNB7IGtI4ujmgLqg3KXNXsD2X2evUfUk4GQ7nIbx1Mj-IR8Z72PwpGR5k9Y9bJU6mlHQcfjzdJNdaOrpbZIjMqBXZLY93wpnOqEoU75aX3i4pg-Hxa38EI7GN8lVBvIN2EQUjAdjYYiL53b7UuPm5SLxfIlDqToxQRGFx-XwQBuZoJwozhnMpSo8ju4JFU5b6e4UXYY18WwWVh32F0fxbO3Hj2L3hoDhZoH9cEMm33WlIRcEimWx_V41gsDVMZ3jIPx9q4qFaK14obE1G1kYrGNa0J0lpFS7TVa9NlBLDT_Fys5FmNL5Dcm2cmI9oNRbOBq00RbyWarEabelG6iV3MSTA-XX3943uMTPtrQS6PmuX5W-EoyNtTKTgYDJqpua-5HcLA-KzOXqI5tqC79GeBido8C2Z1bCyzAsobpDrA7XrDhUYz6qJA7Z5oSFXX6ec4h56dgil2y61iW65fxV8XjbXk6GBjQv304=w1878-h202-no" alt=""><blockquote><p>使用提醒：</p><ul><li>threads 數開得太高有可能會導致被網站封鎖 IP 而禁止連線。</li><li>具體要設定多少 threads 數才不會被鎖?這需要慢慢嘗試過後能才知道，每個網站的反爬蟲機制不太一致。</li><li>建議在抓資料時使用手機分享的網路，因為被封鎖 IP 狀況發生時只需要開啟飛航模式再關閉，就會替換成新 IP 而解除封鎖囉!</li></ul></blockquote></li></ul><h1>組合應用</h1><ul><li>最後我們就來組合以上提到的內容，嘗試抓取大量的新聞資料。</li><li>具體的操作流程如下<ol><li>在搜尋引擎搜索我們感興趣的關鍵詞</li><li>收集各個分頁的新聞連結清單</li><li>開啟多線程抓取上千篇新聞資料</li></ol></li></ul><h2 id="定義函數"><a class="header-anchor" href="#定義函數">¶</a>定義函數</h2><ul><li><p>輸入想要查詢的關鍵詞與要抓幾個分頁的資料<br><img src="https://lh3.googleusercontent.com/EgzhP7GsbJxKBGY8-MIbyEnzVZSfTAcwRw_ITzkOthcph4pRVGHFXsUGyiPufbm6V2AXeJBwzbp-gBtlvXMuothZj4d4Ox3wwyfm2GFjNgzvHtr_M3m6-WC8eipt4BeVUemEbypkric5hJN7_-z4NLy44WmQcZJMO167NBQXqmNuFonatvmdVipfEkzJOeZwuHQdrv_w0UGeKWefx63FpuhPCbAA_LzK-iN6odEJ9cPCfqTm1nMVWlKKl6tbabaj-kyEio-rihYIMrhv8Iuao0j0cVasIhMT7zusm2Of90RQlIPGqlT1vGnn5dOMmcKPXrv9-oDPURnHqnptujePrcuiDaR_mQNfC2PDSSRDg4XZVmf68rzw0H_5UT_pQLsnnuKFmCpLZtnyFz3ubU0ibgzBU0fQQM04E6xtCAvlN9kO16sPMEZzExMS7W9hKkyJDBRaAQfbBSwdaRtQsm8G2T4WAffqanzjVG53J3GSVita_LydGgZlnxcvzzn1IrkAMa43Kb0aO7Bupei3Bv6PsrUp_i67UFNkRo5shPyw0bF_ozAfVTRnL0q0QyP5n_297wbBXor2jG7zcg3sWaMvlh7W-bPvXpkh7zyzcYhT_REqlTZYCKjcr98czUKZd0wxVcp1Y6Fq8xiQM88u9J84D71jK5cS9qqf639ck7skjY11Ak19srFASAapTAAfXmkCyI88POXQvqguekwPHB80EWkpXjRuteX1oX7gvEX0qzvdcMn4=w1874-h525-no" alt=""></p></li><li><p>使用定義完的函數抓取資料<br><img src="https://lh3.googleusercontent.com/DZu_DrjAq13cKmvz2B2vDQopBCo3uJ-U1izOWb2rkJObUZejoXxZl14ke0ETNrvbJsyrkpM3LSMKt4hNT--DMNalxuJg6774RH_9VdyNEzx4UKQd4atGw41ZhZGAoGQN4DSZNk7ihpUAfS6F-GKRtyy_pmqTBRxMXSJUVxobCTclTHTZj0YBKQ2ELHSqGJ6UkDXy2w8yVOahP2B34xv5EUvMfWSXbJ2QoFQ6dYO-x_Z9AamnpPYXu3MNIOZmiOFrkNfChu02G337g3ufdw6qAZCoXzC3hysrUpKmY69RNG1IBGs_BZj5hNQlGsGoUrdMOSbfVwezPOXPrMvrm-fFE0JqqHBNvyB-9Lm_pza9oRggCufuSdSpItCQTzfbh4D73GzT6ha9l20d1ovYru6RFwmx2SrKdBhni0rBhwkvlmHGkue83ecjKp5_XhfXRNLgIly466i9T617iil8ABpW3izK4UzrTdXd96yrsKu6aHfddX3_jET6Ib_0uT7xKs0lTN-3E2DBzXMWv97We8N22916AIW6NgvV9T1RCk_9G-ec0y9G-0wIlUieKuVI-Ie2ul1FjktP4ymcMC_k9UB7MIQP74p5r9jmM8rqaEXxohL-Fb1n92DCqWKH8iVDesWML5Js_Y2Tj6HbcApq2WlgF7d7Oe6KZX-HxvoQG3ogvTYphUW4Y5LN4K9Upq62vlyX1cjEVmU63nZIflxO8sLstkaVKmKcKcGh5hbH30Wut-sSLMJ1=w1876-h859-no" alt=""></p></li></ul><h1>保存結果</h1><ul><li>將資料存在 Google Drive 空間中，有需要提取資料的人可以從 Google Drive 空間下載到本機<br><img src="https://lh3.googleusercontent.com/2J2k1g29zujtoSpyi0gGhYRxruAd6KSkAC9ojdDBui5qDKmi6_m_RFg7-VYEl8FVDimmOrIm8SUlzuPRR8vUZ9MSb0X5wQQMMcMZArufTCoWqOS6QZavpUKbyPn_UdFw19KUrLnHWSzHebFduhfhs2UdX1SR1yPwtQk2WB4nUgYUbHPe3RQbikQ0QhzRsDFo0EVhS6Ik-fwpGdho-8IcQLH2PNoBhB2yQF5gA9FWt3Fza9g8lCegL2h3BJ8QNJa-qzCAGj2VTpbI6dUNXpqB3opixORD28rg9xWBg9DMq70G1GANij20dzpqCqI3_IgNT7kPfC-ToMPmULkjvPwMbzOA7hdPZrbSWMIsyYrdF24pmjcNf16E7b2K5cYK6UR_OJklMXxpZKkyLSweZAUFBLAtfB3jc6HMlH9iY_oEc9adz23MCzny_qsAQou--Cnj_ituBQ1DV9ObHwhw8oDeHv3bjfSUcWlsEf90EhIjeXVB7f4dS9NciVv1JZmrMOzBQAuODAmJOm9JLtht9tGJzLzkb5IalfQmH1KIK6g8PXnAOyF4skq28b9pO6Ul5KtRkdI4cfjSPMqjHOCCORE3DALvMkS4STQatXNCXZzINaCd6va4K5iSTabiMgszqIc62ylM8fYZUwyWhkOR1H-fwoU0Nu3bHZJqAr5O-nd-fgJaxh5UfdAHRwcmZMqKufSeazXehBFDhpv1M3XJ1Y2cKsf9XHiEQt6J9sCtGBK2eonBPW4e=w1874-h476-no" alt=""></li></ul><h1>後記</h1><ul><li>以上是中時電子報的爬蟲實作，完整的程式代碼我已經上傳至 Google 的 Colab空間，只要點擊 <a href="https://colab.research.google.com/drive/12CvYz3OMLgMl1dVW69KEO5IhWgsqUJCj" target="_blank" rel="noopener">中時電子報新聞爬蟲.ipynb</a> 就可以直接在線上執行這個爬蟲程式，並將結果保存在自己的Google Drive 空間囉</li><li>如果覺得有幫助或者相關建議的話歡迎在底下留言給我~</li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 網路爬蟲 </tag>
            
            <tag> 中時電子報 </tag>
            
            <tag> 新聞 </tag>
            
            <tag> 多線程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>網路爬蟲_Facebook粉絲團貼文與留言</title>
      <link href="/2019/05/01/Crawl-Facebook/"/>
      <url>/2019/05/01/Crawl-Facebook/</url>
      <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>上個月參加某公司的職缺面試時，面試長官詢問「有沒有辦法將臉書上的貼文時間、內容、按讚數、留言數與分享數、甚至是粉絲的留言資訊都抓下來？」</p><p>受限於當時還不太熟悉爬蟲，只能簡單回應說透過python與Selenium應該是可以，但卻沒辦法說出更詳細的方法與步驟。後來經過一番研究，總算知道怎麼把這些資料爬來下來了！廢話不多說，我們就開始吧！</p><a id="more"></a><h1>輸入與輸出結果</h1><p>這是我們要爬的臉書頁面，從畫面上我們可以找到許多住要的資訊，如貼文的人名、ID、貼文發佈時間、貼文內容、多少個心情數量（按讚、生氣或哈哈）、以及留言數與分享數等等資訊。如果貼文底下有留言，我們也希望一併把這些留言都抓下來。</p><p><img src="/2019/05/01/Crawl-Facebook/01.JPG" alt="01.JPG"><br>而這是我們抓下來的結果，結果分成兩張表格：</p><p>第一張表格紀錄貼文資訊與互動摘要</p><ul><li>貼文資訊<ul><li>Name：貼文者名稱</li><li>ID：Facebook的客戶編號</li><li>Time：貼文發佈時間</li><li>Content：貼文內容</li></ul></li><li>互動摘要<ul><li>Like：按讚數</li><li>ANGER：生氣數</li><li>HAHA：哈哈數</li><li>commentcount：留言數</li><li>share：分享數</li></ul></li></ul><p>第二張表格則是記錄粉絲留言的資訊。</p><ul><li>留言資訊<ul><li>CommentID：留言人ID</li><li>CommentName：留言人姓名</li><li>CommentTime：留言時間</li><li>CommentContent：留言內容</li><li>Link：貼文連結</li></ul></li></ul><p><img src="/2019/05/01/Crawl-Facebook/02.JPG" alt="02.JPG"></p><h1>過程中曾遇過的坑</h1><p>在這裡先分享一些我在過程中遇到的坑與解決方式</p><ul><li>瀑布式網頁：網頁不會一次把所有貼文都加載給你，當我們把頁面滾動到最下端時才會加載新貼文。<ul><li>解決方案：透過java指令讓視窗滾動到底部</li></ul></li><li>系統彈窗：當我們把頁面往下滾動後，會彈出一個請我們登入或註冊的視窗，阻礙我們爬資料的流程。<ul><li>解決方案：偵測「Not Now」的element位置，並透過程式點擊這個element<br><img src="/2019/05/01/Crawl-Facebook/03.JPG" alt="03.JPG"></li></ul></li><li>心情互動：心情互動分成「讚」、「生氣」與「哈哈」等心情，這邊的坑在於當心情數量大於1才會顯示，若我們單純的設定要抓生氣的心情數量，而該篇文章沒有這個心情，就會顯示錯誤。<ul><li>解決方式：抓資料要運用try-except的方式嘗試抓該項資料，若無法抓這個資料則帶入0</li></ul></li><li>檢視留言：留言不會自動載入我們需要點擊「Comments」後才會顯示留言。<ul><li>解決方式：偵測「Comments」的element位置，並透過程式點擊該element<br><img src="/2019/05/01/Crawl-Facebook/04.JPG" alt="04.JPG"></li></ul></li><li>看更多留言：即使點擊「Comments」後，也只會顯示部分留言，需要反覆點擊「More」後才能不斷加載資料，但問題在於我們不知道到底要點幾次。<ul><li>解決方式：透過while迴圈，偵測頁面上是否還有「More comments」的選項能點選，停止的條件沒有「More comments」後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/05.JPG" alt="05.JPG"></li></ul></li><li>看更多內容：留言中若內容太長，系統只會顯示部分留言，需要點擊「See more」的選項後才會顯示完整訊息。<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/06.JPG" alt="06.JPG"></li></ul></li><li>看更多回覆：除了回應給貼文的留言之外，還有另一種留言是在回應別人的留言。我們也需要將這些留言抓下來<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/07.JPG" alt="07.JPG"></li></ul></li><li>相同element名稱：透過Chrome的檢查功能，我們可以看到我們想要的資訊放在span的timestampContent位置。但是我們如果只輸入這個條件並沒有法辦找出正確的資訊。因為在這個頁面中相同條件的element有許多筆…<ul><li>解決方式：抓資料應用逐層搜索的方式擷取資料，在一開始就設定清楚要抓哪個大區塊中的這個element。<br><img src="/2019/05/01/Crawl-Facebook/08.JPG" alt="08.JPG"><br><img src="/2019/05/01/Crawl-Facebook/09.JPG" alt="09.JPG"></li></ul></li></ul><h1>程式代碼</h1><p>完整的程式代碼會在文末附上，在這裡大家可以先把焦點放在程式碼的理解<br>載入使用套件</p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p><img src="/2019/05/01/Crawl-Facebook/10.JPG" alt="10.JPG"></p><h2 id="搜尋貼文連結"><a class="header-anchor" href="#搜尋貼文連結">¶</a>搜尋貼文連結</h2><p>在這裡我們先定義一個函數，希望把網頁中各篇貼文的連結都找出來!<br>ulr放我們要爬的Facebook網址，n是稍後要送出幾次滾動網頁到底部的命令，藉以加載更多資料。<br><img src="/2019/05/01/Crawl-Facebook/11.JPG" alt="11.JPG"></p><h2 id="展開所有留言"><a class="header-anchor" href="#展開所有留言">¶</a>展開所有留言</h2><p>定義一個展開所有留言的函數，透過while迴圈反覆搜尋與點擊「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕。<br>在過程中會出現請我們登入或註冊的彈跳視窗，但我們不確定到底什麼時候會跳出，因此需要在過程中反覆偵測是有出現這個彈窗，若有就點擊「Not Now」<br><img src="/2019/05/01/Crawl-Facebook/12.JPG" alt="12.JPG"></p><h2 id="擷取貼文資訊與互動摘要"><a class="header-anchor" href="#擷取貼文資訊與互動摘要">¶</a>擷取貼文資訊與互動摘要</h2><p>透過逐層搜索的方式，逐步定位我們要找的資訊<br>在這個環節需要反覆透過Chrome的功能比對資料，需要花一些心力進行比對<br>另外在這部分也使用了大量個try-except，原因是許多資料是有內容才會出現。例如並非每天貼人都會收到「哈哈」、「生氣」的心情。<br><img src="/2019/05/01/Crawl-Facebook/13.JPG" alt="13.JPG"></p><h2 id="擷取粉絲留言資訊"><a class="header-anchor" href="#擷取粉絲留言資訊">¶</a>擷取粉絲留言資訊</h2><p>這邊要留意雖然都是粉絲留言，但實際上分成「回應貼文的留言」與「回應留言的留言」。<br>函數中的第一個迴圈是用來抓「回應貼文的留言」，第二個則是抓「回應留言的留言」。讀者可以自行比較一下兩個迴圈中不同的地方。<br><img src="/2019/05/01/Crawl-Facebook/14.JPG" alt="14.JPG"></p><h1>實作</h1><p>今天要爬的是<a href="https://zh-tw.facebook.com/taiwanmobile/" target="_blank" rel="noopener">台灣大哥大</a>的粉絲團頁面。暫時先設定加載20次資料，若想抓更多/更少資料的話可以自行調整n的數值。<br>注意這裡會透過Selenium開啟一個Chrome瀏覽器，若沒有下載的人可以參考<a href="https://medium.com/@NorthBei/%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%9Dpython-selenium-%E7%B0%A1%E6%98%93%E6%95%99%E5%AD%B8-eade1cd2d12d" target="_blank" rel="noopener">在Windows上安裝Python &amp; Selenium + 簡易教學</a>。<br><img src="/2019/05/01/Crawl-Facebook/15.JPG" alt="15.JPG"><br>接著先創造兩個DataFrame，一個放文章內容，另一個放留言內容。<br>我讓程式自動輸出目前的處理的網址，若有無法抓出的頁面也會送出一個訊息提醒。方便我們後續追蹤哪裡出現錯誤。<br><img src="/2019/05/01/Crawl-Facebook/16.JPG" alt="16.JPG"><br>跑完之後我們就可以看到抓下來的結果囉！<br><img src="/2019/05/01/Crawl-Facebook/17.JPG" alt="17.JPG"><br>將資料保存到桌面，打開檔案的結果在文章的開頭，這裡就不再重複放囉!<br><img src="/2019/05/01/Crawl-Facebook/18.JPG" alt="18.JPG"></p><h1>完整程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re, time, requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FindLinks</span><span class="params">(url, n)</span>:</span></span><br><span class="line">    Links = []</span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 這裡會跳出要我們登入的大畫面，找到「稍後再說」的按鈕並點擊</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">'//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">    soup = BeautifulSoup(driver.page_source)</span><br><span class="line">    posts = soup.findAll(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'clearfix y_c3pyo2ta3'</span>&#125;)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> posts:</span><br><span class="line">        Links.append(<span class="string">'https://www.facebook.com'</span> + i.find(<span class="string">'a'</span>,&#123;<span class="string">'class'</span>:<span class="string">'_5pcq'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'?'</span>,<span class="number">2</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> Links</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(url)</span>:</span></span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//a[@lang="en_US"]'</span>).click()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Now is in EN_US"</span>)</span><br><span class="line">    driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 點擊「comments」，藉以展開留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//a[@data-testid="UFI2CommentsCount/root"]'</span>).click()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.find_element_by_id(<span class="string">'expanding_cta_close_button'</span>).click() </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'There is no comment!'</span>)</span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k != <span class="number">0</span>:</span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> driver.find_elements_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//div[@data-testid="UFI2CommentsList/root_depth_0"]//a[@role="button"]'</span>): </span><br><span class="line">            <span class="comment"># 反覆偵測是否有「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕，若有擇點擊</span></span><br><span class="line">            <span class="keyword">if</span> bool(re.search(<span class="string">'comment|More|Repl'</span>,i.text)) == <span class="keyword">True</span> :</span><br><span class="line">                driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    driver.find_element_by_xpath(<span class="string">'//div[@style="display: block;"]//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'No pupup!'</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    i.click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'Nothing'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文章內容與互動摘要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PostContent</span><span class="params">(soup)</span>:</span></span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># po文人資訊區塊</span></span><br><span class="line">    PosterInfo = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'l_c3pyo2v0u i_c3pynyi2f clearfix'</span>&#125;)</span><br><span class="line">    <span class="comment"># 互動摘要區(讚、留言與分享)</span></span><br><span class="line">    feedback = soup.find(<span class="string">'form'</span>, &#123;<span class="string">'class'</span>:<span class="string">'commentable_item collapsed_comments'</span>&#125;)</span><br><span class="line">    <span class="comment"># 名稱</span></span><br><span class="line">    Name = PosterInfo.find(<span class="string">'img'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="comment"># ID</span></span><br><span class="line">    ID = PosterInfo.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pb8 o_c3pynyi2g _8o _8s lfloat _ohe'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'/?'</span>,<span class="number">2</span>)[<span class="number">0</span>].split(<span class="string">'/'</span>,<span class="number">-1</span>)[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 網址</span></span><br><span class="line">    Link = driver.current_url</span><br><span class="line">    <span class="comment"># 發文時間</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'abbr'</span>).attrs[<span class="string">'title'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_1atc fsm fwn fcg'</span>&#125;).text</span><br><span class="line">    <span class="comment"># 文章內容</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Content = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pbx userContent _3576'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Content = <span class="string">""</span></span><br><span class="line">    <span class="comment"># Like</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Like = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_LIKE'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Like = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># Angry</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ANGER = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_ANGER'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        ANGER = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># HAHA</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        HAHA = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_HAHA'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        HAHA = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># 留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        commentcount = feedback.find(<span class="string">'a'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2CommentsCount/root'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        commentcount = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># 分享</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        share = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_355t _4vn2'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        share = <span class="string">'0'</span> </span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        data = [&#123;<span class="string">'Name'</span>:Name,</span><br><span class="line">                 <span class="string">'ID'</span>:ID,</span><br><span class="line">                 <span class="string">'Link'</span>:Link,</span><br><span class="line">                 <span class="string">'Time'</span>:Time,</span><br><span class="line">                 <span class="string">'Content'</span>:Content,</span><br><span class="line">                 <span class="string">'Like'</span>:Like,</span><br><span class="line">                 <span class="string">'ANGER'</span>:ANGER,</span><br><span class="line">                 <span class="string">"HAHA"</span>:HAHA,</span><br><span class="line">                 <span class="string">'commentcount'</span>:commentcount,</span><br><span class="line">                 <span class="string">'share'</span>:share&#125;],</span><br><span class="line">        columns = [<span class="string">'Name'</span>, <span class="string">'ID'</span>, <span class="string">'Time'</span>, <span class="string">'Content'</span>, <span class="string">'Like'</span>, <span class="string">'ANGER'</span>, <span class="string">'HAHA'</span>, <span class="string">'commentcount'</span>, <span class="string">'share'</span>, <span class="string">'Link'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留言</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CrawlComment</span><span class="params">(soup)</span>:</span></span><br><span class="line">    Comments = pd.DataFrame()</span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 用戶留言區</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 回應貼文的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_0'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg0'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 回應留言的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_1'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg1'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)        </span><br><span class="line">    <span class="keyword">return</span> Comments</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">Links = FindLinks(url = <span class="string">'https://facebook.com/taiwanmobile/'</span>,</span><br><span class="line">                  n = <span class="number">20</span>)</span><br><span class="line">Links</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抓下來所有留言</span></span><br><span class="line">PostsInformation = pd.DataFrame()</span><br><span class="line">PostsComments = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Links:</span><br><span class="line">    print(<span class="string">'Dealing with: '</span> + i)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        expand(i)</span><br><span class="line">        soup = BeautifulSoup(driver.page_source)</span><br><span class="line">        PostsInformation = pd.concat([PostsInformation, PostContent(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">        PostsComments = pd.concat([PostsComments, CrawlComment(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'Load Failed: '</span> + i)</span><br><span class="line"></span><br><span class="line">PostsInformation</span><br><span class="line">PostsComments</span><br><span class="line"></span><br><span class="line">PostsInformation.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsInformation.xlsx'</span>)</span><br><span class="line">PostsComments.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsComments.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Facebook </tag>
            
            <tag> Crawler </tag>
            
            <tag> Selenium </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲_104人力銀行工作清單</title>
      <link href="/2019/04/18/Crawl-JobList104/"/>
      <url>/2019/04/18/Crawl-JobList104/</url>
      <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>找工作是一件很辛苦的事情，當我們在<a href="https://www.104.com.tw/jobs/main/" target="_blank" rel="noopener">104人力銀行</a>的網站上輸入關鍵字查詢職缺時，經常會直接跳出上百、千份的職缺。雖然有很多職缺對於求職者是好事，但這也造成了求職者的大困擾！</p><p>因此我希望能透過Python的爬蟲，一次把我想要查詢的結果(工作內容、地點、薪資、要求技能、工作地點…等等資訊)抓下來，在我自己的電腦上按照我的需求進行篩選，讓我能更有效率的挑選工作並投遞履歷。</p><p>另一方面，如果想了解各個產業會需要哪些工具、技能，也可以從這份資料中進一步的分析。那麼我們就開始吧！</p><a id="more"></a><p>我們直接先看輸入與最終的輸出結果吧!<br>在左邊的圖是104的查詢系統，我在這邊搜尋的關鍵字幾項條件分別是「資料科學」、「台北市」、「最近一個月有更新」等項目，經過搜尋之後，系統幫我查詢到25頁，共730筆職缺。並且也有跟我說各職缺的公司名稱、學歷要求、工作經歷等初步資訊。</p><p>但只有這些資訊是不夠的，當我們看到有興趣的職缺時，還需要進一步的點開超連結，檢視詳細的職務說明(右邊的圖)。在這個分頁中我們就能夠看到詳細的工作內容、條件要求、公司福利與聯繫方式等資訊囉！我最終的目的是希望透過程式自動幫我們這些資訊都整理成一份excel表格！<br><img src="/2019/04/18/Crawl-JobList104/104HomePage.JPG" alt="HomePage"><br>而我們最終的目標就是將所有職缺以及各職缺的內容都整理成這份excel表格，讓我能按照自己的方式篩選資料，提升找工作的效率！<br><img src="/2019/04/18/Crawl-JobList104/JobList.JPG" alt="JobList"></p><h1>載入使用套件</h1><p><img src="/2019/04/18/Crawl-JobList104/01.JPG" alt="01.JPG"></p><h1>設定查詢條件</h1><p>這些查詢條件可以在<a href="https://www.104.com.tw/jobs/search/" target="_blank" rel="noopener">104的搜尋網頁</a>上搜索，在這裡不多做說明<br><img src="/2019/04/18/Crawl-JobList104/02.JPG" alt="02.JPG"></p><h1>展開所有工作清單，後續將依序開始爬蟲</h1><p>這裡會透過Selenium打開一個瀏覽器並開始跑程式~<br><img src="/2019/04/18/Crawl-JobList104/03.JPG" alt="03.JPG"></p><h1>解析爬蟲資料並整理成DataFrame</h1><p>在正式開始爬蟲之前，我預先定義一個函數，專於用來處理「職務類別」這個複選題，稍後將用這個函數將其串接在一起<br><img src="/2019/04/18/Crawl-JobList104/04.JPG" alt="04.JPG"></p><p>開始逐筆爬資料囉!<br><img src="/2019/04/18/Crawl-JobList104/05.JPG" alt="05.JPG"></p><h1>結果</h1><p>這裡爬得很快，大約10分鐘就抓完這700筆資料囉!<br><img src="/2019/04/18/Crawl-JobList104/06.JPG" alt="06.JPG"></p><p><img src="/2019/04/18/Crawl-JobList104/07.JPG" alt="07.JPG"></p><p>因為內容並沒有太難，因此我就不做太多說明了，不過有問題的人也歡迎在底下的留言提出！</p><h1>完整程式代碼</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import re, time, requests</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span><br><span class="line">headers = &#123;&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&apos;&#125;</span><br><span class="line"></span><br><span class="line"># 查詢的關鍵字</span><br><span class="line">my_params = &#123;&apos;ro&apos;:&apos;1&apos;, # 限定全職的工作，如果不限定則輸入0</span><br><span class="line">             &apos;keyword&apos;:&apos;資料科學&apos;, # 想要查詢的關鍵字</span><br><span class="line">             &apos;area&apos;:&apos;6001001000&apos;, # 限定在台北的工作</span><br><span class="line">             &apos;isnew&apos;:&apos;30&apos;, # 只要最近一個月有更新的過的職缺</span><br><span class="line">             &apos;mode&apos;:&apos;l&apos;&#125; # 清單的瀏覽模式</span><br><span class="line"></span><br><span class="line">url = requests.get(&apos;https://www.104.com.tw/jobs/search/?&apos; , my_params, headers = headers).url</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(url)</span><br><span class="line"></span><br><span class="line"># 網頁的設計方式是滑動到下方時，會自動加載新資料，在這裡透過程式送出Java語法幫我們執行「滑到下方」的動作</span><br><span class="line">for i in range(20): </span><br><span class="line">    driver.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight);&apos;)</span><br><span class="line">    time.sleep(0.6)</span><br><span class="line">    </span><br><span class="line"># 自動加載只會加載15次，超過之後必須要點選「手動載入」的按鈕才會繼續載入新資料（可能是防止爬蟲）</span><br><span class="line">k = 1</span><br><span class="line">while k != 0:</span><br><span class="line">    try:</span><br><span class="line">        # 手動載入新資料之後會出現新的more page，舊的就無法再使用，所以要使用最後一個物件</span><br><span class="line">        driver.find_elements_by_class_name(&quot;js-more-page&quot;,)[-1].click() </span><br><span class="line">        # 如果真的找不到，也可以直接找中文!</span><br><span class="line">        # driver.find_element_by_xpath(&quot;//*[contains(text(),&apos;手動載入&apos;)]&quot;).click()</span><br><span class="line">        print(&apos;Click 手動載入，&apos; + &apos;載入第&apos; + str(15 + k) + &apos;頁&apos;)</span><br><span class="line">        k = k+1</span><br><span class="line">        time.sleep(1) # 時間設定太短的話，來不及載入新資料就會跳錯誤</span><br><span class="line">    except:</span><br><span class="line">        k = 0</span><br><span class="line">        print(&apos;No more Job&apos;)</span><br><span class="line"></span><br><span class="line"># 透過BeautifulSoup解析資料</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &apos;html.parser&apos;)</span><br><span class="line">List = soup.findAll(&apos;a&apos;,&#123;&apos;class&apos;:&apos;js-job-link&apos;&#125;)</span><br><span class="line">print(&apos;共有 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line"></span><br><span class="line">def bind(cate):</span><br><span class="line">    k = []</span><br><span class="line">    for i in cate:</span><br><span class="line">        if len(i.text) &gt; 0:</span><br><span class="line">            k.append(i.text)</span><br><span class="line">    return str(k)</span><br><span class="line"></span><br><span class="line">JobList = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">while i &lt; len(List):</span><br><span class="line">    # print(&apos;正在處理第&apos; + str(i) + &apos;筆，共 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line">    content = List[i]</span><br><span class="line">    # 這裡用Try的原因是，有時候爬太快會遭到系統阻擋導致失敗。因此透過這個方式，當我們遇到錯誤時，會重新再爬一次資料！</span><br><span class="line">    try:</span><br><span class="line">        resp = requests.get(&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;))</span><br><span class="line">        soup2 = BeautifulSoup(resp.text,&apos;html.parser&apos;)</span><br><span class="line">        df = pd.DataFrame(</span><br><span class="line">            data = [&#123;</span><br><span class="line">                &apos;公司名稱&apos;:soup2.find(&apos;a&apos;, &#123;&apos;class&apos;:&apos;cn&apos;&#125;).text,</span><br><span class="line">                &apos;工作職稱&apos;:content.attrs[&apos;title&apos;],</span><br><span class="line">                &apos;工作內容&apos;:soup2.find(&apos;p&apos;).text,</span><br><span class="line">                &apos;職務類別&apos;:bind(soup2.findAll(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;cate&apos;&#125;)[0].findAll(&apos;span&apos;)),</span><br><span class="line">                &apos;工作待遇&apos;:soup2.find(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;salary&apos;&#125;).text.split(&apos;\n\n&apos;,2)[0].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;工作性質&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[2].text,</span><br><span class="line">                &apos;上班地點&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[3].text.split(&apos;\n\n&apos;,2)[0].split(&apos;\n&apos;,2)[1].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;管理責任&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[4].text,</span><br><span class="line">                &apos;出差外派&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[5].text,</span><br><span class="line">                &apos;上班時段&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[6].text,</span><br><span class="line">                &apos;休假制度&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[7].text,</span><br><span class="line">                &apos;可上班日&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[8].text,</span><br><span class="line">                &apos;需求人數&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[9].text,</span><br><span class="line">                &apos;接受身份&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[10].text,</span><br><span class="line">                &apos;學歷要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[12].text,</span><br><span class="line">                &apos;工作經歷&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[11].text,</span><br><span class="line">                &apos;語文條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[14].text,</span><br><span class="line">                &apos;擅長工具&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[15].text,</span><br><span class="line">                &apos;工作技能&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[16].text,</span><br><span class="line">                &apos;其他條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[17].text,</span><br><span class="line">                &apos;公司福利&apos;:soup2.select(&apos;div.content &gt; p&apos;)[1].text,</span><br><span class="line">                &apos;科系要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[13].text,</span><br><span class="line">                &apos;聯絡方式&apos;:soup2.select(&apos;div.content&apos;)[3].text.replace(&apos;\n&apos;,&apos;&apos;),</span><br><span class="line">                &apos;連結路徑&apos;:&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;)&#125;],</span><br><span class="line">            columns = [&apos;公司名稱&apos;,&apos;工作職稱&apos;,&apos;工作內容&apos;,&apos;職務類別&apos;,&apos;工作待遇&apos;,&apos;工作性質&apos;,&apos;上班地點&apos;,&apos;管理責任&apos;,&apos;出差外派&apos;,</span><br><span class="line">                       &apos;上班時段&apos;,&apos;休假制度&apos;,&apos;可上班日&apos;,&apos;需求人數&apos;,&apos;接受身份&apos;,&apos;學歷要求&apos;,&apos;工作經歷&apos;,&apos;語文條件&apos;,&apos;擅長工具&apos;,</span><br><span class="line">                       &apos;工作技能&apos;,&apos;其他條件&apos;,&apos;公司福利&apos;,&apos;科系要求&apos;,&apos;聯絡方式&apos;,&apos;連結路徑&apos;])</span><br><span class="line">        JobList = JobList.append(df, ignore_index=True)</span><br><span class="line">        i += 1</span><br><span class="line">        print(&quot;Success and Crawl Next 目前正在爬第&quot; + str(i) + &quot;個職缺資訊&quot;)</span><br><span class="line">        time.sleep(0.5) # 執行完休息0.5秒，避免造成對方主機負擔</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;Fail and Try Again!&quot;)</span><br><span class="line"></span><br><span class="line">JobList</span><br><span class="line"></span><br><span class="line">JobList.to_excel(&apos;C:/Users/TLYu0419/Desktop/JobList2.xlsx&apos;, encoding=&apos;cp950&apos;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> requests </tag>
            
            <tag> Selenium </tag>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> 104人力銀行 </tag>
            
            <tag> BeautifulSoup </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲 - 爬取松果購物商品資訊</title>
      <link href="/2019/04/07/Crawl-SongGuo/"/>
      <url>/2019/04/07/Crawl-SongGuo/</url>
      <content type="html"><![CDATA[<p><em>本文章僅限於學習目的使用，請勿用於商業目的</em></p><p><a href="https://www.pcone.com.tw/" target="_blank" rel="noopener">松果購物</a>是台灣新創的電商，截至2019年1月為止，平台上有超過3,600間廠商進駐，商品數量也超過15萬件。僅用了2年的時間就開始<br>我很喜歡創辦人分享以下兩篇創業歷程的文章。有興趣的人從以下文章了解更多松果購物的故事。</p><ul><li><a href="http://tesa.today/article/1629" target="_blank" rel="noopener">松果的創業故事-第1年</a></li><li><a href="https://tesa.today/article/1909" target="_blank" rel="noopener">松果的創業故事-第2年</a></li></ul><p>在這篇文章中，我們要透過 python 把松果購物上的商品資料爬下來！</p><a id="more"></a><p><em>再強調一次，請勿用於商業目的。</em><br>這邊文章的架構如下</p><ul><li>找出「男生服飾」下所有的商品連結<br>選擇「男生服飾」的原因是這類型的商品數量較少，我們在抓資料比較不會造成系統負擔。若想抓其他類型商品的人可以自行調整類型的代號</li><li>從商品連結抓出店家/商品的資訊<br>包括店家名稱 / 店家評價 / 商品名稱 / 商品銷售量…等等。具體項目會在後面提及。</li><li>轉換成抓取變動目標的語法<br>將以上抓資料的過程撰寫成簡單的函數，讓我們可以輸入類型的代號，就抓出該類型下的所有商品資訊</li></ul><p>那我們就開始吧！</p><h1>找出「男生服飾」下所有的商品連結</h1><p>我們要練習的是SongGuo上男生服飾類型，這是網頁畫面。若想要練習其他類型的商品，可以自行修改網址中最後的3碼數字。<br><img src="https://lh3.googleusercontent.com/Jpw1KzETId2E21gF9etMcEIN9tfow7WUtHGGFIkAV5QA6tqdGGBlm1U9RFEreoJXbsRFzyBm1h3VUGGndTSS-8dRCSVudhFEu9ni3m9qZkEY3O5aKaeh2WTNDY0XnXoCJiy-XhW-tw1SL3b7Wbh_6IK70xUgSFo6El7z4ad97l6OgX6GvKe19Nh97fDl3LUenO0lbJFn7QR8p0cyQ8xE0sxHpVBZ477y1QgmKG07yW5z923DNOtBeILN7VDdS2HKNa34uFBvdBqJPCwFzG_DFPiQjEXDDL39ti1oEmx4ivGhGhZIHiNh4TxclpE-QyGKYUibcAeiEvFBwSdA1uTv4vIOtH7oxpx-qzDK5P_EULUetNHMR3MRedOj5k_2Wuumk0L_qySPxMh_Em9IvwpU3G-qYTTk8fMNXnphcE9079SgnGK--sVdBGSDT89Q89EHN8LpspujUhBhYbK3A7vT_2qJV3uuK4TAHYwtw1lWyIQLqm5puHWllvm1J11ql5YcP1HWGFjVXllLqzfy-5Ie8w9OvAmgDETdKx3o5htobHSEx9njlyxXRegvYKF88_Hk_5BR9e47vHTJkXZESv1HL98iP_PAViaT6f1PGfOVyvtps5hgCS4NwXeGa7wQgZDOKjMd-K6qoNCPotugjHcJJxa8Qh6qTenST0cWpqc7zIuguHQezhcyv2A=w1344-h978-no" alt="01"></p><p>首先我們需要載入使用到的相關套件<br><img src="https://lh3.googleusercontent.com/VGzFBH2gVJKJCNzpvclRaixSSz2FHoOLM6QLajqRlJiX_KCJJWHg1u-zlusYdACywtBD3DR3sjr0yGq95Lb4d2DN4-_DLDl8QXAI4FnfEgZCF-ZRUvj9dIN486IbQbhXTOq_KHgx8EuigcXHz6o0CQdwyLIwLEUAzouEgHRD5ydbO7SphPs7B9S_GUgMoTEZS0CBDGQdRlmScSajQ7Qgw7crnJvRSZ0WqE0z5z2oXIyzRzVWwy9_KIcRnhK0Kly3c9F7aFwlZoTl2_wDkKva_UDagq1qThXM7Ri-F3LwK1BkIv9YohsctC0NWxFAwVFdsgGQFpY3fuktHtiV6h6PbRx4nZy70d8IJMxBXibiESLh_AX7GzjLjSDnf5aworUMGH8Lv-hBRhS5DDurocRTmNNi8LPm03N4qQCQ9WvloRsazoheG0UmYzNSnxaELeZF6gPsgWH-luA2Tpdu8Gz49FwzvHrMMWwIr9gwIHo_3QGd8QQxmiFoHnKjek2nOIOMeOes2qWno5CI7cLav7zP4KIuinSiggBFTz6-5jtPCLZ0qzfcFfqi08b3LU7c9rJkXLNrmDWsTu3z_pWbhS_dC0fam5yLR1IgiwZO6ATPgb2qbATjr4qDb2oUkg6-lA78yEO6QsaglsysTt0G5qQu1Oa4YALF_LqUgQzvgdjYqxSHpuPX7qpZfOQ=w1075-h91-no" alt="02"></p><p>設定待爬網頁的網址，與使用者資訊<br><img src="https://lh3.googleusercontent.com/HxXaKaA-E6ZRdphbNFoSX21KCL-PTF30mYn6DotreKD7b7JNt7WXcZ45vqTbhPUZnKRV9cwqn2gTBI_5ziWjBLnGLmg7dtRUoQyvPfA5lIEduP81S056m-6uxewMPzkoLeTfz60y9upl-jVZfx0fBA8wyc_zFl6DO-r0-bqOP7W4xfa8GW89cXOoJadIAzovyqw1-8Y2B8UIiI5e7pBeiU6JMlIuyVQ5G6IjrO9fvP70OA_7CnhQDvrNnHb0YFxw96afJtrtDOES5-cqWvpJ29pUWxui5LmJU-gEGOv8lv4XzzEgdHTANTmBuTVFr6D-xszbMV3jA7wXEEJe0IqJmmouBTbkI3iQS8TC8g4FRUjpv7uuG0DiyNT0H4Gt8gWSG9r2ho0jdScFQJjWdVopwWM1WdJCaHUfAdBrgWEaG-gPR-uOcpvJbT6GsHw2c9ySpgOqGlfOoE5NGZJcZUPBU25XOhyZTQlwRU1AyrESzjUcfy5MtXXr_4IZClunRb8lrAPqjfdgm95A3snaCu5yz_BlqsK7xkRNRDKgAyVQSBMOiW5Wgm28F3YODbZ7cmyGBIux0oTTsaVueZozljIBdgLh3LotA3kQiDxAXFZVxB5sly_JOeHJP7Y4gm2HFW9Gutzwpb3fZZIYjB_xqdHuwJAfieq0PNMIEVDRTdQzzeeQ4edGhF5m3EI=w1083-h140-no" alt="03"></p><blockquote><p>需要注意的是，若沒有輸入使用者資訊，很容易被系統偵測為爬蟲系統，進而阻止後續的爬蟲作業。因此這裡需要加入這些資訊，藉以模擬真實的瀏覽網頁情境。</p></blockquote><p>接著我們可以透過這段語法找出網頁上所有商品的連結<br><img src="https://lh3.googleusercontent.com/mQ1Dk3nR0wHJq6L-IIxgAT0lySsQjoue0JxmL9TvmHCzxDnQTctFYsjbe42FM82GMqCbDHGp2p9HQUgAlpEXtAQjkKCff6KJGQN8FTLxqr2WrqLjFf0b71_liWfTlLscbzEj8o-ZKozfSGG8WHyZjuPD6vCW5XmoUL-altQQZvZovp7i4ADQmlYX3R9DU4Kt3qlQJ2A1qaS_1L6uwivbHFTIA7Aiqh6oQG6JWioSq-th3q0Vs6e_8pN1plZGEuh0PCQaoeBZQdtIad_4pwMaMFSsfYOqKaOrzEDC4rZWiYpYLsqFwj5UdLbvCK21I93A9Sw_eMHzqZaZEQrhAKUjMewvDhNBMmnFcQwzG9eBYJ7ccuUyNdO7b4ToVVOijtKwxWYPLBqivzunmxnVuoS0IYzN3qM4jpeUPVHlZrOec4uTInd1ukeCgYc47UXEQk4Rt8PIxBFI4sGhvEAhnUWPa6YuC3b7wjkxZ3gjS2lYM7jz-ZHuWe6Hc-NhzVK8q6efVzyLpAebhyUhoP7UBLPuQhGAFsrpTaYgoVmbsMRted67xE7XMNBkbUZmj9oGqfuL6TJEzNg4Ka9Z_-QKb4PzgGVJMqeVcrPt_zK1s9Qbj4zcLaU3AdL1jFqaEeoCqGMTuX_4eeIFz0MW35I-CYfkdF_htC61BGOQMwW6wRRAc1U_F2-G8EeEdtE=w1081-h437-no" alt="04"></p><blockquote><p>需要留意的是，透過輸入’a.product-list-item’的方式找出商品連結只限於用載SongGuo的網站。如果想要在其他網站爬資料，則需要檢視個別網站的網站結構。<br>具體的方法是</p><ol><li>透過Chrome網頁開啟目標網站，並在網頁中點選「滑鼠右鍵」的「檢查」功能。</li><li>Ctrl + Shift + I<br>畫面如下<br><img src="https://lh3.googleusercontent.com/R9xMEOy4pZNHxlffuAYaKgECW1cb4TvVCchsLMkcrpp-Hsjt2JUtV93jL6sltGCB1inPxphnHX1vzSwnu1dg_-C8saC0_-r7UDd4RroQKbzWlQHWnEYpRpeY6gTx8vlBtcF4fC0lZBZpUfNTTD8QFqUwl7KgFKq70Sdd-EbU78Uie38pMmJg7jMhPunoo__dGzTU56OaF7deNYsyKd8ambASsWwlxoNXIz3gGQZVfKAAPfYtS07SxPzHpMI2LSzMWBgC5Qcq6poanBe2VnhtACFAXB2ldDY8__JlFU93C2hM0gK9E2xx3-zdIkvu73ms9i-f8ne9h2ZMJLvC_ElFXO0igMQFTBUb8IZyMfKV_MtKEinZ3z8K67-PuzYdPJvS5znBuvNM5S8Bt80-o35bIiZvEYApaON24AjFNkff9Gx8OqTz_8ia6LaAi2t8bm_DVmmx_oN4PekDB-JzAB_ESLP97fZoOtRjNqU9lGlxJbQA1GZHY8lQiIJVrAg6O_zoU8CsWTCe86aLRjy3tEj9ux1tgRhWASB2ib1vrycngC5b4L5UPtJ-Znqr0OxWIlIZvIZboFqDK00MteOD5rlqXYl64scZ3BvkDzoB_Cl4Dz54ZfsBPxuKhkLbxeb-ELQg6fPdclZFtoJIkFaz5v7xOeRAP1IMVrmqR2sBGEjMrbRR_YmRFXhxZ98=w1833-h978-no" alt="05"></li></ol></blockquote><p>從以上的畫面我們就可以看到，商品的連結就藏在各個Element的href屬性中囉！</p><h1>從商品連結抓出店家/商品的資訊</h1><p>我從中挑選第一個商品連結(/product/info/190117048588)作為範例，商品項目是<a href="https://www.pcone.com.tw/product/info/190117048588" target="_blank" rel="noopener">【瑞典】旅行折疊電熱水壺</a>，網頁畫面如下<br><img src="https://lh3.googleusercontent.com/dTviyk_OLCkbzdQFUKmDtx2Y6W3MOLL6XMgiEDMIWUkeNtYU3PwbxV4m6W-UDYUceamyDL8uB8OP5ae88xAimLNpFvpFpFaO-bR_sAId1t5supGvNHij3u_VSoU4HMaHdsJtiuXfjPMwi30Wisx-VWnbGT3PiTDgr024NV34Veuy8CkX4PTdfJQUb9lI2nAUkGN9PXtfG4gNQ7DlhtranOcRN2UHlOBP8LEzKOlxNZb-WtAMvQAWaiUnhsJEVrrb8aWGObXMjY_CySXb6_szvmTSIldfny0hB3w3qdJS8M9D8Sl2Upc3bSofH5QIvvv4LLu3qw1SPjE4yRn0VeiCINRMemtqRNCnC5Iwt-9yHtmDUjbZPTNnE84C7D2bWstycMgkKg81ptD2rC8poN2eI7CnH-iJN_5vxBvkM4mA65DaYoA5G8tgboI_GvjZy5tmgX0nXRRx3knGc6qVNrKD7sDc_HosDw4BRnGLZlyvNX7Rk2wjyQEkSQz0fFxqsF5dNtE9vcpnmj_AfqIjARrp6rBZ6C0vf8hj-hnt1W5whKctUmv89K1HHJFaDfV07dUjxWRVyEs16xgxq7CBfKovhqr910QYTaAJw81aU2M1oseSuShY4K5hcknYGfH8R8cbd0bSInTxVZPfGTBWXLNEAKh33vta_jJSwLVLLVwofdr1BqouUg3rnUw=w1344-h978-no" alt="06"><br>這個網頁中有相當豐富的資訊，包括店家名稱、店家評價、商品名稱、商品價格…等等<br>我將從網頁中提取出以下項目的資訊</p><ul><li>店家名稱</li><li>店家商品數量</li><li>店家評價</li><li>店家出貨天數</li><li>店家回覆率</li><li>產品名稱</li><li>特價</li><li>原價</li><li>折數</li><li>商品評分</li><li>評價人數</li><li>收藏人數</li><li>提問人數</li><li>商品分類</li><li>商品標籤</li><li>連結</li></ul><p>找出資訊藏在哪個Element與屬性的方法同樣是透過Chrome中的「檢查」功能，以商品名稱作為範例的畫面如下：<br><img src="https://lh3.googleusercontent.com/2cJ9Wg8E48xy-KFa1OFrqOuSvjhT8xGvIl4TbBBoCYUYWbKmkXnkOVbozUG54-UDJiNDF-rtKgM4qzGRGo6WReE9vUVQEXe6aR_DpvHGD9Wr5Ft0SHpdhuogbN8HkKo0Wh1hMsC3EEoD39Xcg9SZWE_UmMPJDLYKNr182WByZNQ8a7WTHI4RzPh3Y1ceOJ-JOuajuylFQHKHOKldCIhgAtrKowOulzyewry5rTkV_tNu8E_PwgtwrxVsu96diY6i6BY6CIGbpvLOEhPJ2aqEijliWlaLDls_vsa4NqJNqBGAbxgZoWr5D5Q7Fxb0NXqreok6XF2cUdE0EMb6A4NhDeznayjpGRGwZ77UkFFgo1zCswW9IDGlyjJMcrpxS7qktsDJ9o2xG_kfDwFwkSMIj__thTzG4NO9qjmFUopysBiDYoY_IMFZM5KJglkcaS4pH8v4b7cdkZIzHkjn-e12a0bwXgCajWvWG_VqGjOA4zq2ER66fq3K84YuffFdhKC5Z8aLUcx9V9iho2SF-5PqhaSk2pdEt-6kYRfJFPdbufW13eEXp5viYS3HIfN_N3pYFajTqoOoy5BnNuNDdlRFc4GfKsjfDMXzBUvVy1Brs640NXfntJEEL_Ir1_v0N2EUq4X7IWzF5kAFunowG09wc9wHug-tfdKnsQoUtYbo9VsmjQ4VfVayP6E=w1844-h978-no" alt="07"><br>這邊是找出以上項目的程式代碼<br><img src="https://lh3.googleusercontent.com/rSOr9p5_Hvk7ucpdQWe8aNUI3f7C4MIUsLyB7kISrkDxj7Cz19obNxjNAlpOoXO8CtvSFXYbcBRQBMh0EC6QE-9GI64c2tNaBuqIJuuGYYE_J-d67cNc7FcYcJzzv1_uAy9xCtu9WsrVV-80rMgQlNQ8Pu6_na4_h1n4TFSPY4p_3qqCA5QEtZKKqg0VUq7oMO54hopLYu0WIgoSN6ASjIIjYY4Dx3mhOUgfqCzHCxagXrGX409pq8LqKHmzVu5s1jW451WzyNTZKj66GZwdD8omQnzPyfTUf7NPSPhpbUE-hBbPh9xw8quUD0cXw7ZIrH0ZKDFCJDju8TYd5ZCfx2VCGKkxdvrNmhkcKOvbNpXgdx9QVGhng2MDoGyIf63cEkZc0weC_TD6_06LRDUgLZ6INhZ74lh1T14kpoDOTsq-XNpms43xkvfHbMKk_QQAWQGzkH30ZUQbphOPLDh-fbsveyRKt_XlXVP2fG8QMsCi5x3jOPAwlQ0_Y1DCQcshDK1buGcGXSIm1co4k3hycgvEB0d11yPJaXVen8qekp2blRyZsbRBNmfkpSK1JLAf2xcC4YRw9qXrCQNhRfebQH6ZTW8CG0F3FO6XN3jH4EFdVXG74R7zoEgLpBsCWm62RfAt7q1smUMYY9hjYtKT3keDnsVYrCiBDi1mmhwfk2RoGGI08sdCWPM=w1093-h677-no" alt="08"><br><img src="https://lh3.googleusercontent.com/jiB3fz5HTgrGzMWY-g26LO0LWz_2DDNTIG3M7F_bg_pJIQtjBF01nRokBAvlOWeuUmKD32hSRkTL2xiraQXX9TwvgZwA-gl_zBLIZKOe_jiSRGdOWXVmmVlyGE4eF2752NsErFtwgpTI_gGf6_7xyeHRflyhKWXkpPvk0h9Em-OGd0USm2AlXe1TO1BGRcRe1hlOXU3r6nNuY_8wLZAqLaZAxseexwG6ugEaUP_ioGh8c5X9thqEJfHCfPUvGMyHt40joss9SxHgiFOOflveQ3tjBz_MwOIkCjNNnxNSO5r9z-TUUR95Jk0v88K03BKMCKz2qY-4RX9iOWECxmME6VnnHvFBu2RMyXaNgqFjM0degFHstky8iMKnSay2XfxwgPHyH8uVbRbjSxGG9UIuLfX7RXi7kcG_AiIiCS_bnEakZLyayY8nrIN332IvvcWm04E_723xy4_SKQQkBleXn2zNlP2GvHBT3kFs4R10SxDEs_x8wUXFdWYiOUvGj5RBF5y2e_KeDQpwWA2qVOUNh08hUiNcDeTj5IwRmfF5KPQycWJKxkEjIq0y5CRFdqnhwpraRoELhrFUg_nBvI2f6NZ6Ej3lAM6rCYWe4zDHduIeBe-5IaFH-47W9O6G1B3LGEViwez9bJTD2zgN4y12am5UjeF89UvTE_B_iIIBbPL92-S49sRgABY=w1089-h790-no" alt="09"></p><blockquote><p>其實客戶的留言也是相當重要的資訊，但礙於篇幅這裡就不多做說明，找出節點與屬性的方法是相同的，有興趣的人可以自行練習。</p></blockquote><p>透過以上的方式，我們確認用程式把商品的資料抓出來是沒問題的！接下來我們只需要把抓取「固定」目標程式語法轉換成「變動」的代號，我們就能自動抓出所有的商品資訊囉！</p><h1>轉換成抓取變動目標的語法</h1><p>簡單整理一下，我們在第一段的輸入是某個商品分類的編號，回傳的是該分類下的所有商品連結。<br>而第二段則是輸入商品的連結，自動幫我們抓出商品的各項資訊。<br>因此我們要透過迴圈，逐一地抓出(第二段語法)所有商品(第一段的產出)資訊</p><p>定義ProdList函數，輸入商品分類的編號，回傳該分類下的商品連結清單<br><img src="https://lh3.googleusercontent.com/7SlOEdsz7ndW7COAbcLJ5DNCzhKtrlHsr4zBBa3oAU2DHluoResMGY2d-27VMsGCB9vji48lLpfKuNJ7HIAs5e0I1bvCGNSp0cNnS6rL6BEU_mx_2Wib0yERNYZRex1e2780N9qmnO4uxfovc8ivcYcPK0-praLR1S4PJO1ji9VUmB2EuGg_Q4zXzWUMFXt7xc1gr_rGznmNrX9H-rj9JoNGN0I25_yfuMk2nA0fnCY1HYvY_9SdWVLlkn6ENeqoMTy-MxKliGFPnCDAqyulEa85NgArvuhIapmNd5-oYWBPGrxcq41asaOynILecU3p4nq-5r1JJMVxofmM6LquTDvzW-sCh99sTPiN6AVA1-l_mpee9XE7FM5b_DjAObZko35IhuDrcKbMj2JGOP7VIWN_8ZOp2JHctFXafADKIEO-UJ6NFKXrZC_-k7_eyJurkCkyhJ04jkg5c5iUVq6Ly6OkqDCnQKY1Zs9_pwIPLueVt7CYqgSv4MFK5n5805qSw745EdoKDM7M1wv5Se6cIKYBlA3PWSNotIWzEv4Lxsb1UarLTWdpZz_T-qZ82XK2Jun4U9wJ4QnPt1L-gRd98LMpNqXtnWPDM0jwRLMYAxafBm2tqj8EZVX0Hida0OOjkTQKG8eBlBdxrOmvpg46soh_zDu1i_eg1QIp77C94RRGgyaMyS4Ufv8=w1092-h93-no" alt="10"></p><p>定義Crawl_SongGuo函數，輸入商品的連結，回傳商品的資訊<br><img src="https://lh3.googleusercontent.com/6yrE3PjDwcWQk9hczw3vnh-A66Q1zKLgCw-l2PfmiyHVeqaM5GBjg5kn-JqKTLIfkhsO6CouisMoMlBpyWXos0dtSvakvKup85UMe3kdef5f0ZLLBJLEsUsQousaCqugA6vrRnqwyXaymRu_3mljpsiHZR0MBsOibfoz45c1JpAKuM7IsGCGlmZ6rlZhyB6yCRrkMuwwZ51r0qzEEsJHmtpjq77Fx0G31LllomGIrhdWAe6RvgBwv9bi5x573NwDb9g9Zxh8L2yPBx7hHwZEnsLqIo0CtQRjBvI0nhfToYQeHoXUkXopNCqb01Ej29CkqpkYPZq8yRQGzpvAjgmTtwwwIpBeDVi__tbCTfUIGKgZqgNPIKE82QxEKLj1SMd0TVDKYeW2z6ln-MiI-Odlc5l9_ZEqF6bDMeZd2Nw9iTVM6rS-kjRkBkWJ7IyEi8lxbJbIW7hrC9HmYTGhzr60nY5jkmEvm_GIbDyzY_E65R3HWTsS70pMwWAoD0SSEn47FYqNggH3Mj3fCcTVossvRMHaFtCP675p0P7ZL_7yc-eD3eBLohRNzLw3huQVYD-QoiUROPq4_VwjXqHffpl4ltcTmX8PoKpDG6mEMczia0Tko-wpdHin6QG6ggRA5OlQPSijjGLX_IHfdTNAVkIWO6jj2bIlae4ZN8PSUfPGPOvoefN32IzzBhk=w1091-h446-no" alt="11"></p><p>結果以上兩個函數，輸入商品分類的編號，抓取商品的各項屬性，並放在df中<br><img src="https://lh3.googleusercontent.com/CVUlflxHvvuS6kvacjRepoZ508kTfX8EBRqP7-MubmxAeHdxJV3c9a3TTF-gvr4CDxvdoSnOBDHayLUU_3u2yp_9eGTrq-Q2-iOeUQU17sKj9kd2w-9-9EIYpPdTWKYwBro0ODdGZ0M1auPSLrd93oVCH2RoJGBAhi2P2LYPrBzTaZRAlIsWMNk6Jm2qR4719Sci9M0SpwHPd6Igfz1lUofLhiTBrohyCY9I4-06o3QmAK83pwvb76itxZEUzlhUo3uHB5fi74nipssCfXDPpbpLG7jPjhiR96HkEfdMOVJLznrqw0Fg5dMNKjF-DEz3O6RZwkLidl2mOnu0deZwrorhGujysoxTxVY4Bo4nTfRvVC565R7_M9EhNu7sr5MYH1Ryag-5dn_j_GKFOIbDSVR0RRQYfLd0ZbN1sK4hiwh6JcjOhXqI4xctP-QEBt6FXWbXeykikoCADneIoI6GF3QleEZtcNYduoep5xHeJoqvUKJC1i47-gFn4Qd0PNOmnNL5EojQVU6EQBW9ehrYi3YDN43NWH_rDUFaIlN2O7JFFRHIdEX-_OnXrFQLvpI_13J_AS7QfopssE_L-gbEqj0l591az5lkAIh1SD_KYmDM5IkK3Vrx5SVjV35x9QPNVeOlisDPstISVuspRv8eVw8cEi0_I6yf4O7c0oBZ1nW61QEJeXur3Z0=w1096-h334-no" alt="12"></p><blockquote><p>這裡會輸出的商品編號的原因是，我在Crawl_SongGuo函數中刻意加入print()指令，目的是方便我們追蹤程式是否正常執行。</p></blockquote><p>系統執行的效率非常快，不到1分鐘就執行完畢了，我們來呼叫df看一下結果吧！<br><img src="https://lh3.googleusercontent.com/iyFNqo1VEO8Z5fFjGMQR-Tuy4NtiMdwYcJslILRYsKvlyIc5Vpw9uUvd3vDJatRdubrsytwLxpncZdQzDoZNFDwp5yHCN780CRY7_xF5MbOQX5CYGlBZ1MXRVjhRqnfceScKBZ5s-Umnmxanvu59h5W9N_iVsT4H5o6TK-oIfpDwHFcEIrHEtwFak6Yghftxok_41WKuIjt5dIPJm9lx5yp0tGT8dlMXWnNq1H6semxrBAhjkEosxtQS_8ooFrSK5s3RZC4f6f8aom2lsgmQ7YQSd_0Zuc8wdV1aPTtF8E6bSs2AsXZ7cl6sEkr1uPDdWWSj6i0mS4TQOCvei4tGYoEBHY1KN5MKmpNtGylM2aHxxL9Ze7-czaa28LrhDs30kmr3rFeKCaxrWtgc5aYcQZPsPdxkhfRHHrCkLneZDORN3evBfcDDSI7h7nHPDJZDhADgLhR4bTQREBrDNbPrqGkxjE7xP-0ezfvg0SdSR9Gl8J_BI11YbXzmJVg5RUXtx2lXoDYSFWCUAsGASVKu-8MNKgN3xnZt6tGD5aGlRk1YjnpOntnkLDhE-Qp_sn9EnrfF310VGBpGEHtqbluDF01TCuPcJ0gB_6H0Rzocp4iXVj_cBXtWIOfFTJYmjUj5JseT7wTuiXwvYuoVZ4pCOHTeZZSHSroGYaNKIBmnMXfxZoVgqYRbBrc=w1608-h566-no" alt="13"><br>因為畫面的限制，只顯示出第一筆資料，不過我們確定資料已經以DataFrame的格式抓下來囉！<br>接著我們嘗試把資料存成excel來檢視吧！</p><h1>保存資料</h1><p><img src="https://lh3.googleusercontent.com/MVXZ9tNyUPYJ7byE37nW8qFvupJXZTVPsTWaGF3OzQsQf6-HMZIWOIKAwUjHjIK8o12yBYOTFxqurA4uim4nkv91ajlmw-2rtj5qeT3nmbmYR1poE0JEr0eP_F9cAx_hHjK6-uP5SaatWkB5CUEEEA9PqjV4VIjBfI2P2YWERtTld--PbjXoynL33JSj6KWigfT0UqbShFD6YUKhA0CmufiUZ_Tg0X6bP7Z0PtncM_tWSU64dOFtB-QyX0mvTSbHgNtdjyTPvW-MNydLoGdezO9U96IdHTAlIpJ27Qcilx3h_CCyrU8YOnQa-NCCVfM8UBYYm7pj-EVqb5hC2NIuUraBbWDlAeXbEcULjVFw5bqD1TQZXTKJXs6z6JmejnZRbH-86BsSWxAG-YQokkrwfD_0aOU4oSNgO5Fg9kdvELlt6caM1OjwgXN4SendRsPD93X2Zne3Y6Ll7Ru-5EQK8lIvcvRXuMNU6aWPvJt74GpNQlPqAsfmihHP2MsgCd8YlwHgATUs_dasYD82IiR1xqJEgyRMwLJjq_Tbm7c1fvL_dG2jx6xw7jy_7wusaya_TMasJzlwN-5e98W08ug0j969ZCQEfjWc1tGYNIHoGu3azsRrCm7A9MWrizpqHfmoVGEpmLPr4uPj4DL6evPRyHwLjaE9sfuEbwV707nI-7TQNXqlSvQS5LI=w1611-h56-no" alt="14"><br><img src="https://lh3.googleusercontent.com/M9MkCw4o_uqvo__BsBU_flCesCmQL9saEP8VZfmxq4RqrHdD4sUC9lGYixkK77hTjHKtnBbprgBvhva15BSgnMIypSRfHGPt5tA026bpj7Dtm7hqjWhoNlNzL49Zoz8UmWpqofMjWoALa--Juk_uWUYH0wJMSdtiirGKxUctOPjFRr3rN0KsACbJ2QCETX9oi3XQHVCrrcUoApXqD6RvJPblpBuxlGLgTPHTEQ0iikdoD8rZ5d0Y82rgb0oNsnFYKsZGNTm1HcDmSSffNh5PnC6K6hEOpj8J3nnQ6bReem2kYNxtqLPzQIhy7m6kTSHS2_-Yb9i7AZd0p64oRt26ou_0y7vJ-OzlaUB1murjBNFSg_uOnD1PYpVHkIyk6e3xqQp_lIxvg3aZhLmZYKPe3AEQYm_mCRrGLUq1pVnIScU8Ny_z6Y7kLaKGZRZHy6jm4BHkqoVA4om5w1nqWchhzb0KBviHMO-Xjd3K9C9xI0miUKOG9d5L3HlIcodb3Z1CkXpdFMoBfbp3yl6YMEHdhZ7ZLO-4M22yJNwLAswyAmOvxdpeT062wcr3PZddVzPP_bRODt1fdIkAvignhSLbknXhKhGyRQQimNyoNFhYyhfi61JJxjTWrqjWtKwD8Wxua7oXipVGkNo_uokPPjwGr-UdiY_lqPqkw840hHzjVBsHBUE7gYCk3u8=w1882-h978-no" alt="15"></p><p>以上次這個爬蟲的說明，有問題可以在以下留言區提問~</p><h1>完整程式待碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入相關套件</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸入爬蟲網址與使用者資訊</span></span><br><span class="line">url = <span class="string">'https://www.pcone.com.tw/product/'</span></span><br><span class="line"><span class="comment"># 男生服飾</span></span><br><span class="line">info = <span class="string">'327'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視是否成功抓到資料</span></span><br><span class="line">resp = requests.get(url + info, headers=headers)  </span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line">a = html.find(<span class="string">'a.product-list-item'</span>)</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挑選第一筆資料作為範例</span></span><br><span class="line">resp = requests.get(<span class="string">'https://www.pcone.com.tw/'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>], headers=headers)</span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 店家名稱</span></span><br><span class="line">html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 店家商品數量</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家評價</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家出貨天數</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家回覆率</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 產品名稱</span></span><br><span class="line">html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 特價</span></span><br><span class="line">html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 原價</span></span><br><span class="line">html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 折數</span></span><br><span class="line">html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品評分</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 評價人數</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 收藏人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 提問人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 商品分類</span></span><br><span class="line">html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品標籤</span></span><br><span class="line">html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品連結</span></span><br><span class="line"><span class="string">'https://www.pcone.com.tw'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義ProdList函數，輸入商品分類編號，輸出該分類下所有商品連結</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ProdList</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(url + str(info), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    <span class="keyword">return</span>(html.find(<span class="string">'a.product-list-item'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義Crawl_SongGuo函數，輸入商品網址，輸出該商品的各項屬性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl_SongGuo</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group(), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    print(re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group())</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">            data=[&#123;</span><br><span class="line">                <span class="string">'店家名稱'</span>:html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'店家商品數量'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家評價'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家出貨天數'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家回覆率'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'產品名稱'</span>:html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'特價'</span>:html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'原價'</span>:html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'折數'</span>:html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品評分'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'評價人數'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'收藏人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'提問人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'商品分類'</span>:html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品標籤'</span>:html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'連結'</span>:<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group()&#125;],</span><br><span class="line">            columns = [<span class="string">'店家名稱'</span>, <span class="string">'店家商品數量'</span>, <span class="string">'店家評價'</span>, <span class="string">'店家出貨天數'</span>, <span class="string">'店家回覆率'</span>,  <span class="string">'產品名稱'</span>, <span class="string">'特價'</span>, <span class="string">'原價'</span>, <span class="string">'折數'</span>,</span><br><span class="line">                       <span class="string">'商品評分'</span>, <span class="string">'評價人數'</span>, <span class="string">'收藏人數'</span>,<span class="string">'提問人數'</span>, <span class="string">'商品分類'</span>, <span class="string">'商品標籤'</span>, <span class="string">'連結'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 組合以上兩個函數，輸入商品分類的編號，即自動爬出所有商品的屬性，並將資料存在df中</span></span><br><span class="line">prodlist = ProdList(<span class="number">327</span>)</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(prodlist)):</span><br><span class="line">    df = df.append(Crawl_SongGuo(prodlist[i]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視抓下來的資料</span></span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將df轉成excel並存在桌面上</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/SongGuo.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> SongGuo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲 - 爬取Dcard文章</title>
      <link href="/2019/04/06/Crawl-Dcard/"/>
      <url>/2019/04/06/Crawl-Dcard/</url>
      <content type="html"><![CDATA[<p><a href="(https://www.dcard.tw/f)">Dcard</a>是非常適合練習爬蟲的網站，<br>除了Dcard台灣熱門的社群網站之外，Dcard也提供了非常便利的API讓我們能從網站上爬下文章。<br>在這篇文章中，我將展示如何透過python爬下Dcard上的文章！</p><a id="more"></a><p>這邊文章的架構如下</p><ul><li>抓取一篇Dcard的文章<br>具體項目如下：編號 / 標題 / 引言 / 內容 / 發布時間 / 更新時間…等等</li><li>一次爬100篇Dcard文章<br>透過系統提供的api，一次抓取100篇熱門文章</li><li>爬超過100篇Dcard文章<br>因為API限制一次最多100篇，在這裡我們透過簡單的迴圈一次爬1000篇文章。</li></ul><p>在這裡我們先練習爬文章內容的方法，若想進一步爬文章底下留言的人，可以參考補充資料中的範例，以下我們就開始練習吧！</p><p>補充資料：<a href="https://medium.com/pyladies-taiwan/%E7%88%AC%E8%9F%B2-%E5%BE%9Edcard%E7%B6%B2%E7%AB%99%E7%9C%8B%E7%88%AC%E8%9F%B2%E5%85%A5%E9%96%80-iii-ded52759d922" target="_blank" rel="noopener">爬蟲-從dcard網站看爬蟲入門-iii</a></p><h1>抓取一篇Dcard的文章</h1><p>我們先隨機挑選一篇Dcard上的文章作為練習，我挑選到的是這篇文章<a href="https://www.dcard.tw/f/funny/p/231030181" target="_blank" rel="noopener">警察閃光get</a>。</p><p>文章在Chrome上的畫面如下<br><img src="/2019/04/06/Crawl-Dcard/01.JPG" alt="01"><br>從網址列中可以看到這篇文章的編號是231030181，因此我們稍後會透過這個編號來爬這篇文章</p><p>首先我們先載入需要使用到的套件<br><img src="/2019/04/06/Crawl-Dcard/02.JPG" alt="02"><br>將這篇文章的編號透過quest套件讀取，並檢視抓下來資料的結構</p><p><img src="/2019/04/06/Crawl-Dcard/03.JPG" alt="03"><br>透過比對網站顯示的內容與上面輸出的資料結構後，我們可以從中發現id即為文章的編號, title是標題, conten則是內容，其他欄位的說明如下表：</p><table><thead><tr><th style="text-align:left">欄位</th><th style="text-align:center">說明</th><th style="text-align:left">備註</th></tr></thead><tbody><tr><td style="text-align:left">ID</td><td style="text-align:center">編號</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">title</td><td style="text-align:center">標題</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">content</td><td style="text-align:center">內容</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">excerpt</td><td style="text-align:center">摘要</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">createdAt</td><td style="text-align:center">發布時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">updatedAt</td><td style="text-align:center">更新時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">commentCount</td><td style="text-align:center">留言數</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">forumName</td><td style="text-align:center">分類</td><td style="text-align:left">中文</td></tr><tr><td style="text-align:left">forumAlias</td><td style="text-align:center">分類</td><td style="text-align:left">英文</td></tr><tr><td style="text-align:left">gender</td><td style="text-align:center">性別</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">likeCount</td><td style="text-align:center">心情數量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">reactions</td><td style="text-align:center">心情細項</td><td style="text-align:left">把以上心情細分為「愛心」、「哈哈」、「跪」、「森77」、「驚訝」等類型</td></tr><tr><td style="text-align:left">topics</td><td style="text-align:center">標籤</td><td style="text-align:left"></td></tr></tbody></table><blockquote><p>在上表中的心情數量是各種心情數量的加總，若想進一步分析各種心情，可以再從reactions欄位提取。</p></blockquote><p>我們來嘗試把資料轉換為DataFrame吧！</p><p><img src="/2019/04/06/Crawl-Dcard/04.JPG" alt="04"></p><p>確認可以透過程式把文章爬下來之後，我們就來寫個簡單的Crawl函數，期望只需要輸入文章的ID後，就回傳爬下來的文章內容！</p><p><img src="/2019/04/06/Crawl-Dcard/05.JPG" alt="05"></p><p>接著我們就透過Crawl來爬文章吧！<br><img src="/2019/04/06/Crawl-Dcard/06.JPG" alt="06"><br>Good!</p><p>確認函數能正常執行!</p><h1>一次爬100篇Dcard文章</h1><p>在這邊我使用dcard提供便利的API，讓我們可以直接快速爬取資料<br><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">dcard API</a><br>以下簡單說明這個網址</p><ul><li>popular參數：若設定為true，表示按照熱門程度排序，若設定為false，則按照發布時間排序</li><li>limit參數：限定在0-100的數值，表示要抓多少文章</li></ul><p><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">https://www.dcard.tw/_api/posts?popular=true&amp;limit=100</a></p><p><img src="/2019/04/06/Crawl-Dcard/07.JPG" alt="07"></p><h1>爬超過100篇Dcard文章</h1><p>由於API限制最多載入100篇文章，如果我們想要爬更多資料，可以透過before參數與迴圈進行!<br><br>before參數後面是接文章的ID，讓我們可以抓取某篇文章之前的資料<br><br>而透過迴圈，我們只需要把之前抓到最後一篇文章的ID放入before參數中，我們就可以抓到這篇文章的前100篇文章。<br><img src="/2019/04/06/Crawl-Dcard/08.JPG" alt="08"></p><h1>保存資料</h1><p>將資料轉換為excel保存到桌面<br><img src="/2019/04/06/Crawl-Dcard/09.JPG" alt="09"><br>用excel檢視抓下來的資料<br><img src="/2019/04/06/Crawl-Dcard/10.JPG" alt="10"></p><h1>完整的程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入使用的套件</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視資料結構</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line"><span class="comment"># 透過request套件抓下這個網址的資料</span></span><br><span class="line">requ = requests.get(url)</span><br><span class="line"><span class="comment"># 初步檢視抓到的資料結構</span></span><br><span class="line">requ.json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將抓下來的資料轉為DataFrame</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line">requ = requests.get(url)</span><br><span class="line">rejs = requ.json()</span><br><span class="line">pd.DataFrame(</span><br><span class="line">    data=</span><br><span class="line">    [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">      <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">      <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">      <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">      <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">      <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">      <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">      <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">      <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">      <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">      <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">      <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">      <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">    columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撰寫簡單的函數，透過輸入文章ID，就輸出文章的資料</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl</span><span class="params">(ID)</span>:</span></span><br><span class="line">    link = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + str(ID)</span><br><span class="line">    requ = requests.get(link)</span><br><span class="line">    rejs = requ.json()</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">        data=</span><br><span class="line">        [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">          <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">          <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">          <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">          <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">          <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">          <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">          <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">          <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">          <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">          <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">          <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">          <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">        columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嘗試使用撰寫出的函數，抓取編號231030181的文章</span></span><br><span class="line">Crawl(<span class="number">231030181</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次讀取100篇最熱門的文章</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100'</span></span><br><span class="line">resq = requests.get(url)</span><br><span class="line">rejs = resq.json()</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">    df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]),ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過迴圈讀取10*100篇文章，若需讀取更多資料，可以將range(10)中的數值提升</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    last = str(int(df.tail(<span class="number">1</span>).ID)) <span class="comment"># 找出爬出資料的最後一筆ID</span></span><br><span class="line">    url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100&amp;before='</span> + last</span><br><span class="line">    resq = requests.get(url)</span><br><span class="line">    rejs = resq.json()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">        df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將資料存到桌面</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/Dcard.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> Dcard </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
