<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>比好還要更好 — PyConTW2020 演講心得</title>
      <link href="/2020/09/12/PyConTW20/"/>
      <url>/2020/09/12/PyConTW20/</url>
      <content type="html"><![CDATA[<p>PyConTW2020 在上週（9/5-9/6）結束了，來記錄和分享一下從自行提案到上台演講的心路歷程～</p><a id="more"></a><p><img src="https://lh3.googleusercontent.com/wCzsscUWgCNqiHshMUsj6H1pb42Ud67EEkgV95wlCPizp6Dgj_Iwqu-f5T7ZcUKthSi7u6C58QvmC5U4trXhbDVbenT2o5E3BZC8LpRfjx3kCzVbuW8esilFKiAuKfMxLt3_5cD4bxs3GDtw9U_3zZfYZiTQDsyQlpkwpcpJ84GKgIspFvIebK6xhcvcCwtllTCWg3kdbdYZXYHzaOXSo8QU054a6oDHT3k_esknSHDLUY-vyoDvhOSTltSEcndVqDED7Yqd888Fb39lVaFaktLWdeA4w_KArbiyIhh_tApQZSEf6dnXv8vIukl3byMnYi632bvij0SfOwCDzlbKVBU7KOwJv5IZAO2stEBdYpdHyt_44klTdbCVGSiQiT7jR7MYfWN7un5inNK3H7BTk93o8G8GWf9fZ_bcDWhkx2WF0sNN774tw8V6JPmBdqwzKS6tmNoAxR-LZz0EPDJUpSILmIyROSI3JY8yE6Msryq20rROgRP3ZcneVR_gq-aMA4qbMBnQXpExH_PHV70o97iVbQGiWsZtSP6qRmbzf5kmCTPpJHD68O0rDzbNtSUTVp0PotG29FYBbf9M8h-48H1jt29hXDcMthTAnPaSXCNMWcVbJhb39kNSrlWoAYzUyWslJtCHio8oETwb0rJWEWMObvSwF_JmwAieovkRm0r6n_azte1yZJDgQKrgWA=w1679-h944-no?authuser=1" alt=""></p><p>準備的過程大致可以分成以下 4 個階段：</p><ol><li><p>提案<br>這個階段需撰寫約 800 字的研究提案，主要是說明為什麼想做這個議題（含目的、重要性、大綱、方法…等等資料）。<br>在網站上會有指定的欄位讓我們填寫，如摘要、目標、詳細說明。另外還有一些封閉式的問題，如預計演講的時間長度、Python 難易度、演講類型…等等。</p></li><li><p>審稿<br>提案完成後主辦單位會請各領域的專家審稿和給予修改建議，並投票決定是否同意接受提案。<br>在這個部分是我覺得最暖心的地方，因為 PyCon 有要求評審不是用挑戰的角度挑缺點，而是會跟講者一起思考如何讓演講發揮出更大的價值。不論最終的結果是被接受或拒絕，相信每位提案人在這個階段都會有滿滿的收穫與成長！</p></li><li><p>分析<br>一旦確定入選後，你會需要把前面提案中的想法轉化成實際的分析成果（但其實應該要更早就開始準備資料）！這個階段其實也相當不容易，因為我們平常可能知道/看過/學過相當多的分析技術，很容易會「覺得」怎麼做就可以完成，但只有實際進去分析後才會發現理想和現實間有多少差距。<br>分析結果符合理想當然很好，但是不理想的結果才會帶給我們更多的成長，因為我們需要思考為什麼結果會不理想並嘗試找各種方法來處理問題，這些過程（特別是失敗的過程）都會變成你成長的養分。</p></li><li><p>報告<br>完成分析後還需要開始將分析的方法、流程與發現梳理成有系統的知識，並製作成簡報向會眾分享。<br>這裡要提醒的是，一定要回頭檢視並緊扣演講的目標！在分析時一定有相當繁雜的過程，例如資料收集、前處理、特徵工程、分析方法…等等，因為演講的時間很寶貴，所以一定要把時間留給真正有價值的地方！ 然後反覆進行練習，要多從會眾的角度思考簡報的陳述方式、傳遞的想法是否清晰、容易吸收。</p></li></ol><p>其實在每個階段難免都會遇到各種大大小小的困難，如果有遇到一些令你崩潰到想要放棄的瞬間，請回頭想想當初為什麼會想要投稿的原因～而且能獲得登台演講的機會真的非常不容易，當機會到來時一定要好好把握住，然後跟自己說「加油，一定要做的比好還要更好！」。</p><p>感謝主辦單位、匿名評審、聽眾、長官、同事以及朋友在過程中曾給我的許多協助！</p>]]></content>
      
      
        <tags>
            
            <tag> PyCon TW2020 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何透過代理伺服器請求資料?</title>
      <link href="/2020/08/02/Crawler-usproxy/"/>
      <url>/2020/08/02/Crawler-usproxy/</url>
      <content type="html"><![CDATA[<p>當我們在對特定網站進行網路爬蟲的任務時，經常會遇到 鎖定IP 的反爬蟲機制，這時候透過代理伺服器來向網站請求資料就是對應的解決方式!</p><p>*本文接受 <a href="https://www.books.com.tw/web/sys_puballb/books/?pubid=deepmind" target="_blank" rel="noopener">深智數位出版社</a> 贊助撰寫而成</p><a id="more"></a><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3dHcQm6hQu-EMLFxBt7PijnZ0RSRS0JBJFNgSCg2GJT5JNwW4qHfUK1tay3flHRDsmpYB9EPD2K-jjRAd5tsicjQsCt6I8IcJgmf8YLQsnGD_5WTPkJCxFrQNRm6rA2Ljvgw9TQSb1NA7IHm4OZ79mO=w1164-h600-no?authuser=1" alt=""></p><p>網路上有許多網站有提供免費的代理伺服器，其中一個就是 <a href="https://www.us-proxy.org/" target="_blank" rel="noopener">USProxy</a>。由於這個網站並沒有設定反爬蟲機制，因此我們只需要直接 requests 這個網址就可以抓到對應的資料囉!</p><h1>透過代理伺服器請求資料</h1><ul><li>只需要在 get 的參數中加上 proxies 的參數就可以囉，而 proxies 中則需要放入在 us-proxy 取回的 IP 資料</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>:<span class="string">'34.105.59.26:80'</span>,</span><br><span class="line">    <span class="string">'https'</span>:<span class="string">'34.105.59.26:80'</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">'https://news.ltn.com.tw/news/world/breakingnews/3247226'</span></span><br><span class="line">resp = requests.get(url, </span><br><span class="line">                    proxies=proxies,</span><br><span class="line">                    headers=&#123;<span class="string">'user-agent'</span>:<span class="string">'GoogleBot'</span>&#125;)</span><br><span class="line">soup = BeautifulSoup(resp.text)</span><br><span class="line">print(<span class="string">'標題:'</span>, soup.find(<span class="string">'h1'</span>).text)</span><br><span class="line">print(<span class="string">'內文:'</span>, <span class="string">''</span>.join(i.text <span class="keyword">for</span> i <span class="keyword">in</span> soup.find(<span class="string">'div'</span>,&#123;<span class="string">'itemprop'</span>:<span class="string">'articleBody'</span>&#125;).findAll(<span class="string">'p'</span>)))</span><br></pre></td></tr></table></figure><h1>附件</h1><p>完整程式代碼位置如下：<a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/USProxy/USProxy.ipynb" target="_blank" rel="noopener">USProxy.ipynb</a></p><h1>後記</h1><p>最後來分享一下我個人學習爬蟲的心得～</p><p>學習爬蟲有兩種途徑﹕</p><ol><li>挑個網站直接開始練習爬蟲，遇到問題再上網找答案</li><li>購買課程/書籍系統性的學習爬蟲的知識、技能</li></ol><p>第一種的優點是會讓學習的過程比較有趣，可以邊做邊學，但缺點是學習到的知識會比較鎖碎；第二種則反過來，雖然可以比較有系統、完整的學習到爬蟲的知識，但相應的也會多花費一些時間。</p><p>如果你是剛開始學習爬蟲的初學者，我滿推薦你可以買個課程/書籍來學習，因為在爬蟲的過程中會遇到各式各樣、光怪陸離的問題，有系統性的知識你將可以比較知道目前是卡在哪個環節，以及要用什麼樣的方式來克服問題。推薦爬蟲的初學者可以參考看看下面這本書唷!</p><ul><li><a href="https://www.books.com.tw/products/0010836667?sloc=main&amp;fbclid=IwAR0JYLYuMaoe17fSj-SX-XicD3gDbEqpqoaBps20tynFS5kvQB5-gCHTokc" target="_blank" rel="noopener">Python網路爬蟲：大數據擷取、清洗、儲存與分析：王者歸來</a><br><img src="https://lh3.googleusercontent.com/pw/ACtC-3euPGo2WISLh6WE9XQUqo617di410zacGT0KVk2YPorT8ajWtezpHbEtWzgPJZpEOkhm3owxM-ImSWTn6QCZx2qpYiOOexCF4GTQlUib6Fu8KUJV9OQbS3qLDQH3i2xxqGrBqAHWNwzTnEzN5clXO1X=s348-no?authuser=1" alt=""></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Crawler </tag>
            
            <tag> Python </tag>
            
            <tag> USProxy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>104人力銀行 網路爬蟲</title>
      <link href="/2020/06/19/Crawler-104HumanResource/"/>
      <url>/2020/06/19/Crawler-104HumanResource/</url>
      <content type="html"><![CDATA[<p>先前曾發過一篇透過 Selenium 的版本 <a href="https://tlyu0419.github.io/2019/04/18/Crawl-JobList104/">爬蟲_104人力銀行工作清單</a> ，但隨著爬蟲技術的提升，就想要來更新一下爬蟲的程式 xD</p><p>註：本篇文章僅供研究使用，請勿惡意大量爬取資料造成對方公司的負擔</p><a id="more"></a><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3ciaYU9BbEwHw4gX7S46FHWoRrAcOXTndnGqWfmueCxFomDoMznUGIxyTGMaTL7OjBlJQ31Vac8q-BhHdNcozJvU2bIlb_2khkYbqtlfEsdbaA1tvcUhI3VhX0Q25wGKf1HyKXbaBHPqoRgoJ7a-ql5=w662-h150-no?authuser=1" alt=""></p><h1>104人力銀行 爬蟲要點</h1><ol><li>每次查詢職缺的結果最多回傳20筆*150個分頁的資料量，也就是說當我們查詢的條件超過3000筆時更多的資料就會被截掉。怎麼樣才能抓完完整的資料呢?<ul><li>解決方式是透過條件來將查詢的結果細緻化，例如按照區域、職務類型將查詢結果細分，最後再進行合併</li></ul></li><li>因為資料量相當多，如果要抓完完整的資料會需要約1整天的時間，那麼一次抓不完的話要怎麼辦呢?<ul><li>一種解決方式是用多線程的方式來爬資料，加速爬蟲的效率</li><li>如果是用一般的方式爬蟲的話，可以將爬取的結果分階段保存資料，而下次爬蟲時只需要繼續爬未完成的資料即可</li></ul></li><li>承1，由於我們將查詢結果按照地區與職務進行細分，大多數組合並不會都有150個分頁的資料量，如果將每個組合都送 150 次 request 無疑會浪費相當多時間，怎麼樣可以更有效率呢?<ul><li>解決方式是檢查每次查詢的結果，如果回傳20筆職缺，表示下一個分頁還有資料，但如果小於20個分頁就表示後面沒有資料了</li></ul></li><li>雖然 104 沒有設定反爬蟲機制，但是當 request 的速度太快時仍偶爾會有連線失敗的情況發生，但我們不想因此就讓每次 request 都 sleep 一次而降低爬蟲的效率，這時候該如何處理呢?<ul><li>解決方式是透過 try-except，當 try 成功時才繼續前往下一個分頁，如果失敗了就將同一頁的網址再送一次 request 取資料</li></ul></li></ol><h1>104人力銀行 爬蟲流程</h1><h2 id="載入使用套件"><a class="header-anchor" href="#載入使用套件">¶</a>載入使用套件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</span><br></pre></td></tr></table></figure><h2 id="設定-request-參數"><a class="header-anchor" href="#設定-request-參數">¶</a>設定 request 參數</h2><p>出於習慣就順手加上 user-agents了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="細分查詢結果"><a class="header-anchor" href="#細分查詢結果">¶</a>細分查詢結果</h2><p>同第 1 個爬蟲要點所述，因為資料量龐大，我們需要將查詢結果細分才能確保抓到完整的資料，而在這裡細分的方式是依照地區代碼和職務類型進行查詢。</p><h3 id="地區代碼"><a class="header-anchor" href="#地區代碼">¶</a>地區代碼</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://static.104.com.tw/category-tool/json/Area.json'</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line">df1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> resp.json()[<span class="number">0</span>][<span class="string">'n'</span>]:</span><br><span class="line">    ndf = pd.DataFrame(i[<span class="string">'n'</span>])</span><br><span class="line">    ndf[<span class="string">'city'</span>] = i[<span class="string">'des'</span>]</span><br><span class="line">    df1.append(ndf)</span><br><span class="line">df1=pd.concat(df1, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">df1 = df1.loc[:,[<span class="string">'city'</span>,<span class="string">'des'</span>,<span class="string">'no'</span>]]</span><br><span class="line">df1 = df1.sort_values(<span class="string">'no'</span>)</span><br><span class="line">df1</span><br></pre></td></tr></table></figure><h3 id="職務代碼"><a class="header-anchor" href="#職務代碼">¶</a>職務代碼</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">url= <span class="string">'https://static.104.com.tw/category-tool/json/JobCat.json'</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line">df2 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> resp.json():</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> i[<span class="string">'n'</span>]:</span><br><span class="line">        ndf = pd.DataFrame(j[<span class="string">'n'</span>])</span><br><span class="line">        ndf[<span class="string">'des1'</span>] = i[<span class="string">'des'</span>]<span class="comment"># 職務大分類</span></span><br><span class="line">        ndf[<span class="string">'des2'</span>] = j[<span class="string">'des'</span>]<span class="comment"># 職務小分類</span></span><br><span class="line">        df2.append(ndf)</span><br><span class="line">df2 = pd.concat(df2, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">df2 = df2.loc[:,[<span class="string">'des1'</span>, <span class="string">'des2'</span>, <span class="string">'des'</span>, <span class="string">'no'</span>]]</span><br><span class="line">df2 = df2.sort_values(<span class="string">'no'</span>)</span><br><span class="line">df2</span><br></pre></td></tr></table></figure><h2 id="讀取先前的爬取結果"><a class="header-anchor" href="#讀取先前的爬取結果">¶</a>讀取先前的爬取結果</h2><p>同第 2 個爬蟲要點所述，如果是第一次執行程式這段語法可以不用執行，而如果先前有保存結果的話會在這裡偵測先前爬過哪些資料</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp = pd.DataFrame([re.sub(<span class="string">'\.pkl'</span>,<span class="string">''</span>,file)<span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">'./data'</span>)],columns=[<span class="string">'no'</span>])</span><br><span class="line">df1 = pd.merge(df1, tmp, how=<span class="string">'left'</span>,on=<span class="string">'no'</span>,indicator=<span class="keyword">True</span>)</span><br><span class="line">df1 = df1.loc[df1[<span class="string">'_merge'</span>]!=<span class="string">'both'</span>,:]</span><br><span class="line">df1</span><br></pre></td></tr></table></figure><h2 id="爬取職缺資料"><a class="header-anchor" href="#爬取職缺資料">¶</a>爬取職缺資料</h2><ul><li>這裡是處理第 3 和第 4 點的爬蟲要點，因為不會每個查詢組合都有 150 個分頁的資料，因此需要偵測 request 回來幾筆職缺的資料，等於 20 筆就繼續爬下一個分頁的資料；而小於 20 筆資料則打破迴圈轉去爬下一個組合的職缺資料。</li><li>而 request 太快時偶爾會失敗，因此需要用 try-except 函數，當 request 失敗就重新再爬一次資料</li><li>最後就是要從 request 回來的結果做一些解析，讓我們從半結構化的資料中萃取成結構化的資料，因為是比較瑣碎的定位 elemnet 在這裡就不多說明了!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">columns = [<span class="string">'公司名稱'</span>,<span class="string">'公司編號'</span>,<span class="string">'公司類別'</span>,<span class="string">'公司類別描述'</span>, <span class="string">'公司連結'</span>,<span class="string">'職缺名稱'</span>,<span class="string">'職務性質'</span>,<span class="string">'職缺大分類'</span>, <span class="string">'職缺中分類'</span>,<span class="string">'職缺小分類'</span>, <span class="string">'職缺編號'</span>, <span class="string">'職務內容'</span>,<span class="string">'更新日期'</span>, <span class="string">'職缺連結'</span>, <span class="string">'標籤'</span>,<span class="string">'公司地址'</span>,<span class="string">'地區'</span>,<span class="string">'經歷'</span>,<span class="string">'學歷'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> areades, areacode <span class="keyword">in</span> zip(df1[<span class="string">'des'</span>],df1[<span class="string">'no'</span>]):</span><br><span class="line">    values = []</span><br><span class="line">    <span class="keyword">for</span> jobdes1, jobdes2, jobdes, jobcode <span class="keyword">in</span> zip(df2[<span class="string">'des1'</span>], df2[<span class="string">'des2'</span>], df2[<span class="string">'des'</span>], df2[<span class="string">'no'</span>]):</span><br><span class="line">        print(areades, <span class="string">' | '</span>, jobdes1, <span class="string">' - '</span>, jobdes2, <span class="string">' - '</span> ,jobdes)</span><br><span class="line">        page = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> page &lt;<span class="number">150</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                url = <span class="string">'https://www.104.com.tw/jobs/search/?ro=0&amp;jobcat=&#123;&#125;&amp;jobcatExpansionType=1&amp;area=&#123;&#125;&amp;order=11&amp;asc=0&amp;page=&#123;&#125;&amp;mode=s&amp;jobsource=2018indexpoc'</span>.format(jobcode, areacode, page)</span><br><span class="line">                print(url)</span><br><span class="line">                resp = requests.get(url,headers=headers)</span><br><span class="line">                soup = BeautifulSoup(resp.text)</span><br><span class="line">                soup2 = soup.find(<span class="string">'div'</span>,&#123;<span class="string">'id'</span>:<span class="string">'js-job-content'</span>&#125;).findAll(<span class="string">'article'</span>,&#123;<span class="string">'class'</span>:<span class="string">'b-block--top-bord job-list-item b-clearfix js-job-item'</span>&#125;)</span><br><span class="line">                print(len(soup2))</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> job <span class="keyword">in</span> soup2:</span><br><span class="line">                                        </span><br><span class="line">                    update_date = job.find(<span class="string">'span'</span>,&#123;<span class="string">'class'</span>:<span class="string">'b-tit__date'</span>&#125;).text</span><br><span class="line">                    update_date = re.sub(<span class="string">'\r|\n| '</span>,<span class="string">''</span>,update_date)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        address = job.select(<span class="string">'ul &gt; li &gt; a'</span>)[<span class="number">0</span>][<span class="string">'title'</span>]</span><br><span class="line">                        address = re.findall(<span class="string">'公司住址：(.*?)$'</span>,address)[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        address = <span class="string">''</span></span><br><span class="line">                   </span><br><span class="line">                    loc = job.find(<span class="string">'ul'</span>,&#123;<span class="string">'class'</span>:<span class="string">'b-list-inline b-clearfix job-list-intro b-content'</span>&#125;).findAll(<span class="string">'li'</span>)[<span class="number">0</span>].text</span><br><span class="line">                    exp = job.find(<span class="string">'ul'</span>,&#123;<span class="string">'class'</span>:<span class="string">'b-list-inline b-clearfix job-list-intro b-content'</span>&#125;).findAll(<span class="string">'li'</span>)[<span class="number">1</span>].text</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        edu = job.find(<span class="string">'ul'</span>,&#123;<span class="string">'class'</span>:<span class="string">'b-list-inline b-clearfix job-list-intro b-content'</span>&#125;).findAll(<span class="string">'li'</span>)[<span class="number">2</span>].text</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        edu = <span class="string">''</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        content = job.find(<span class="string">'p'</span>).text</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        content = <span class="string">''</span></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        tags = [tag.text <span class="keyword">for</span> tag <span class="keyword">in</span> soup2[<span class="number">0</span>].find(<span class="string">'div'</span>,&#123;<span class="string">'class'</span>:<span class="string">'job-list-tag b-content'</span>&#125;).findAll(<span class="string">'span'</span>)]</span><br><span class="line">                    <span class="keyword">except</span>:</span><br><span class="line">                        tags = []</span><br><span class="line">                    </span><br><span class="line">                    </span><br><span class="line">                    value = [job[<span class="string">'data-cust-name'</span>], <span class="comment"># 公司名稱</span></span><br><span class="line">                             job[<span class="string">'data-cust-no'</span>], <span class="comment"># 公司編號</span></span><br><span class="line">                             job[<span class="string">'data-indcat'</span>], <span class="comment"># 公司類別</span></span><br><span class="line">                             job[<span class="string">'data-indcat-desc'</span>], <span class="comment"># 公司類別描述</span></span><br><span class="line">                             job.select(<span class="string">'ul &gt; li &gt; a'</span>)[<span class="number">0</span>][<span class="string">'href'</span>], <span class="comment"># 公司連結</span></span><br><span class="line">                             job[<span class="string">'data-job-name'</span>],<span class="comment"># 職缺名稱</span></span><br><span class="line">                             job[<span class="string">'data-job-ro'</span>], <span class="comment"># 職務性質 _判斷全職兼職 1全職/2兼職/3高階/4派遣/5接案/6家教</span></span><br><span class="line">                             jobdes1, <span class="comment"># 職缺大分類</span></span><br><span class="line">                             jobdes2, <span class="comment"># 職缺中分類</span></span><br><span class="line">                             jobdes, <span class="comment"># 職缺小分類</span></span><br><span class="line">                             job[<span class="string">'data-job-no'</span>],<span class="comment"># 職缺編號</span></span><br><span class="line">                             content, <span class="comment"># 職務內容</span></span><br><span class="line">                             update_date, <span class="comment"># 更新日期</span></span><br><span class="line">                             job.find(<span class="string">'a'</span>,&#123;<span class="string">'class'</span>:<span class="string">'js-job-link'</span>&#125;)[<span class="string">'href'</span>], <span class="comment"># 職缺連結</span></span><br><span class="line">                             tags, <span class="comment"># 標籤</span></span><br><span class="line">                             address,<span class="comment"># 公司地址</span></span><br><span class="line">                             loc, <span class="comment"># 地區</span></span><br><span class="line">                             exp,<span class="comment"># 經歷</span></span><br><span class="line">                             edu  <span class="comment"># 學歷</span></span><br><span class="line">                            ]</span><br><span class="line">                    values.append(value)</span><br><span class="line">                </span><br><span class="line">                page+=<span class="number">1</span></span><br><span class="line">                print(len(values))</span><br><span class="line">                <span class="keyword">if</span> len(soup2) &lt; <span class="number">20</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">'Retry'</span>)</span><br><span class="line">        </span><br><span class="line">    df = pd.DataFrame()</span><br><span class="line">    df = pd.DataFrame(values, columns=columns)</span><br><span class="line">    df.to_pickle(<span class="string">'./data/'</span> + areacode + <span class="string">'.pkl'</span>)</span><br><span class="line">    clear_output()</span><br><span class="line">    print(<span class="string">'===================================  Save Data  ==================================='</span>)</span><br></pre></td></tr></table></figure><h2 id="組合爬蟲結果"><a class="header-anchor" href="#組合爬蟲結果">¶</a>組合爬蟲結果</h2><ul><li>由於先前將職缺資訊分成許多小查詢來抓資料並存成不同的檔案，因此在這裡我們就寫個簡單的迴圈來讀取與合併資料吧!</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(<span class="string">'./data/'</span>):</span><br><span class="line">    ndf = pd.read_pickle(<span class="string">'./data/'</span> + i)</span><br><span class="line">    df.append(ndf)</span><br><span class="line">df = pd.concat(df, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">df.info()</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3ee_55qmAvXrGapUzuWX-3QJq1EkLlxInkqVz8HxXz6vkgVTsoI-W8RclyPhrG6-A_SPrqFrrLqVdOz3OC_nKSTIOJQuYJC1-B_H1PDFnqnLwM83l6OSSTwul23XsEQj9QCIvhX1SWmrg8AlzogTcpk=w865-h389-no?authuser=1" alt=""></p><ul><li>如圖所示，我們抓到的許多有價值的資訊，包含<ul><li>公司名稱</li><li>公司編號</li><li>公司類別</li><li>公司類別描述</li><li>公司連結</li><li>職缺名稱</li><li>職務性質</li><li>職缺大分類</li><li>職缺中分類</li><li>職缺小分類</li><li>職缺編號</li><li>職務內容</li><li>更新日期</li><li>職缺連結</li><li>標籤</li><li>公司地址</li><li>地區</li><li>經歷要求</li><li>學歷要求</li></ul></li></ul><h2 id="後記"><a class="header-anchor" href="#後記">¶</a>後記</h2><p>有了這些資料我們就可以進行許多有價值的分析囉，包含各縣市的職缺數量、產業結構差異，並且可以進一步幫助求職者們在選擇工作時可以評估自己個興趣與期望的工作地點等等!<br><img src="https://lh3.googleusercontent.com/pw/ACtC-3cqXeBjLllocOAX3JBLY74kfpStFSa2fP4tnBVo5U7GLA3vRoaf8Gvu0kKb9mWxqbtzRWjfA0azeeGFyoFIdrcNVE58LipYtIRPhCP6AfuhFtTSiLZgxi2hTPUPN_c3LO9RE7RvV_JIieC2nMVAa05L=w1702-h929-no?authuser=1" alt=""></p><h2 id="附件"><a class="header-anchor" href="#附件">¶</a>附件</h2><ul><li><a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/HumanResource_104/HumanResource_104.ipynb" target="_blank" rel="noopener">HumanResource_104.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Crawler </tag>
            
            <tag> Python </tag>
            
            <tag> 104人力銀行 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MOMO購物網 網路爬蟲</title>
      <link href="/2020/06/15/Crawler-momo/"/>
      <url>/2020/06/15/Crawler-momo/</url>
      <content type="html"><![CDATA[<p>最近梅雨季讓房間變得好潮濕，為了避免衣服發霉就決定要來採購一台除濕機，<br>但是除濕機的品牌又多又雜，在網站上總是看的頭昏眼花，不如就來寫個爬蟲程式自動幫我們整理出結構化的資料吧!(咦!這句話怎麼這麼熟悉?)</p><p>註：本篇文章僅供研究使用，請勿惡意大量爬取資料造成對方公司的負擔</p><a id="more"></a><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3cO43RFldRLVUBSGWgpzGV12Dj8nAHJYzQ6hD63Gsq0P9y-vCV62RdZpHM-7z3_YI_8FSbMJ7uZ_XCmYy9MalTddzIR1jFWmxgqvhoOZGav4kDbAiRi4wzbhLEVQ30mweWTWupk16RxUsmNCzKoSQEn=w540-h240-no?authuser=1" alt=""></p><h1>MOMO購物網 爬蟲要點</h1><ol><li>如果電腦版的網頁不好爬怎麼辦…?那就改爬手機版的網頁吧!</li><li>資料的整理會需要使用一些正則表達式，但這比較瑣碎就不多說了</li></ol><h1>MOMO購物網 爬蟲流程</h1><h2 id="載入使用套件"><a class="header-anchor" href="#載入使用套件">¶</a>載入使用套件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure><h2 id="設定-request-參數"><a class="header-anchor" href="#設定-request-參數">¶</a>設定 request 參數</h2><p>在這裡沒有設定 User-Agent 會被拒絕連線，因此當我們無法成功取回資料時的第一個步驟一定是加上 User-Agent 來進行測試!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="收集產品連結清單"><a class="header-anchor" href="#收集產品連結清單">¶</a>收集產品連結清單</h2><p>為什麼前面說電腦版的網頁不好爬呢?我們看第一張圖，跟伺服器請求資料的方式是用 POST 的方式，而且 POST 過去的參數也相當複雜，我有嘗試 POST 這些資料過去但還是沒辦法正常請求到資料，估計還有其他的反爬蟲機制&gt;&quot;&lt;</p><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3dqxgliG8avJpnCbeeezHz8p9u0JGiGQgvpImD3Gbx7rLyTEOYbFyzEC0G_mZADkAeejET10Gy4SQCMIzj67CDyy8U-Cc25TusTz9tpPSx9c054XEhrdTOybi2ThKQk0tWTZJ1CdYdu24N-x0L3gsXP=w1012-h495-no?authuser=1" alt=""></p><p>接著改成用手機版的網頁，請求資料的方式變成 GET，而且送的參數變得相當簡單，基本上就是查詢的關鍵詞和第一個分頁就可以了!所以…在這裡就可以改成抓簡單的手機版網頁就好 XDD<br><img src="https://lh3.googleusercontent.com/pw/ACtC-3c7SUhZwiF4pNg7UbxwOkhsEJ40BMPTiY8-XLflUaPnAnw1gNQhIAnvk1a190vvzflx8HAZ5OtMEn6cjBn5bfN8dLqC8LFYjvRhiZy4kLaUxpMLEzUnZe8AwZsBiO3Q1os_yNVJU508a-V1HvhuVnaQ=w1012-h507-no?authuser=1" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">keyword = <span class="string">'除濕機'</span></span><br><span class="line">pages = <span class="number">30</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'</span>&#125;</span><br><span class="line">urls = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, pages):</span><br><span class="line">    url = <span class="string">'https://m.momoshop.com.tw/search.momo?_advFirst=N&amp;_advCp=N&amp;curPage=&#123;&#125;&amp;searchType=1&amp;cateLevel=2&amp;ent=k&amp;searchKeyword=&#123;&#125;&amp;_advThreeHours=N&amp;_isFuzzy=0&amp;_imgSH=fourCardType'</span>.format(page, keyword)</span><br><span class="line">    print(url)</span><br><span class="line">    resp = requests.get(url, headers=headers)</span><br><span class="line">    <span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</span><br><span class="line">        soup = BeautifulSoup(resp.text)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> soup.select(<span class="string">'li.goodsItemLi &gt; a'</span>):</span><br><span class="line">            urls.append(<span class="string">'https://m.momoshop.com.tw'</span>+item[<span class="string">'href'</span>])</span><br><span class="line">    urls = list(set(urls))</span><br><span class="line">    print(len(urls))</span><br><span class="line"><span class="comment">#     break</span></span><br></pre></td></tr></table></figure><h2 id="爬取產品資料"><a class="header-anchor" href="#爬取產品資料">¶</a>爬取產品資料</h2><p>剩下就是單純的依照前面保存的網址送 request 請求資料，接著再自行定位一下需要資訊的位置，並且搭配正則表達式清理一下不需要的資料即可! 因為比較瑣碎在這裡就不多說了~</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">df = []</span><br><span class="line"><span class="keyword">for</span> i, url <span class="keyword">in</span> enumerate(urls):</span><br><span class="line">    columns = []</span><br><span class="line">    values = []</span><br><span class="line">    </span><br><span class="line">    resp = requests.get(url, headers=headers)</span><br><span class="line">    soup = BeautifulSoup(resp.text)</span><br><span class="line">    <span class="comment"># 標題</span></span><br><span class="line">    title = soup.find(<span class="string">'meta'</span>,&#123;<span class="string">'property'</span>:<span class="string">'og:title'</span>&#125;)[<span class="string">'content'</span>]</span><br><span class="line">    <span class="comment"># 品牌</span></span><br><span class="line">    brand = soup.find(<span class="string">'meta'</span>,&#123;<span class="string">'property'</span>:<span class="string">'product:brand'</span>&#125;)[<span class="string">'content'</span>]</span><br><span class="line">    <span class="comment"># 連結</span></span><br><span class="line">    link = soup.find(<span class="string">'meta'</span>,&#123;<span class="string">'property'</span>:<span class="string">'og:url'</span>&#125;)[<span class="string">'content'</span>]</span><br><span class="line">    <span class="comment"># 原價</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        price = re.sub(<span class="string">r'\r\n| '</span>,<span class="string">''</span>,soup.find(<span class="string">'del'</span>).text)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        price = <span class="string">''</span></span><br><span class="line">    <span class="comment"># 特價</span></span><br><span class="line">    amount = soup.find(<span class="string">'meta'</span>,&#123;<span class="string">'property'</span>:<span class="string">'product:price:amount'</span>&#125;)[<span class="string">'content'</span>]</span><br><span class="line">    <span class="comment"># 類型</span></span><br><span class="line">    cate = <span class="string">''</span>.join([i.text <span class="keyword">for</span> i <span class="keyword">in</span> soup.findAll(<span class="string">'article'</span>,&#123;<span class="string">'class'</span>:<span class="string">'pathArea'</span>&#125;)])</span><br><span class="line">    cate = re.sub(<span class="string">'\n|\xa0'</span>,<span class="string">' '</span>,cate)</span><br><span class="line">    <span class="comment"># 描述</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        desc = soup.find(<span class="string">'div'</span>,&#123;<span class="string">'class'</span>:<span class="string">'Area101'</span>&#125;).text</span><br><span class="line">        desc = re.sub(<span class="string">'\r|\n| '</span>, <span class="string">''</span>, desc)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        desc = <span class="string">''</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'==================  &#123;&#125;  =================='</span>.format(i))    </span><br><span class="line">    print(title)</span><br><span class="line">    print(brand)</span><br><span class="line">    print(link)</span><br><span class="line">    print(amount)</span><br><span class="line">    print(cate)</span><br><span class="line">    </span><br><span class="line">    columns += [<span class="string">'title'</span>, <span class="string">'brand'</span>, <span class="string">'link'</span>, <span class="string">'price'</span>, <span class="string">'amount'</span>, <span class="string">'cate'</span>, <span class="string">'desc'</span>]</span><br><span class="line">    values += [title, brand, link, price, amount, cate, desc]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 規格</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'div.attributesArea &gt; table &gt; tr'</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            column = i.find(<span class="string">'th'</span>).text</span><br><span class="line">            column = re.sub(<span class="string">'\n|\r| '</span>,<span class="string">''</span>,column)</span><br><span class="line">            value = <span class="string">''</span>.join([j.text <span class="keyword">for</span> j <span class="keyword">in</span> i.findAll(<span class="string">'li'</span>)])</span><br><span class="line">            value = re.sub(<span class="string">'\n|\r| '</span>,<span class="string">''</span>,value)</span><br><span class="line">            columns.append(column)</span><br><span class="line">            values.append(value)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    ndf = pd.DataFrame(data=values, index=columns).T</span><br><span class="line">    df.append(ndf)</span><br><span class="line">df=pd.concat(df, ignore_index=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="爬取結果"><a class="header-anchor" href="#爬取結果">¶</a>爬取結果</h2><p>最後總共抓取了 580 個產品的資訊，包含了商品名稱、品牌、描述、價格、類型、連結等等資訊，詳細資訊如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3eBr7o6YFk0r6LoG3qE8Hqa0Kwh9jnXIXISjkbaRAj5WxlGgps2VYbw0Ak-32KX__-pYNimknbj0lKeMolELyRdJocHJGhtvekE97MF73wHOtLOviDY90ghHhv7mePUuIqK_R34CPVGIaQ1kjzGdqeb=w298-h350-no?authuser=1" alt=""></p><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><p>抓完資料記得保存結果，不然你會需要重新再抓一次&gt;&quot;&lt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_excel(<span class="string">'./MOMO.xlsx'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3dVLlxeydSw0k76kRresUiOwycW2IlYaJF4eZV_iP1oNe3SYFdAOzDngqYEFDkA8o-7aQKW7cf6HtcltIBbN36dZOo4Mzm4ujSQL-oHTLJxtZmtqQ8g8EPnkpxFOj630w6cs5tUbPFznlzcISf-yKAa=w1785-h978-no?authuser=1" alt=""></p><h2 id="附件"><a class="header-anchor" href="#附件">¶</a>附件</h2><ul><li><a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/MOMO/MOMO.ipynb" target="_blank" rel="noopener">MOMO.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Crawler </tag>
            
            <tag> Python </tag>
            
            <tag> MOMO </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>露天拍賣 網路爬蟲</title>
      <link href="/2020/06/14/Crawler-Ruten/"/>
      <url>/2020/06/14/Crawler-Ruten/</url>
      <content type="html"><![CDATA[<p>最近想要購買新的筆記型電腦，但是筆記型電腦的品牌/規格又多又複雜，在網站上總是看的頭昏眼花，不如就來寫個爬蟲程式自動幫我們整理出結構化的資料吧!</p><p>註：本篇文章僅供研究使用，請勿惡意大量爬取資料造成對方公司的負擔</p><a id="more"></a><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3f3Eiml_Fx6A3olC8OkgSLDRr09cXKaRBzV-QiDbCmMYubfCTr3ueum3dwMzhqCyltgw5vtqzy3zjsvh56EcYX3tv8t854ibfxQtHQTLjnbOhrVaNioBrMyTdcPqlDs6ZpqI5JHu2qIvupbDa5iH3wI=w512-h250-no?authuser=1" alt=""></p><h1>露天拍賣 爬蟲要點</h1><ol><li>在收集產品清單時需要透過 F12 的檢查功能來找出 API</li><li>產品描述要呼叫另外一個 API 來取資料</li></ol><h1>露天拍賣 爬蟲流程</h1><h2 id="載入使用套件"><a class="header-anchor" href="#載入使用套件">¶</a>載入使用套件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure><h2 id="設定-request-參數"><a class="header-anchor" href="#設定-request-參數">¶</a>設定 request 參數</h2><p>在這裡沒有設定 User-Agent 會被拒絕連線，因此當我們無法成功取回資料時的第一個步驟一定是加上 User-Agent 來進行測試!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="收集產品連結清單"><a class="header-anchor" href="#收集產品連結清單">¶</a>收集產品連結清單</h2><p>這裡在收集產品清單時使用的 API 要用 F12 的檢查功能來尋找，而在這個 API 中有個 offset 的參數，這是在記錄要從哪裡開始加載更多資料。由於每次查詢是80筆(limit參數)，因此每次查詢的 offset 是用 1 + 80*第幾次查詢來抓資料</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 商品分類</span></span><br><span class="line">cateid = <span class="string">'001100020027'</span></span><br><span class="line">pages = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">prodids = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> list(range(<span class="number">1</span>, pages)):</span><br><span class="line">    offset = <span class="number">1</span> + <span class="number">80</span>*page</span><br><span class="line">    url = <span class="string">'https://rtapi.ruten.com.tw/api/search/v3/index.php/core/prod?cateid=&#123;&#125;&amp;sort=rnk%2Fdc&amp;offset=&#123;&#125;&amp;limit=80&amp;2653512&amp;_callback=jsonpcb_CoreProd'</span>.format(cateid, offset)</span><br><span class="line">    resp = requests.get(url, headers=headers)</span><br><span class="line">    data = re.sub(<span class="string">r'try\&#123;jsonpcb_CoreProd\(|\);\&#125;catch\(e\)\&#123;if\(window.console\)\&#123;console.log\(e\);\&#125;\&#125;'</span>,<span class="string">''</span>, resp.text)</span><br><span class="line">    <span class="keyword">for</span> prod <span class="keyword">in</span> json.loads(data)[<span class="string">'Rows'</span>]:</span><br><span class="line">        prodids.append(prod[<span class="string">'Id'</span>])</span><br><span class="line">    prodids = list(set(prodids))</span><br><span class="line">    print(<span class="string">'There are &#123;&#125; prods in list.'</span>.format(len(prodids)))</span><br></pre></td></tr></table></figure><h2 id="爬取產品資料"><a class="header-anchor" href="#爬取產品資料">¶</a>爬取產品資料</h2><ul><li>這裡在抓產品名稱、幣別、價格、商品分類等等資訊的方法相當單純，就是簡單 request 產品的網址就可以了! 而產品的網址就是在前面「收集產品清單」階段中收集到的產品 ID前面加上’<a href="https://goods.ruten.com.tw/item/show?'%E7%9A%84%E7%B6%B2%E5%9D%80%E5%8D%B3%E5%8F%AF%E3%80%82" target="_blank" rel="noopener">https://goods.ruten.com.tw/item/show?'的網址即可。</a><blockquote><p>例如ID：30201451592585 產品的網址就是 <a href="https://goods.ruten.com.tw/item/show?30201451592585" target="_blank" rel="noopener">https://goods.ruten.com.tw/item/show?30201451592585</a></p></blockquote></li><li>比較複雜的地方在於產品描述不容易獲取，在這裡有兩個反爬蟲的機制：<ol><li>request 的網址藏在前面回傳的 descriptionUrl 中<blockquote><p>這裡要花時間找一下產品描述的 API 網址，結果是藏在前面 request 的結果當中。</p></blockquote></li><li>Referer：要在 request 的 headers 加上 referer，這個參數是告訴對方的伺服器說我們是從哪裡進來到這個API的。</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">df = []</span><br><span class="line"><span class="keyword">for</span> i, prodid <span class="keyword">in</span> enumerate(prodids):</span><br><span class="line">    print(<span class="string">'='</span>*<span class="number">60</span>)</span><br><span class="line">    print(<span class="string">'Dealing with &#123;&#125;: &#123;&#125;'</span>.format(i, prodid))</span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>&#125;</span><br><span class="line">    url = <span class="string">'https://goods.ruten.com.tw/item/show?'</span> + prodid</span><br><span class="line">    resp = requests.get(url, headers=headers)</span><br><span class="line">    soup = BeautifulSoup(resp.text)</span><br><span class="line">    data1 = json.loads(soup.findAll(<span class="string">'script'</span>,&#123;<span class="string">'type'</span>:<span class="string">'application/ld+json'</span>&#125;)[<span class="number">0</span>].text)</span><br><span class="line">    data2 = json.loads(soup.findAll(<span class="string">'script'</span>,&#123;<span class="string">'type'</span>:<span class="string">'application/ld+json'</span>&#125;)[<span class="number">1</span>].text)</span><br><span class="line">    data3 = json.loads(soup.findAll(<span class="string">'script'</span>,&#123;<span class="string">'type'</span>:<span class="string">'application/ld+json'</span>&#125;)[<span class="number">2</span>].text)</span><br><span class="line">    <span class="comment"># 產品名稱</span></span><br><span class="line">    print(data1[<span class="string">'name'</span>])</span><br><span class="line">    <span class="comment"># 幣別</span></span><br><span class="line">    print(data1[<span class="string">'offers'</span>][<span class="string">'priceCurrency'</span>])</span><br><span class="line">    <span class="comment"># 價格</span></span><br><span class="line">    print(data1[<span class="string">'offers'</span>][<span class="string">'price'</span>])</span><br><span class="line">    <span class="comment"># 商品分類</span></span><br><span class="line">    print(<span class="string">r'/'</span>.join([i[<span class="string">'item'</span>][<span class="string">'name'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data3[<span class="string">'itemListElement'</span>]]))</span><br><span class="line">    <span class="comment"># 商品描述</span></span><br><span class="line">    descurl = re.findall(<span class="string">r'goods_comments.php\?id=.*?o=[0-9]&#123;1,&#125;'</span>,resp.text)[<span class="number">0</span>]</span><br><span class="line">    descurl = re.sub(<span class="string">'&amp;amp;'</span>,<span class="string">'&amp;'</span>,descurl)</span><br><span class="line">    descurl = <span class="string">'https://goods.ruten.com.tw/item/'</span> + descurl</span><br><span class="line">    headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'GoogleBot'</span>,</span><br><span class="line">               <span class="string">'Referer'</span>: <span class="string">'https://goods.ruten.com.tw/item/show?&#123;&#125;'</span>.format(prodid)&#125;</span><br><span class="line">    resp2 = requests.get(descurl, headers=headers)</span><br><span class="line">    soup2 = BeautifulSoup(resp2.text)</span><br><span class="line">    desc = <span class="string">' '</span>.join(i.text <span class="keyword">for</span> i <span class="keyword">in</span> soup2.findAll(<span class="string">'p'</span>))</span><br><span class="line">    </span><br><span class="line">    ndf = pd.DataFrame([&#123;<span class="string">'name'</span>: data1[<span class="string">'name'</span>],</span><br><span class="line">                         <span class="string">'description'</span>:data1[<span class="string">'description'</span>],</span><br><span class="line">                         <span class="string">'productId'</span>:data1[<span class="string">'productId'</span>],</span><br><span class="line">                         <span class="string">'brand_name'</span>:data1[<span class="string">'brand'</span>][<span class="string">'name'</span>],</span><br><span class="line">                         <span class="string">'priceCurrency'</span>:data1[<span class="string">'offers'</span>][<span class="string">'priceCurrency'</span>],</span><br><span class="line">                         <span class="string">'price'</span>:data1[<span class="string">'offers'</span>][<span class="string">'price'</span>],</span><br><span class="line">                         <span class="string">'contentLocation'</span>:data2[<span class="string">'contentLocation'</span>],</span><br><span class="line">                         <span class="string">'item_name'</span>:<span class="string">r'/'</span>.join([i[<span class="string">'item'</span>][<span class="string">'name'</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data3[<span class="string">'itemListElement'</span>]]),</span><br><span class="line">                         <span class="string">'describe'</span>:desc&#125;])</span><br><span class="line">    df.append(ndf)</span><br><span class="line"></span><br><span class="line">df = pd.concat(df, ignore_index=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="爬取結果"><a class="header-anchor" href="#爬取結果">¶</a>爬取結果</h2><p>最後總共抓取了 3,920 個產品的資訊，包含了產品ID、名稱、價格、賣家名稱、商品位置、商品描述等等資訊，詳細資訊如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3d_PsSe8-oV2COYsOD8PzUVjAbnkzJ21B0Mu9_oA64V2VBfKpMlHe_9-BNGNYjNXf3ejKRX-Ube79JJr9UNkPMv6t22TYp5sZ8mBznTRxL_3LhzPChX0BwdDo3WEgWM4c8N08rbkDgXF10mhiRXs4ps=w463-h267-no?authuser=1" alt=""></p><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><p>抓完資料記得保存結果，不然你會需要重新再抓一次&gt;&quot;&lt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_excel(<span class="string">'./RuTen.xlsx'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3dQrP2AI9YMGmYzh7_4ZeaKHh-axwd-alHCYLsFPQaKxreC5S2c-iPPWBxrxHbNgnCop6dMu7UHS-4xGgSKfxF6PrGj_eTDvC2G1BQ5tUMzQSCq5XN9zKLzg50-X8NjSlhPqW9XQsfMlkTbY19UQPCk=w1784-h978-no?authuser=1" alt=""></p><h2 id="附件"><a class="header-anchor" href="#附件">¶</a>附件</h2><ul><li><a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/ruten/Ruten.ipynb" target="_blank" rel="noopener">Ruten.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Crawler </tag>
            
            <tag> Python </tag>
            
            <tag> 露天拍賣 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PChome 網路爬蟲</title>
      <link href="/2020/06/13/Crawler-PChome/"/>
      <url>/2020/06/13/Crawler-PChome/</url>
      <content type="html"><![CDATA[<p>PChome 的網路爬蟲程式並不難寫，但是如何寫得簡潔又有效率就不容易了! 在這篇文章我將簡介在開發 Pchome 的爬蟲程式時需要注意的事項與解決方法，並於最後附上程式碼提供參考。<br>註：本篇文章僅供研究使用，請勿惡意大量爬取資料造成對方公司的負擔</p><a id="more"></a><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3cjV87riQWFKNEXaGYqzdNpZxQrOFJapQukQx5vsDefWp8eE38M0nn0thmq9rUu1kRMvBPpGCS39GIZ9K6Dv3fx5YXyaXRamz2oycCFXUqumD7nGRH53b-neM0y3rUx-JS7W2H0Y3_mt5BUCs5WUtVh=w1495-h428-no?authuser=1" alt=""></p><h1>Pchome 爬蟲要點</h1><ol><li>工作階段要分清楚，request、parse不要混在一起做</li><li>先用 list 的方式存資料，最後再一起整理成 Dataframe</li><li>透過塞 Cookie 取代 sleep 5秒</li></ol><h1>Pchome 爬蟲流程</h1><h2 id="載入使用套件"><a class="header-anchor" href="#載入使用套件">¶</a>載入使用套件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br></pre></td></tr></table></figure><h2 id="設定-request-參數"><a class="header-anchor" href="#設定-request-參數">¶</a>設定 request 參數</h2><p>一般的爬蟲程式只有簡單塞 User-Agent 來作為模仿真人瀏覽網頁，而我在這裡多塞了 Cookie 的資訊。如果沒有放 Cookie 的話每次 request 資料會需要 sleep 5 秒，不然就會被鎖IP。而Cookie 中的 GoogleBot 是我隨意寫的，實際上可以自己填資料。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;<span class="string">'cookie'</span>: <span class="string">'ECC=GoogleBot'</span>,</span><br><span class="line">           <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="收集產品連結清單"><a class="header-anchor" href="#收集產品連結清單">¶</a>收集產品連結清單</h2><p>設定要查詢關鍵字與資料量，在這裡設定要查詢的是「筆電」，並查詢50個分頁的資料。可以依照自己的需求調整關鍵字，分頁的數量也可以自行增加或減少。</p><blockquote><p>註: 每個分頁大約20筆資料</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">'筆電'</span></span><br><span class="line"><span class="comment"># 每次加載分頁有20筆資料，可以依需求增加/減少</span></span><br><span class="line">pages = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get prods list</span></span><br><span class="line">prodids = []</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> list(range(<span class="number">1</span>, pages)):</span><br><span class="line">    url = <span class="string">'https://ecshweb.pchome.com.tw/search/v3.3/all/results?q=&#123;&#125;&amp;page=&#123;&#125;&amp;sort=sale/dc'</span>.format(query, page)</span><br><span class="line">    resp = requests.get(url,headers=headers)</span><br><span class="line">    <span class="keyword">for</span> prodid <span class="keyword">in</span> resp.json()[<span class="string">'prods'</span>]:</span><br><span class="line">        prodids.append(prodid[<span class="string">'Id'</span>])</span><br><span class="line">    prodids = list(set(prodids))</span><br><span class="line">    print(<span class="string">'There are &#123;&#125; products in list.'</span>.format(len(prodids)))</span><br></pre></td></tr></table></figure><h2 id="爬取產品資料"><a class="header-anchor" href="#爬取產品資料">¶</a>爬取產品資料</h2><p>Pchome 將產品資料放在兩個不同 API，因此我們在爬蟲時需要送產品ID 到這兩個 API上才能取到完整的資料。這裡建議先把所有商品在一個API上的資料都抓完後再去取第二個 API 的資料，因為兩個 API response的資料結構不一樣，我們在 parse 時會很容易搞混。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ecapi</span></span><br><span class="line">df1 = []</span><br><span class="line"><span class="keyword">for</span> i, Id <span class="keyword">in</span> enumerate(prodids):</span><br><span class="line">    columns, values = [], []</span><br><span class="line">    print(<span class="string">'Dealing with &#123;&#125;: &#123;&#125;'</span>.format(i, Id))</span><br><span class="line">    sleep(<span class="number">0.7</span>)</span><br><span class="line">    ecapi = <span class="string">'https://mall.pchome.com.tw/ecapi/ecshop/prodapi/v2/prod/&#123;&#125;&amp;fields=Seq,Id,Stmt,Slogan,Name,Nick,Store,PreOrdDate,SpeOrdDate,Price,Discount,Pic,Weight,ISBN,Qty,Bonus,isBig,isSpec,isCombine,isDiy,isRecyclable,isCarrier,isMedical,isBigCart,isSnapUp,isDescAndIntroSync,isFoodContents,isHuge,isEnergySubsidy,isPrimeOnly,isWarranty,isLegalStore,isOnSale,isPriceTask,isBidding,isETicket&amp;_callback=jsonp_prod&amp;1587196620'</span>.format(Id)</span><br><span class="line">    resp = requests.get(ecapi, headers=headers)</span><br><span class="line">    data = re.sub(<span class="string">'try&#123;jsonp_prod\(|\&#125;\);\&#125;catch\(e\)\&#123;if\(window.console\)\&#123;console.log\(e\)\;\&#125;'</span>,<span class="string">''</span>,resp.text)</span><br><span class="line">    data = json.loads(data)[Id+<span class="string">'-000'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> data.items():</span><br><span class="line">        columns.append(key)</span><br><span class="line">        values.append(value)</span><br><span class="line">    ndf = pd.DataFrame(data=values, index=columns).T</span><br><span class="line">    df1.append(ndf)</span><br><span class="line">df1 = pd.concat(df1, ignore_index=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cdn</span></span><br><span class="line">df2 = []</span><br><span class="line"><span class="keyword">for</span> i, Id <span class="keyword">in</span> enumerate(prodids):</span><br><span class="line">    columns, values = [], []</span><br><span class="line">    print(<span class="string">'Dealing with &#123;&#125;: &#123;&#125;'</span>.format(i, Id))</span><br><span class="line">    sleep(<span class="number">0.7</span>)</span><br><span class="line">    cdn = <span class="string">'https://ecapi.pchome.com.tw/cdn/ecshop/prodapi/v2/prod/&#123;&#125;/desc&amp;fields=Id,Stmt,Equip,Remark,Liability,Kword,Slogan,Author,Transman,Pubunit,Pubdate,Approve&amp;_callback=jsonp_desc'</span>.format(Id+<span class="string">'-000'</span>)</span><br><span class="line">    resp = requests.get(cdn, headers=headers)</span><br><span class="line">    data = re.sub(<span class="string">'try\&#123;jsonp_desc\(|\&#125;\);\&#125;catch\(e\)\&#123;if\(window.console\)\&#123;console.log\(e\)\;\&#125;'</span>,<span class="string">''</span>,resp.text)</span><br><span class="line">    data = json.loads(data)</span><br><span class="line">    data = data[Id]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> data.items():</span><br><span class="line">        columns.append(key)</span><br><span class="line">        values.append(value)</span><br><span class="line">    ndf = pd.DataFrame(data=values, index=columns).T</span><br><span class="line">    df2.append(ndf)</span><br><span class="line"></span><br><span class="line">df2 = pd.concat(df2, ignore_index=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="合併兩個資料表"><a class="header-anchor" href="#合併兩個資料表">¶</a>合併兩個資料表</h2><p>這裡就是我說資料結構不一致的地方<br>第一個 API 取回來的 產品ID 最後會包含’-000’的字串，而第二個 API 取回來的則是乾淨的產品ID，要將兩個資料表透過產品ID合併在一起時要先將’-000’的字串拿掉，具體的語法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1[<span class="string">'Id'</span>] = df1[<span class="string">'Id'</span>].apply(<span class="keyword">lambda</span> x: re.sub(<span class="string">'-000$'</span>,<span class="string">''</span>,x))</span><br><span class="line">df = pd.merge(df1, df2, how=<span class="string">'left'</span>,on=<span class="string">'Id'</span>)</span><br></pre></td></tr></table></figure><h2 id="爬取結果"><a class="header-anchor" href="#爬取結果">¶</a>爬取結果</h2><p>這次我們總共取回 978個產品，與 45 個產品特徵，一下簡單挑選重要的特徵來說明</p><ul><li>Id： 產品ID，在產品ID前面加上’<a href="https://24h.pchome.com.tw/prod/" target="_blank" rel="noopener">https://24h.pchome.com.tw/prod/</a>’，即是產品在Pchome上的網址<blockquote><p>範例：<a href="https://24h.pchome.com.tw/prod/DHAFHQ-A900AMA95" target="_blank" rel="noopener">https://24h.pchome.com.tw/prod/DHAFHQ-A900AMA95</a></p></blockquote></li><li>Name：產品名稱</li><li>Price：產品價格，欄位中包含原價和特價兩種價格，在使用時需要再從中解析資料。</li><li>Stmt：產品描述</li><li>Equip：配備</li><li>Remark：備註</li><li>Liability：保固資訊</li><li>Kword：關鍵字</li><li>Slogan：標語</li><li>…</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.info()</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3fp8RFalaWpXoV1H80c0p_89j-Y_7en4rbxfVj98wktEqHoIoC70PouzTRTdCAiQiMR0LiML63oimhc1OmdbJtyjxZrFkWz7kqeWAHJuFZI8u2gzasR5zo7KGXVoICBPO1WvaphjV9iXu_1mfmMTs3p=w523-h893-no?authuser=1" alt=""><br><img src="https://lh3.googleusercontent.com/pw/ACtC-3d5_qxesWknTpPEViGZTdvuzCzp38m-jW-SB4eMAs349PR36n8kavxEZmqjkh6UUTz8mVhlR6z2fBPqsSof3REFz-e4FB6hO38tbaOhxKxO9fc0_madAb5BV_4BrQaGwDOsEP-w16Tp7qG0_t1cysRs=w1026-h381-no?authuser=1" alt=""></p><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.to_excel(<span class="string">'./Pchome_NB.xlsx'</span>)</span><br></pre></td></tr></table></figure><h2 id="附件"><a class="header-anchor" href="#附件">¶</a>附件</h2><ul><li><a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/PChome/PChome.ipynb" target="_blank" rel="noopener">Pchome爬蟲程式.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> Crawler </tag>
            
            <tag> PChome </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>博客來_每字美句網路爬蟲</title>
      <link href="/2020/06/11/Crawler-everylettermatters/"/>
      <url>/2020/06/11/Crawler-everylettermatters/</url>
      <content type="html"><![CDATA[<p>「真正重要的東西，只用眼睛是看不見的。」</p><a id="more"></a><p>別懷疑，這句話不是我說的😂😂<br><img src="https://lh3.googleusercontent.com/pw/ACtC-3dqUmF_w3AiIbH2xIBCfoVt_pUfJb_nvvCpUMnwBsQ7QlGD_POsSpWr24cAG44IOI7p29Dxzn1xlgtwv3YylmFEmEiYq5dvkElB6eo_u9Mqjgpfm_A30KgIMoe6KAG30_Z1dMH8dTKLptnDE4Fo-4P4=w981-h908-no?authuser=1" alt=""></p><p>在網路上意外發現了 <a href="https://activity.books.com.tw/everylettermatters/sentence/most" target="_blank" rel="noopener">博客來每字美句</a> 的網站，網站從書籍中摘錄了許多美麗的句子，因為想要留下這些資料就順手寫了一個簡單的爬蟲程式，有需要的人可以參考使用：)</p><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3c92p4d8Isv1JOlMi8UNcxSeKJh_Uj3lVcQvpJkXX7SotsLv2gCnSp1ReEQWSSzeLaqZVn39IrNr6VMu1qceP2OY4dBKUzqDJGOQUub1c-EsRH83eNaHxRG7KQMm7jdmaW6leY1gIp6OCJW5l5-Snkq=w1298-h422-no?authuser=1" alt=""></p><p>最後附上完整的語法提供有興趣的人參考: <a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/everylettermatters/everylettermatters.ipynb" target="_blank" rel="noopener">everylettermatters.ipynb</a></p>]]></content>
      
      
        <tags>
            
            <tag> Web Crawler </tag>
            
            <tag> 博客來 </tag>
            
            <tag> 每字美句 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>台灣博碩士論文網爬蟲v2</title>
      <link href="/2020/06/07/Crawler-ndltd2/"/>
      <url>/2020/06/07/Crawler-ndltd2/</url>
      <content type="html"><![CDATA[<p>前幾天記錄了一篇完全透過 Selenium 來爬博碩士論文網的文章 <a href="https://tlyu0419.github.io/2020/06/07/Crawler-ndltd/">台灣博碩士論文網爬蟲</a>，但有沒有辦法透過 requests 更快速的完成呢?<br>這篇簡單記錄了如何透過 request 的 post來保存當前的 Session，再藉由這個 Session 的狀態來 get 我們需要的資料，另方面也優化部分的程式碼，讓程式變得更簡潔些!</p><a id="more"></a><p>如同前一篇文章所述的內容，博碩士論文網會記錄你的 Session 資訊，因此當我們把連結的網址給別人時，別人並沒有辦法看到我們轉貼的文章，只會被重新導回首頁 Orz…</p><p>同樣的邏輯，我們如果只是單純的 get 特定的網址也沒辦法取得需要的資訊，所以我們要先開一個Session，並 post 查詢的參數到對方的伺服器，讓對方記得我們，然後再用這個 Session 去 get 我們需要的資料。</p><p>怎麼做呢?其實並不能，我們首先觀察瀏覽器 post 過去的資料，並且把這些資料透過 python 送到對方的伺服器<br><img src="https://lh3.googleusercontent.com/pw/ACtC-3fyIiqAg5sBZBbaDujlicUI1JLLEkNO55r08oQOvBMbC4bsbNo1nIAu5Zan5bzhWbo4b9hqso3m4acJjRww4li7CsXIRpt2l3bmtJUNxSZil-wCFoYdFAPuXg2ePVQ_oRLbN-jwktgZpCuWrMoFahHX=w622-h491-no?authuser=1" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">payload = &#123;<span class="string">'qs0'</span>: <span class="string">'"博士".ty and ("社會服務學門" or "社會及行為科學學門").sglv1'</span>,</span><br><span class="line">                   <span class="string">'qf0'</span>: <span class="string">'_hist_'</span>,</span><br><span class="line">                   <span class="string">'gs32search.x'</span>: <span class="string">'27'</span>,</span><br><span class="line">                   <span class="string">'gs32search.y'</span>: <span class="string">'9'</span>,</span><br><span class="line">                   <span class="string">'displayonerecdisable'</span>: <span class="string">'1'</span>,</span><br><span class="line">                   <span class="string">'dbcode'</span>: <span class="string">'nclcdr'</span>,</span><br><span class="line">                   <span class="string">'action'</span>:<span class="string">''</span>,</span><br><span class="line">                   <span class="string">'op'</span>:<span class="string">''</span>,</span><br><span class="line">                   <span class="string">'h'</span>:<span class="string">''</span>,</span><br><span class="line">                   <span class="string">'histlist'</span>:<span class="string">''</span>,</span><br><span class="line">                   <span class="string">'opt'</span>: <span class="string">'m'</span>,</span><br><span class="line">                   <span class="string">'_status_'</span>: <span class="string">'search__v2'</span>&#125;</span><br><span class="line">        </span><br><span class="line">rs = requests.session()</span><br><span class="line">res = rs.post(<span class="string">'https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=&#123;&#125;/search'</span>.format(<span class="string">'fuy93N'</span>),data=payload, headers=headers)</span><br></pre></td></tr></table></figure><p>接著我們就可以從這個 Session 開始request資料囉! 後面在整理資料的流程也有做一些優化，但就不在這邊詳細說明了，有需要的人可以自行檢視程式碼。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res2 = rs.get(<span class="string">'https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=&#123;&#125;/record?r1=&#123;&#125;&amp;h1=1'</span>.format(cookie, r1))</span><br></pre></td></tr></table></figure><p>最後大家會問說，那麼 Cookie 要怎麼來?超過查詢數量的限制怎麼辦?<br>這時候我們與其花時間去研究/破解 Cookie 的生成方式，不如直接開個 Selenium 直接取結果會更有效益!<br>畢竟我們最想要節省的時間是中間在抓上千、萬篇論文資料時，頁面切換的時間，語法如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(<span class="string">'https://ndltd.ncl.edu.tw/'</span>)</span><br><span class="line">sleep(<span class="number">2</span>)</span><br><span class="line">driver.find_element_by_xpath(<span class="string">'//a[@title="指令查詢"]'</span>).click()</span><br><span class="line">cookie = re.findall(<span class="string">r'ccd=(.*?)/'</span>, driver.current_url)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>最後附上完整的語法提供有興趣的人參考: <a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/NDLTD/%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%B6%B2v2.ipynb" target="_blank" rel="noopener">博碩士論文網v2.ipynb</a></p>]]></content>
      
      
        <tags>
            
            <tag> Web Crawler </tag>
            
            <tag> 博碩士論文 </tag>
            
            <tag> NDLTD </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>台灣博碩士論文網爬蟲</title>
      <link href="/2020/06/07/Crawler-ndltd/"/>
      <url>/2020/06/07/Crawler-ndltd/</url>
      <content type="html"><![CDATA[<p>因為想要做學術圈的社群網絡分析，就順手寫了<a href="https://ndltd.ncl.edu.tw/" target="_blank" rel="noopener">博碩士論文網</a>的爬蟲程式，以下我簡要記錄了在爬這個網站時遇到的問題與解決方式，並於文末附上程式，供有需要的人參考</p><a id="more"></a><p>當我們嘗試在<a href="https://ndltd.ncl.edu.tw/" target="_blank" rel="noopener">博碩士論文網</a>輸入關鍵字並送出查詢後會的看到以下的畫面，在這個畫面中，有以下幾個值得注意的事情：</p><ol><li>網址列：網址中有個ccd的參數，這是 cookie 資訊，不同的人、不同的時間進來都會不一樣，而我們把這個網址複製給其他人時，也會重新被導向首頁，並產生新的 Cookie。</li><li>檢索策略：這裡有許多種查詢方式，其中簡易檢索的查詢方式有許多限制，如果我們想要查詢特定學門的所有論文，要採用指令檢索的方式查論文。</li><li>查詢結果：查詢出的論文網址是專屬於這個 Cookie 使用的網址，複製網址給其他人並沒有辦法使用，會重新導向首頁。在這裡查詢出的結果如下，觀察後會發現當中有 cookie(ccd) 與 頁數(r1)的資訊。<ul><li><a href="https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=fuy93N/record?r1=1&amp;h1=1" target="_blank" rel="noopener">https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=fuy93N/record?r1=1&amp;h1=1</a></li><li><a href="https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=fuy93N/record?r1=2&amp;h1=1" target="_blank" rel="noopener">https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=fuy93N/record?r1=2&amp;h1=1</a></li><li><a href="https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=fuy93N/record?r1=3&amp;h1=1" target="_blank" rel="noopener">https://ndltd.ncl.edu.tw/cgi-bin/gs32/gsweb.cgi/ccd=fuy93N/record?r1=3&amp;h1=1</a></li><li>…</li></ul></li></ol><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3fxslKKYVGSGaW6Hez_IUFdsCBLL8B4RkLW0_cmqujo5pt0CqD_9V2fBpAZ7ZmT2rtJtzYMCLTOzbZ824MDF22RIsSFtGREKbIhiiYmORXkfIShD6DvE938KWO83Xn2RZ3jx9QmzXfBUZPLRDLciujE=w851-h868-no?authuser=1" alt=""></p><p>另外我們如果觀察網頁的背後送的 request 中有哪些參數的話，我們會發現裡面的參數相當複雜，裡面有查詢的關鍵字，與一些不知名的參數<br><img src="https://lh3.googleusercontent.com/pw/ACtC-3fyIiqAg5sBZBbaDujlicUI1JLLEkNO55r08oQOvBMbC4bsbNo1nIAu5Zan5bzhWbo4b9hqso3m4acJjRww4li7CsXIRpt2l3bmtJUNxSZil-wCFoYdFAPuXg2ePVQ_oRLbN-jwktgZpCuWrMoFahHX=w622-h491-no?authuser=1" alt=""></p><p>考量我們要查詢的資料量並不大，而且查詢的參數也相當複雜，甚至還有加入Cookie的反爬蟲機制，這時候我們就直接選擇使用Selenium來解決這次的任務!</p><p>詳細的語法可以參考<a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/NDLTD/%E5%8D%9A%E7%A2%A9%E5%A3%AB%E8%AB%96%E6%96%87%E7%B6%B2.ipynb" target="_blank" rel="noopener">博碩士論文網.ipynb</a>，執行後可以得到以下資訊：</p><ul><li>研究生</li><li>研究生(外文)</li><li>論文名稱</li><li>論文名稱(外文)</li><li>指導教授</li><li>指導教授(外文)</li><li>口試委員</li><li>口試委員(外文)</li><li>學位類別</li><li>校院名稱</li><li>系所名稱</li><li>學門</li><li>學類</li><li>論文出版年</li><li>畢業學年度</li><li>語文別</li><li>論文頁數</li><li>中文關鍵詞</li><li>外文關鍵詞</li><li>相關次數</li><li>點閱</li><li>下載</li><li>書目收藏</li><li>摘要</li><li>引用</li><li>連結網址</li></ul><p><img src="https://lh3.googleusercontent.com/pw/ACtC-3f1IC3kbVE6PUtVpsRHHbYNMs7QGHAEPuzgI-DfCyWXxj99WK8T4bXEv3R35hgkJSc-VxCSneD4HvtA-wTkreLry52d7HB7bdA343cB5CjAngCcOOb69C9w3BbTjbXAr6qmrctoOnI5DUlgS3-X1YE3=w1481-h800-no?authuser=1" alt=""></p><p>註：最後要提醒博碩士論文網還有一個反爬蟲機制，一個 Cookie 查詢約 500 篇論文後就會被鎖Session，這時候我們需要關閉 Selenium 的網頁，並重新開啟網頁取得新的 Session &amp; Cookie 繼續爬蟲任務。因此可以寫個簡單的 try-except 函數，當網頁請求失敗時就自動啟動!</p>]]></content>
      
      
        <tags>
            
            <tag> Web Crawler </tag>
            
            <tag> 博碩士論文 </tag>
            
            <tag> NDLTD </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>文本分類大亂鬥_文本特徵處理與機器學習模型綜合分析</title>
      <link href="/2020/04/04/Text-Classification/"/>
      <url>/2020/04/04/Text-Classification/</url>
      <content type="html"><![CDATA[<p>當我們在進行自然語言處理(NLP)中的文本分類任務時，我們常常會看到文本特徵有相當多種的處理方式，如詞頻、TF-IDF、word2vec…；而在處理完特徵後的建模階段也有相當多的機器學習與深度學習的演算法。你是不是也跟我一樣好奇這些文本特徵處理與機器學習模型各自組合在一起的時候會有怎樣的結果呢?</p><p>這篇文章將嘗試分析 5 萬篇 <a href="https://udn.com/news/index" target="_blank" rel="noopener">聯合新聞網站</a> 的新聞，利用新聞的內文來預測新聞的類型(共 10分類)，在此感謝 <a href="https://udn.com/news/index" target="_blank" rel="noopener">聯合新聞網站</a> 提供這些豐富的資料!</p><a id="more"></a><p>我們直接先來看測試的結果吧!在這張圖中的 X 軸是在測試資料上的準確度，Y 軸是模型的訓練時間。如果點越接近左下，表示模型的準確度跟訓練時間都很低，如果越接近右上則表示準確度和訓練時間都很高。而如果接近左上，表示模型訓練很久但是模型並不准，接近右下則表示訓練速度很快而且準確度很高。<br><img src="https://lh3.googleusercontent.com/Tn37TAEzkC9SspiUXXx7ohUpi1DpUwfDW-3dfuRee4T39AZjFm6PAGPOo7Zsf4bEGcrFoXpqRxMHJXXsEY5yOrQobHPx-4ps9_2RY_w--u8xO8GHSEKeZHV1BnxoUNr2CMKzgEfBBbv-5Ihu4vNZOz4XGukSTtmDpoWbfap5MKKlFQHaRmcqA57-Bl4Ze5uPoTWzlv-_bbmrcqQDKa5Es6J329Y6kLcBtEjUp59LEfoQYzAKM0sb6JeyaEE1H3qaLxRgyhMa6aqRkDGe_brOraHhMqT6fKBoXLAxtCtiocKanbCd3qR0AWdBgTIUuE_Sc5NbM3q4H1jFnK4B2CckaTsWafQDZEquzi1NwvaX0088BXKRzZ7rDDtk7RmCCDH0gYdG6U908B5F5IsnYgul9DteN4q80MDhLRIZffT7VcIbmMTK7IbteGcKucLp7mx-dWrk2t2UT0YoSl2-x5NEHRyIVHrIq3UNC5X_PLzmX_FPaP3ERR3uNWb1Erj4Zs6hqOz9QIEr7IbXr7vAuNHV02Kfn6jTTCOEcDisruFiI10VYn9SQ1haTDSYNm3fl1j910SSQ2LN-oHceV4K93Jrp_IGD3FfQwbhu2lwHatz4u_-3-VzM3giZO9r90x-xZsiQoHeDnUAm-tzo82zbEqYEzcp_wMEh7Nbho0ZqhUr-wxc9iDkyjXK5WccdGiP0KmJfKBL_SAu6DjYpDvooL6K56JqDWxr2ltjBMd1iPk1o75btqQxm2re9nQ=w932-h929-no" alt=""></p><h1>載入使用套件</h1><p>TensorFlow 近期推出 2.0 的版本了，如果要使用舊版的 TensorFlow 需要輸入以下指令指定 1.x 的版本</p><p>因為這篇文章比較了相當多種模型，因此在這裡使用了相當多的套件，直接全部載入即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">%tensorflow_version <span class="number">1.</span>x</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score, GridSearchCV, train_test_split, cross_validate, RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB  </span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression </span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  </span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier, RandomForestClassifier </span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras <span class="keyword">import</span> layers, models, optimizers</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Conv1D, MaxPooling1D, Embedding</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.preprocessing <span class="keyword">import</span> text, sequence</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Dense, Input, Flatten, Dropout, LSTM, BatchNormalization</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.python <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.callbacks <span class="keyword">import</span> EarlyStopping</span><br></pre></td></tr></table></figure><h1>讀取資料</h1><p>如開頭所述，這次使用的資料是來自聯合新聞網站的 5萬則新聞，<a href="https://drive.google.com/file/d/1VlBVVcWD4n3_CrOYfw4jm8bbSyeOXmRc/view?usp=sharing" target="_blank" rel="noopener">點我下載udn新聞</a>，這份資料中包含了政治、國際焦點、全球財經…等 10 個類型的新聞，每個類型下各有5000則。為了簡化分析的流程，我直接先對標題與新聞內容進行斷詞。在資料的讀取方式與各個欄位的說明如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 這邊記得替換成檔案的路徑</span></span><br><span class="line">df = pd.read_pickle(<span class="string">'/content/drive/.../undnews.pickle'</span>)</span><br></pre></td></tr></table></figure><ul><li>TITLE：新聞標題</li><li>CATE：新聞類型(大類)</li><li>SUB：新聞類型(小類)</li><li>CONTENT：新聞內文</li><li>KEYWORDS：新聞關鍵詞</li><li>SUB2：對SUB欄位做的 Lable Encoding，後續在建模時使用</li><li>TITLE_SEG：針對 TITLE 欄位做的斷詞結果</li><li>CONTENT_SEG：針對 CONTENT 欄位做的斷詞結果</li></ul><p>讀取完資料後就可以切分訓練/測試資料囉</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將資料中切出 20% 作為測試資料</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(df[<span class="string">'CONTENT_SEG'</span>], df[<span class="string">'SUB2'</span>], test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)</span><br></pre></td></tr></table></figure><h1>文本特徵處理</h1><ul><li><p>CountVector：計算文本中每個詞出現的次數，整理出來的維度數是文本中所有詞的數量</p><ul><li>這邊執行完會看到 counts_train 是40000 * 49471的矩陣。</li><li>CountVectorizer 函數中有的 min_df 的函數，可以過濾掉出現次數太少的詞，幫助我們排除到許多雜訊。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">count_vect = CountVectorizer(analyzer=<span class="string">'word'</span>, token_pattern=<span class="string">r'\w&#123;1,&#125;'</span>, min_df=<span class="number">10</span>)</span><br><span class="line">count_vect.fit(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">counts_train = CountVectorizer(vocabulary=count_vect.vocabulary_).fit_transform(X_train)</span><br><span class="line">counts_test = CountVectorizer(vocabulary=count_vect.vocabulary_).fit_transform(X_test)</span><br><span class="line">counts_train</span><br></pre></td></tr></table></figure></li><li><p>TFIDFVector：在 CountVector 的基礎上，進一步考量文本的長度資訊(TF)與每個詞在多少文本中出現的狀況(IDF)的分數。</p><ul><li>這邊執行完會看到 tfidf_train 是40000 * 49471的矩陣。</li><li>TfidfVectorizer 函數中同樣有 min_df 的函數，可以過濾掉出現次數太少的詞，幫助我們排除到許多雜訊。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tfidf_vect = TfidfVectorizer(analyzer=<span class="string">'word'</span>, token_pattern=<span class="string">r'\w&#123;1,&#125;'</span>, min_df=<span class="number">10</span>)</span><br><span class="line">tfidf_vect.fit(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">tfidf_train = TfidfVectorizer(vocabulary=tfidf_vect.vocabulary_).fit_transform(X_train)</span><br><span class="line">tfidf_test = TfidfVectorizer(vocabulary=tfidf_vect.vocabulary_).fit_transform(X_test)</span><br><span class="line">tfidf_train</span><br></pre></td></tr></table></figure></li><li><p>word2vec：詞向量是用一個多維度的向量來表示詞意的方法，用這個方法可以解決 CountVector/TFIDFVector 在處理文本特徵的資料時出現非常龐大且稀疏的矩陣的問題。詞向量的取得方式有以下兩種</p><ul><li><p>運用我們這裡的資料自行訓練詞向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">program = os.path.basename(sys.argv[<span class="number">0</span>])</span><br><span class="line">logger = logging.getLogger(program)</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br><span class="line">logger.info(<span class="string">"running %s"</span> % <span class="string">' '</span>.join(sys.argv))</span><br><span class="line">w2v_model = Word2Vec(df[<span class="string">'CONTENT_SEG'</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>,<span class="number">-1</span>)), <span class="comment"># input要是list不是str</span></span><br><span class="line">                     min_count=<span class="number">10</span>,</span><br><span class="line">                     size=<span class="number">200</span>,</span><br><span class="line">                     workers=multiprocessing.cpu_count())  <span class="comment"># 訓練skip-gram模型</span></span><br></pre></td></tr></table></figure><p>訓練完成後我們就可以簡單將文本中每個詞的向量加總後平均，藉以表示這個文本的意義。由於我們的新聞數量較多，這裡的運算會需要 1小時左右。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AvgVector</span><span class="params">(w2v_model, sentence)</span>:</span></span><br><span class="line">vec = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sentence.split(<span class="string">' '</span>, <span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">in</span> w2v_model.wv.index2word:</span><br><span class="line">        vec.append(w2v_model[i])</span><br><span class="line">vector = np.mean(vec, axis=<span class="number">0</span>)</span><br><span class="line">vector = pd.Series(vector)</span><br><span class="line"><span class="keyword">return</span> vector</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">w2v_train = X_train.apply(<span class="keyword">lambda</span> x: AvgVector(w2v_model, x))</span><br><span class="line">w2v_test = X_test.apply(<span class="keyword">lambda</span> x: AvgVector(w2v_model, x))</span><br><span class="line">w2v_train.shape</span><br></pre></td></tr></table></figure></li><li><p>借用外部大量的文本訓練而得的詞向量</p><p>這是我抓取<a href="https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91" target="_blank" rel="noopener">維基百科</a>的33萬則頁面後，訓練出的詞向量，可以直接下載這 3個檔案放在同一個資料夾後讀取model檔即可使用</p><ul><li><a href="https://drive.google.com/file/d/1MY8zMmiGTV5yW1c2-_Yu1xqdZ35I4TU9/view?usp=sharing" target="_blank" rel="noopener">wiki.zh.text.model</a></li><li><a href="https://drive.google.com/file/d/1A4D1-2m_0NGaFTgTd7WMZTXkWRZ-Gg9R/view?usp=sharing" target="_blank" rel="noopener">wiki.zh.text.model.trainables.syn1neg.npy</a></li><li><a href="https://drive.google.com/file/d/1x83QuJ9TD0vN5c-NPVQC70dRKoDJRLJG/view?usp=sharing" target="_blank" rel="noopener">wiki.zh.text.model.wv.vectors.npy</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接讀取詞向量的模型即可</span></span><br><span class="line">pretrain_w2v_model = Word2Vec.load(<span class="string">'/content/drive/Shared drives/Colab Notebooks/NLP/202003_機器學習大亂鬥_新聞文本分類/wordvec_wiki/wiki.zh.text.model'</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1>機器學習模型</h1><ul><li><p>Logistic Regression</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/" target="_blank" rel="noopener">Simple Guide to Logistic Regression in R and Python</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = LogisticRegression(max_iter=<span class="number">1000</span>, n_jobs=<span class="number">-1</span>)   </span><br><span class="line">clf.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on Counts feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(counts_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(counts_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>TFIDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = LogisticRegression(max_iter=<span class="number">1000</span>, n_jobs=<span class="number">-1</span>)   </span><br><span class="line">clf.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on TFIDF feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(tfidf_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(tfidf_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>平均詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = LogisticRegression(max_iter=<span class="number">1000</span>, n_jobs=<span class="number">-1</span>)   </span><br><span class="line">clf.fit(w2v_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on AvgVector feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(w2v_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(w2v_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Naive Bayes</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/" target="_blank" rel="noopener">6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">param_grid = &#123;<span class="string">'alpha'</span>:[<span class="number">1.4</span>, <span class="number">1.2</span>, <span class="number">1</span>, <span class="number">0.8</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">estimators = GridSearchCV(estimator = MultinomialNB(),</span><br><span class="line">                      param_grid = param_grid,</span><br><span class="line">                      n_jobs = <span class="number">-1</span>,</span><br><span class="line">                      cv = <span class="number">5</span>)</span><br><span class="line">estimators.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on CV result '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Best Score: '</span>, estimators.best_score_)</span><br><span class="line">print(<span class="string">'Best Params: '</span>, estimators.best_params_)</span><br><span class="line">clf = MultinomialNB(alpha = estimators.best_params_[<span class="string">'alpha'</span>])   </span><br><span class="line">clf.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on Counts feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(counts_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(counts_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>TFIDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">param_grid = &#123;<span class="string">'alpha'</span>:[<span class="number">1.4</span>, <span class="number">1.2</span>, <span class="number">1</span>, <span class="number">0.8</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">estimators = GridSearchCV(estimator = MultinomialNB(),</span><br><span class="line">                      param_grid = param_grid,</span><br><span class="line">                      n_jobs = <span class="number">-1</span>,</span><br><span class="line">                      cv = <span class="number">5</span>)</span><br><span class="line">estimators.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on CV result '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Best Score: '</span>, estimators.best_score_)</span><br><span class="line">print(<span class="string">'Best Params: '</span>, estimators.best_params_)</span><br><span class="line">%%time</span><br><span class="line">clf = MultinomialNB(alpha = estimators.best_params_[<span class="string">'alpha'</span>])   </span><br><span class="line">clf.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on TFIDF feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(tfidf_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(tfidf_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>平均詞向量特徵</p><p>因為詞向量的結果會有負值，而貝氏分類器不接受，因此這裡多做了一個 MinMax 的轉換，將資料轉換到 0-1 之間。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">scaler.fit(w2v_train)</span><br><span class="line"></span><br><span class="line">w2v_train_mm = scaler.transform(w2v_train)</span><br><span class="line">w2v_test_mm = scaler.transform(w2v_test)</span><br><span class="line"></span><br><span class="line">param_grid = &#123;<span class="string">'alpha'</span>:[<span class="number">1.4</span>, <span class="number">1.2</span>, <span class="number">1</span>, <span class="number">0.8</span>, <span class="number">0.6</span>]&#125;</span><br><span class="line">estimators = GridSearchCV(estimator = MultinomialNB(),</span><br><span class="line">                        param_grid = param_grid,</span><br><span class="line">                        n_jobs = <span class="number">-1</span>,</span><br><span class="line">                        cv = <span class="number">5</span>)</span><br><span class="line">estimators.fit(w2v_train_mm, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on CV result '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Best Score: '</span>, estimators.best_score_)</span><br><span class="line">print(<span class="string">'Best Params: '</span>, estimators.best_params_)</span><br><span class="line">%%time</span><br><span class="line">clf = MultinomialNB(alpha = estimators.best_params_[<span class="string">'alpha'</span>])   </span><br><span class="line">clf.fit(w2v_train_mm, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on AvgVector feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(w2v_train_mm)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(w2v_test_mm)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>SVM</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" target="_blank" rel="noopener">Understanding Support Vector Machine(SVM) algorithm from examples (along with code)</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on Counts feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(counts_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(counts_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>TFIDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on TFIDF feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(tfidf_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(tfidf_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>平均詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(w2v_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on AvgVector feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(w2v_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(w2v_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>KNN</p><p>參考資料：<a href="https://towardsdatascience.com/k-nearest-neighbors-knn-algorithm-bd375d14eec7" target="_blank" rel="noopener">K-Nearest Neighbors (KNN) Algorithm</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">param_grid = &#123;<span class="string">'n_neighbors'</span>:list(range(<span class="number">1</span>,<span class="number">9</span>))&#125;</span><br><span class="line">estimators = GridSearchCV(estimator = KNeighborsClassifier(),</span><br><span class="line">                        param_grid = param_grid,</span><br><span class="line">                        n_jobs = <span class="number">-1</span>,</span><br><span class="line">                        cv = <span class="number">5</span>)</span><br><span class="line">estimators.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on CV result '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Best Score: '</span>, estimators.best_score_)</span><br><span class="line">print(<span class="string">'Best Params: '</span>, estimators.best_params_)</span><br><span class="line">%%time</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors = estimators.best_params_[<span class="string">'n_neighbors'</span>])   </span><br><span class="line">clf.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on Counts feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(counts_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(counts_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>TFIDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">param_grid = &#123;<span class="string">'n_neighbors'</span>:list(range(<span class="number">1</span>,<span class="number">9</span>))&#125;</span><br><span class="line">estimators = GridSearchCV(estimator = KNeighborsClassifier(),</span><br><span class="line">                        param_grid = param_grid,</span><br><span class="line">                        n_jobs = <span class="number">-1</span>,</span><br><span class="line">                        cv = <span class="number">5</span>)</span><br><span class="line">estimators.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on CV result '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Best Score: '</span>, estimators.best_score_)</span><br><span class="line">print(<span class="string">'Best Params: '</span>, estimators.best_params_)</span><br><span class="line">%%time</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors = estimators.best_params_[<span class="string">'n_neighbors'</span>], n_jobs=<span class="number">-1</span>)   </span><br><span class="line">clf.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on TFIDF feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(tfidf_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(tfidf_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>平均詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">param_grid = &#123;<span class="string">'n_neighbors'</span>:list(range(<span class="number">1</span>,<span class="number">9</span>))&#125;</span><br><span class="line">estimators = GridSearchCV(estimator = KNeighborsClassifier(),</span><br><span class="line">                        param_grid = param_grid,</span><br><span class="line">                        n_jobs = <span class="number">-1</span>,</span><br><span class="line">                        cv = <span class="number">5</span>)</span><br><span class="line">estimators.fit(w2v_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on CV result '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Best Score: '</span>, estimators.best_score_)</span><br><span class="line">print(<span class="string">'Best Params: '</span>, estimators.best_params_)</span><br><span class="line">%%time</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors = estimators.best_params_[<span class="string">'n_neighbors'</span>], n_jobs=<span class="number">-1</span>)   </span><br><span class="line">clf.fit(w2v_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on AvgVector feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(w2v_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(w2v_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>RandomForest</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="noopener">Introduction to Random forest – Simplified</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = RandomForestClassifier(n_estimators = <span class="number">500</span>, max_features = <span class="string">'sqrt'</span>, n_jobs=<span class="number">-1</span>, random_state = <span class="number">10</span>)  </span><br><span class="line">clf.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on Counts feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(counts_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(counts_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>TFIDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = RandomForestClassifier(n_estimators = <span class="number">500</span>, max_features = <span class="string">'sqrt'</span>, n_jobs=<span class="number">-1</span>, random_state = <span class="number">10</span>)  </span><br><span class="line">clf.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on TFIDF feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(tfidf_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(tfidf_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>平均詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = RandomForestClassifier(n_estimators = <span class="number">500</span>, max_features = <span class="string">'sqrt'</span>, n_jobs=<span class="number">-1</span>, random_state = <span class="number">10</span>)  </span><br><span class="line">clf.fit(w2v_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on AvgVector feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(w2v_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(w2v_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>XGBoost</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/" target="_blank" rel="noopener">How to use XGBoost algorithm in R in easy steps</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = xgb.XGBClassifier(n_estimators=<span class="number">500</span>, objective=<span class="string">'multi:softmax'</span>, n_jobs=<span class="number">-1</span>, silent=<span class="keyword">False</span>)</span><br><span class="line">clf.fit(counts_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on Counts feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(counts_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(counts_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>TFIDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = xgb.XGBClassifier(n_estimators=<span class="number">500</span>, objective=<span class="string">'multi:softmax'</span>, n_jobs=<span class="number">-1</span>, silent=<span class="keyword">False</span>)</span><br><span class="line">clf.fit(tfidf_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on TFIDF feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(tfidf_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(tfidf_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li><li><p>平均詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">clf = xgb.XGBClassifier(n_estimators=<span class="number">500</span>, objective=<span class="string">'multi:softmax'</span>, n_jobs=<span class="number">-1</span>, silent=<span class="keyword">False</span>)</span><br><span class="line">clf.fit(w2v_train, y_train)</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">40</span>, <span class="string">' Score on AvgVector feature '</span>, <span class="string">'='</span>*<span class="number">40</span>)</span><br><span class="line">print(<span class="string">'Score on Train: '</span>, metrics.accuracy_score(y_train, clf.predict(w2v_train)))</span><br><span class="line">print(<span class="string">'Score on Test: '</span>, metrics.accuracy_score(y_test, clf.predict(w2v_test)))</span><br><span class="line">print(<span class="string">'='</span>*<span class="number">100</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1>深度學習模型</h1><p>這裡開始的程式碼會變得比較複雜，並且執行時會需要使用到 GPU 來加速，如果沒有 GPU 或不會設定的人可以直接使用 Google 的 Colab。</p><ul><li><p>MLP</p><p>參考資料：<a href="https://towardsdatascience.com/machine-learning-101-artificial-neural-networks-3-46ccb04cba30" target="_blank" rel="noopener">Machine Learning 101 — Artificial Neural Networks</a></p><ul><li><p>詞頻特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 設定超參數</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">MOMENTUM = <span class="number">0.95</span></span><br><span class="line"><span class="comment"># 建立資料格式</span></span><br><span class="line"><span class="comment"># 考量時間與記憶體容量，僅保留數量最多的1萬個詞</span></span><br><span class="line">tokenizer = text.Tokenizer(num_words=<span class="number">10000</span>) </span><br><span class="line">tokenizer.fit_on_texts(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">counts_train = tokenizer.texts_to_sequences(X_train) </span><br><span class="line">counts_train = tokenizer.sequences_to_matrix(counts_train, mode=<span class="string">'freq'</span>)</span><br><span class="line">counts_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">counts_test = tokenizer.sequences_to_matrix(counts_test, mode=<span class="string">'freq'</span>)</span><br><span class="line">print(<span class="string">'Shape of counts_train tensor:'</span>, counts_train.shape)</span><br><span class="line">print(<span class="string">'Shape of counts_test tensor:'</span>, counts_test.shape)</span><br><span class="line"></span><br><span class="line">y_train_dummy = to_categorical(np.asarray(y_train))</span><br><span class="line">y_test_dummy = to_categorical(np.asarray(y_test))</span><br><span class="line">print(<span class="string">'Shape of y_train_dummy tensor:'</span>, y_train_dummy.shape)</span><br><span class="line">print(<span class="string">'Shape of y_test_dummy tensor:'</span>, y_test_dummy.shape)</span><br><span class="line"><span class="comment"># 搭建模型框架</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(units=<span class="number">512</span>, input_shape=(counts_train.shape[<span class="number">1</span>],), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(y_train_dummy.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># 載入 Callbacks, 並將 monitor 設定為監控 validation loss</span></span><br><span class="line">earlystop = EarlyStopping(monitor=<span class="string">"val_acc"</span>, </span><br><span class="line">                        patience=<span class="number">10</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            optimizer=optimizers.Adam(lr=LEARNING_RATE, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>),</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">%%time</span><br><span class="line">model.fit(counts_train, y_train_dummy,</span><br><span class="line">        epochs=EPOCHS, </span><br><span class="line">        validation_split=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        callbacks=[earlystop])</span><br><span class="line">model.evaluate(counts_test, y_test_dummy)</span><br></pre></td></tr></table></figure></li><li><p>TF-IDF特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 超參數設定</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">MOMENTUM = <span class="number">0.95</span></span><br><span class="line"><span class="comment"># 建立資料格式</span></span><br><span class="line"><span class="comment"># 考量時間與記憶體容量，僅保留數量最多的1萬個詞</span></span><br><span class="line">tokenizer = text.Tokenizer(num_words=<span class="number">10000</span>) </span><br><span class="line">tokenizer.fit_on_texts(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">tfidf_train = tokenizer.texts_to_sequences(X_train) </span><br><span class="line">tfidf_train = tokenizer.sequences_to_matrix(tfidf_train, mode=<span class="string">'tfidf'</span>)</span><br><span class="line">tfidf_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">tfidf_test = tokenizer.sequences_to_matrix(tfidf_test, mode=<span class="string">'tfidf'</span>)</span><br><span class="line">print(<span class="string">'Shape of tfidf_train tensor:'</span>, tfidf_train.shape)</span><br><span class="line">print(<span class="string">'Shape of tfidf_test tensor:'</span>, tfidf_test.shape)</span><br><span class="line"></span><br><span class="line">y_train_dummy = to_categorical(np.asarray(y_train))</span><br><span class="line">y_test_dummy = to_categorical(np.asarray(y_test))</span><br><span class="line">print(<span class="string">'Shape of y_train tensor:'</span>, y_train_dummy.shape)</span><br><span class="line">print(<span class="string">'Shape of y_test tensor:'</span>, y_test_dummy.shape)</span><br><span class="line"><span class="comment"># 搭建模型框架</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(units=<span class="number">512</span>, input_shape=(tfidf_train.shape[<span class="number">1</span>],), activation=<span class="string">'relu'</span>)) <span class="comment">################</span></span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(y_train_dummy.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># 載入 Callbacks, 並將 monitor 設定為監控 validation loss</span></span><br><span class="line">earlystop = EarlyStopping(monitor=<span class="string">"val_acc"</span>, </span><br><span class="line">                        patience=<span class="number">5</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            optimizer=optimizers.Adam(lr=LEARNING_RATE, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>),</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">%%time</span><br><span class="line">model.fit(tfidf_train, y_train_dummy,</span><br><span class="line">        epochs=EPOCHS, </span><br><span class="line">        validation_split=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        callbacks=[earlystop])</span><br><span class="line">model.evaluate(tfidf_test, y_test_dummy)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>CNN</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/" target="_blank" rel="noopener">Architecture of Convolutional Neural Networks (CNNs) demystified</a></p><ul><li><p>詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">MAX_SEQUENCE_LENGTH = <span class="number">300</span> <span class="comment"># 每条新闻最大长度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">200</span> <span class="comment"># 词向量空间维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 超參數設定</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">MOMENTUM = <span class="number">0.95</span></span><br><span class="line"><span class="comment"># 建立資料格式</span></span><br><span class="line">tokenizer = text.Tokenizer(num_words=<span class="number">10000</span>)</span><br><span class="line">tokenizer.fit_on_texts(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">vec_train = tokenizer.texts_to_sequences(X_train) </span><br><span class="line">vec_train = sequence.pad_sequences(vec_train, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">vec_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">vec_test = sequence.pad_sequences(vec_test, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">print(<span class="string">'Shape of vec_train tensor:'</span>, vec_train.shape)</span><br><span class="line">print(<span class="string">'Shape of vec_test tensor:'</span>, vec_test.shape)</span><br><span class="line"></span><br><span class="line">y_train_dummy = to_categorical(np.asarray(y_train))</span><br><span class="line">y_test_dummy = to_categorical(np.asarray(y_test))</span><br><span class="line">print(<span class="string">'Shape of y_train_dummy tensor:'</span>, y_train_dummy.shape)</span><br><span class="line">print(<span class="string">'Shape of y_test_dummy tensor:'</span>, y_test_dummy.shape)</span><br><span class="line"><span class="comment"># 將詞替換成對應的向量</span></span><br><span class="line">embedding_matrix = np.zeros((len(word_index) + <span class="number">1</span>, EMBEDDING_DIM))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items(): </span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> w2v_model:</span><br><span class="line">        embedding_matrix[i] = np.asarray(w2v_model[word],</span><br><span class="line">                                        dtype=<span class="string">'float32'</span>)</span><br><span class="line">embedding_layer = Embedding(len(word_index) + <span class="number">1</span>,</span><br><span class="line">                            EMBEDDING_DIM,</span><br><span class="line">                            weights=[embedding_matrix],</span><br><span class="line">                            input_length=MAX_SEQUENCE_LENGTH,</span><br><span class="line">                            trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 搭建模型框架</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(embedding_layer)</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line"></span><br><span class="line">model.add(Conv1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'valid'</span>, activation=<span class="string">'relu'</span>, strides=<span class="number">1</span>))</span><br><span class="line">model.add(MaxPooling1D(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(EMBEDDING_DIM, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(y_train_dummy.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># 載入 Callbacks, 並將 monitor 設定為監控 validation loss</span></span><br><span class="line">earlystop = EarlyStopping(monitor=<span class="string">"val_acc"</span>, </span><br><span class="line">                        patience=<span class="number">5</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            optimizer= optimizers.Adam(lr=LEARNING_RATE, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>),</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(vec_train, y_train_dummy,</span><br><span class="line">        epochs=EPOCHS, </span><br><span class="line">        validation_split=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        callbacks=[earlystop])</span><br><span class="line">model.evaluate(vec_test, y_test_dummy)</span><br></pre></td></tr></table></figure></li><li><p>預訓練的詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">MAX_SEQUENCE_LENGTH = <span class="number">300</span> <span class="comment"># 每条新闻最大长度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">400</span> <span class="comment"># 词向量空间维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 超參數設定</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">MOMENTUM = <span class="number">0.95</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立資料格式</span></span><br><span class="line">tokenizer = text.Tokenizer(num_words=<span class="number">10000</span>)</span><br><span class="line">tokenizer.fit_on_texts(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">vec_train = tokenizer.texts_to_sequences(X_train) </span><br><span class="line">vec_train = sequence.pad_sequences(vec_train, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">vec_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">vec_test = sequence.pad_sequences(vec_test, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">print(<span class="string">'Shape of vec_train tensor:'</span>, vec_train.shape)</span><br><span class="line">print(<span class="string">'Shape of vec_test tensor:'</span>, vec_test.shape)</span><br><span class="line"></span><br><span class="line">y_train_dummy = to_categorical(np.asarray(y_train))</span><br><span class="line">y_test_dummy = to_categorical(np.asarray(y_test))</span><br><span class="line">print(<span class="string">'Shape of y_train tensor:'</span>, y_train_dummy.shape)</span><br><span class="line">print(<span class="string">'Shape of y_test tensor:'</span>, y_test_dummy.shape)</span><br><span class="line"><span class="comment"># 將詞替換成對應的向量</span></span><br><span class="line">embedding_matrix = np.zeros((len(word_index) + <span class="number">1</span>, EMBEDDING_DIM))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items(): </span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> pretrain_w2v_model:</span><br><span class="line">        embedding_matrix[i] = np.asarray(pretrain_w2v_model[word],</span><br><span class="line">                                        dtype=<span class="string">'float32'</span>)</span><br><span class="line">embedding_layer = Embedding(len(word_index) + <span class="number">1</span>,</span><br><span class="line">                            EMBEDDING_DIM,</span><br><span class="line">                            weights=[embedding_matrix],</span><br><span class="line">                            input_length=MAX_SEQUENCE_LENGTH,</span><br><span class="line">                            trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 搭建模型框架</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(embedding_layer)</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line"></span><br><span class="line">model.add(Conv1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'valid'</span>, activation=<span class="string">'relu'</span>, strides=<span class="number">1</span>))</span><br><span class="line">model.add(MaxPooling1D(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(EMBEDDING_DIM, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(y_train_dummy.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># 載入 Callbacks, 並將 monitor 設定為監控 validation loss</span></span><br><span class="line">earlystop = EarlyStopping(monitor=<span class="string">"val_loss"</span>, </span><br><span class="line">                        patience=<span class="number">5</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            optimizer= optimizers.Adam(lr=LEARNING_RATE, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>),</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(vec_train, y_train_dummy,</span><br><span class="line">        epochs=EPOCHS, </span><br><span class="line">        validation_split=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        callbacks=[earlystop])</span><br><span class="line">model.evaluate(vec_test, y_test_dummy)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>RNN</p><p>參考資料：<a href="https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/" target="_blank" rel="noopener">Essentials of Deep Learning : Introduction to Long Short Term Memory</a></p><ul><li><p>詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">MAX_SEQUENCE_LENGTH = <span class="number">300</span> <span class="comment"># 每条新闻最大长度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">200</span> <span class="comment"># 词向量空间维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 超參數設定</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">MOMENTUM = <span class="number">0.95</span></span><br><span class="line"><span class="comment"># 建立資料格式</span></span><br><span class="line">tokenizer = text.Tokenizer()</span><br><span class="line">tokenizer.fit_on_texts(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">pre_vec_train = tokenizer.texts_to_sequences(X_train) </span><br><span class="line">pre_vec_train = sequence.pad_sequences(pre_vec_train, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">pre_vec_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">pre_vec_test = sequence.pad_sequences(pre_vec_test, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">print(<span class="string">'Shape of pre_vec_train tensor:'</span>, pre_vec_train.shape)</span><br><span class="line">print(<span class="string">'Shape of pre_vec_test tensor:'</span>, pre_vec_test.shape)</span><br><span class="line"></span><br><span class="line">y_train_dummy = to_categorical(np.asarray(y_train))</span><br><span class="line">y_test_dummy = to_categorical(np.asarray(y_test))</span><br><span class="line">print(<span class="string">'Shape of y_train tensor:'</span>, y_train_dummy.shape)</span><br><span class="line">print(<span class="string">'Shape of y_test tensor:'</span>, y_test_dummy.shape)</span><br><span class="line"><span class="comment"># 將詞替換成對應的向量</span></span><br><span class="line">embedding_matrix = np.zeros((len(word_index) + <span class="number">1</span>, EMBEDDING_DIM))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items(): </span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> w2v_model:</span><br><span class="line">        embedding_matrix[i] = np.asarray(w2v_model[word],</span><br><span class="line">                                        dtype=<span class="string">'float32'</span>)</span><br><span class="line">embedding_layer = Embedding(len(word_index) + <span class="number">1</span>,</span><br><span class="line">                            EMBEDDING_DIM,</span><br><span class="line">                            weights=[embedding_matrix],</span><br><span class="line">                            input_length=MAX_SEQUENCE_LENGTH,</span><br><span class="line">                            trainable=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 搭建模型框架</span></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(embedding_layer)</span><br><span class="line">model.add(LSTM(<span class="number">200</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Dense(y_train_dummy.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># 載入 Callbacks, 並將 monitor 設定為監控 validation loss</span></span><br><span class="line">earlystop = EarlyStopping(monitor=<span class="string">"val_acc"</span>, </span><br><span class="line">                        patience=<span class="number">5</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>)</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            optimizer=optimizers.Adam(lr=LEARNING_RATE, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>),</span><br><span class="line">            metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(pre_vec_train, y_train_dummy,</span><br><span class="line">        epochs=EPOCHS, </span><br><span class="line">        validation_split=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        callbacks=[earlystop]</span><br><span class="line">        )</span><br><span class="line">model.evaluate(pre_vec_test, y_test_dummy)</span><br></pre></td></tr></table></figure></li><li><p>預訓練的詞向量特徵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">MAX_SEQUENCE_LENGTH = <span class="number">300</span> <span class="comment"># 每条新闻最大长度</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">400</span> <span class="comment"># 词向量空间维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 超參數設定</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br><span class="line">EPOCHS = <span class="number">100</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">MOMENTUM = <span class="number">0.95</span></span><br><span class="line">tokenizer = text.Tokenizer()</span><br><span class="line">tokenizer.fit_on_texts(df[<span class="string">'CONTENT_SEG'</span>])</span><br><span class="line">word_index = tokenizer.word_index</span><br><span class="line">print(<span class="string">'Found %s unique tokens.'</span> % len(word_index))</span><br><span class="line"></span><br><span class="line">pre_vec_train = tokenizer.texts_to_sequences(X_train) </span><br><span class="line">pre_vec_train = sequence.pad_sequences(pre_vec_train, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">pre_vec_test = tokenizer.texts_to_sequences(X_test)</span><br><span class="line">pre_vec_test = sequence.pad_sequences(pre_vec_test, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">print(<span class="string">'Shape of pre_vec_train tensor:'</span>, pre_vec_train.shape)</span><br><span class="line">print(<span class="string">'Shape of pre_vec_test tensor:'</span>, pre_vec_test.shape)</span><br><span class="line"></span><br><span class="line">y_train_dummy = to_categorical(np.asarray(y_train))</span><br><span class="line">y_test_dummy = to_categorical(np.asarray(y_test))</span><br><span class="line">print(<span class="string">'Shape of y_train_dummy tensor:'</span>, y_train_dummy.shape)</span><br><span class="line">print(<span class="string">'Shape of y_test_dummy tensor:'</span>, y_test_dummy.shape)</span><br><span class="line"></span><br><span class="line">embedding_matrix = np.zeros((len(word_index) + <span class="number">1</span>, EMBEDDING_DIM))</span><br><span class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> word_index.items(): </span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> pretrain_w2v_model:</span><br><span class="line">        embedding_matrix[i] = np.asarray(pretrain_w2v_model[word],</span><br><span class="line">                                        dtype=<span class="string">'float32'</span>)</span><br><span class="line">embedding_layer = Embedding(len(word_index) + <span class="number">1</span>,</span><br><span class="line">                            EMBEDDING_DIM,</span><br><span class="line">                            weights=[embedding_matrix],</span><br><span class="line">                            input_length=MAX_SEQUENCE_LENGTH,</span><br><span class="line">                            trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">keras.backend.clear_session()</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(embedding_layer)</span><br><span class="line">model.add(LSTM(<span class="number">200</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.2</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(Dense(y_train_dummy.shape[<span class="number">1</span>], activation=<span class="string">'softmax'</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 載入 Callbacks, 並將 monitor 設定為監控 validation loss</span></span><br><span class="line">earlystop = EarlyStopping(monitor=<span class="string">"val_acc"</span>, </span><br><span class="line">                        patience=<span class="number">5</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">            optimizer=optimizers.Adam(lr=LEARNING_RATE, epsilon=<span class="keyword">None</span>, decay=<span class="number">0.0</span>),</span><br><span class="line">            metrics=[<span class="string">'acc'</span>])</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line">model.fit(pre_vec_train, y_train_dummy,</span><br><span class="line">        epochs=EPOCHS, </span><br><span class="line">        validation_split=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=BATCH_SIZE,</span><br><span class="line">        shuffle=<span class="keyword">True</span>,</span><br><span class="line">        callbacks=[earlystop]</span><br><span class="line">        )</span><br><span class="line">model.evaluate(pre_vec_test, y_test_dummy)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1>預測結果解析</h1><ul><li><p>從分析圖的結果來看，模型在區分政治類新聞(類型3)與總統大選觀測站類的新聞(類型8)時是比較吃力的，互相預測錯129/130個樣本。但是我們透過事後的檢視則會知道其實這兩類新聞的性質非常相近，實際在使用時可以思考是否有必要區分出這兩個類別，或著直接將兩者合併成政治類新聞即可。</p><ul><li><p>前面有將目標變數的10個類別透過 Label Encoding 轉換成數值資料，各個數值對應的類別如下：</p><ul><li>0: NBA</li><li>1: 全球財經</li><li>2: 國際焦點</li><li>3: 政治</li><li>4: 流行消費</li><li>5: 生活新聞</li><li>6: 產業綜合</li><li>7: 科技產業</li><li>8: 總統大選觀測站</li><li>9: 金融要聞</li></ul></li></ul><p><img src="https://lh3.googleusercontent.com/hQuCXfMrkfDRpZ5oE4B_qYPMgASLxpGR3XB_mXlx79bl1xeYbZWsKKVZxGR8pqbPP50MaqBvZV-RA-0DpwdMF5bcqQ707BWWMw-1jkVyqMDYWCpTcmWci8I4gxri3rvjWvF0ifZoZziZ4byAIP5wFPDc-jtmD2rNn72BI1RCqg0O1oEGpMXa_ZXixOzY10NpvyoJjUPuN1NsBu9sDVYso1ZpctdpsN__h90sEvKXj3EzGv6mjVUSD_XTdZ7NVBrjkM3-bn2-Lf3m-9lTnyebsJjovLdtENwYf2CP-RP0oIbiVRfEOiEkrvMCTwV8NyC4pF9FDKabHl48A7att1Q8rpFirIjHYil5UG46PYR8wsio6AFuq1Jtcg-PHdMHXgL4CNyjgG3_7i1CSW8Ls9fThua3cTLZpovZ7oAg8h2VaFz_kTCJ0iR-XLxKYMKPoFzaH1iBhS7lt7jhLu12gltGGXnyXo55fVQYql3lC6f8sGiMxYx9_n4_WM3bYWVik7OLnGn_9AM7kx8wl0KRTLIqCCFuTwHqMwdQO0eso13aoZ8_jphegrYa0dUkuGUKajyDt-dExnkMEE6UAfncuH0MWTeU3zA0g5yWVLpP7mkuynEYCvM8DPFSTHcvF36i5DdynMqdfrUr1WmMrOgfj6OqH5JwuIDh2cu0X4QNNEWFVUEbOxWsc0LzVwN-gCoCNhS16MWvcGTQM_3Tg8jd8TJIUo7mwQHAgxSarbANcS1Zx9CzlRhFaIGz0vU=w1115-h929-no" alt=""></p></li></ul><h1>後記</h1><ul><li>本次分析是使用新聞的內文進行建模，大多數的模型表現都有將近 85% 的準確度，這當然是因為長文本提供了較多的資訊所致，然而實務上碰到大情況大多是短文本的問題，這時候由於資訊量較少，要用來預測新聞主題的難度就會大幅增加。有興趣的人可以自行嘗試用新聞的標題進行預測新聞類型，我簡單測試時的準確度約在70%左右。</li><li>我們過去大多都是用分類的效果來選擇模型，但是我們經常忽略了訓練的時間成本，想這次因為要比較的東西太多了，我就沒有逐一的調整模型的參數，如果分類錯誤的成本不這麽高的時候，我們可以考慮使用快速準確度不低的模型來完成任務。對於想透過調參知道模型極限的人也可以再自行調參檢視效果。</li></ul><h1>附件</h1><ul><li><a href="https://drive.google.com/file/d/1VlBVVcWD4n3_CrOYfw4jm8bbSyeOXmRc/view?usp=sharing" target="_blank" rel="noopener">udn新聞資料集</a></li><li>維基百科訓練出的詞向量<ul><li><a href="https://drive.google.com/file/d/1MY8zMmiGTV5yW1c2-_Yu1xqdZ35I4TU9/view?usp=sharing" target="_blank" rel="noopener">wiki.zh.text.model</a></li><li><a href="https://drive.google.com/file/d/1A4D1-2m_0NGaFTgTd7WMZTXkWRZ-Gg9R/view?usp=sharing" target="_blank" rel="noopener">wiki.zh.text.model.trainables.syn1neg.npy</a></li><li><a href="https://drive.google.com/file/d/1x83QuJ9TD0vN5c-NPVQC70dRKoDJRLJG/view?usp=sharing" target="_blank" rel="noopener">wiki.zh.text.model.wv.vectors.npy</a></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> 文本分類 </tag>
            
            <tag> 聯合新聞 </tag>
            
            <tag> TF-IDF </tag>
            
            <tag> word2vec </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Facebook粉絲專頁爬蟲筆記</title>
      <link href="/2020/03/17/Crawl-Facebook-Pages/"/>
      <url>/2020/03/17/Crawl-Facebook-Pages/</url>
      <content type="html"><![CDATA[<p>先前曾發表過 <a href="https://tlyu0419.github.io/2019/05/01/Crawl-Facebook/">網路爬蟲_Facebook粉絲團貼文與留言</a> 的文章，可以透過 Selenium 來抓取 Facebook 粉絲專頁的貼文與留言，但由於 Selenium 的效率實在太慢了，因此這篇文章是記錄我嘗試用 request 抓取資料的筆記，並且於文末附上台灣的2020總統大選 3位候選人從 2019年 1 月至 2020 年 2 月間的貼文與留言資料~</p><a id="more"></a><h1>文章架構</h1><ul><li>使用時機</li><li>抓取貼文</li><li>抓取留言</li><li>後記</li></ul><h1>使用時機</h1><ul><li><p>現在許多網站都是屬於瀑布式(Waterfall)的網頁結構，相對於靜態式的網頁是一次就讀取完全部的資料，而瀑布式網頁則是將很多份的內容轉化為個別獨立的區塊，在滑鼠滾輪向下滾動時不斷的加載新資料，並將新資料附加在網頁的最底端。典型的網站有<a href="https://www.facebook.com/twherohan/" target="_blank" rel="noopener">Facebook</a>、<a href="https://www.104.com.tw/jobs/search/?keyword=%E8%B3%87%E6%96%99%E7%A7%91%E5%AD%B8&amp;order=1&amp;jobsource=2018indexpoc&amp;ro=0" target="_blank" rel="noopener">104人力銀行</a>。</p><p><img src="https://lh3.googleusercontent.com/cVJxdsY25nJinI9_OHsu4m4KBzKm7xXB1BQAP6bpvO7bj4buMJ0CiUrXT0J7e21Qi7Kf0vhGmqIROHISQhwZu9XH0kqoifKT8oYaCqhFP4Q83xZDfvtCb8Fc8tn6P8v9EKjeCGssai_A5lRqmSFVviH1xb50hY8gHoPPPJ_6fP3IUgNNtTgiyXtb2SGcb5Vagml-z8PYn8y5z0ZvjzqVBcIoyZqGYQ5ezzNr7ali3aV4_oOC-okBWzhhSnHVgbaSp5gtkBPjF3p8mdNdtp9ZzvJ3lgIjqLszGEjs_rdzWgXDfmSpjdFBcY4I8Rzycb9_fad5AjcDuXqasKQ_PF9LJdlSXOpU34Rva0yecpU_yYC8cAfD9hP7sKuQyq1VS0sfwcuc4gVHGoX7vbCcD3lcKQ8bI80OZaABqkeUy9cORoYaNtv_7zO7skCeCENWA9oB3UKDewUMzZrPO7j7iajXStiwPEWKNJAavPgItGMJQI_OAQ-jieIvcnQ7vA4S1VnefP1qQdD9_QpFcoR4fTaXAgaun5BuxKe9IvDBNItC-cqZ3a0xGIXKfSEhRkztM2jA5Ib66RgWgEsT34CgyUxY1W6FzdDc_FCcuJga_dnCfDOpOeaeGEHNcNP2wxrlZDqE7mIBtP5YP3dcfLPa_g01yb5QH2NOp8gq2npShQR4UTNzn36HkvMcAU5jtfOBfgHWBOivGkSIWvSH9DG9F7saq2DKul1wQYQT_IRrGvQmkITsZQqWPh7Y52k=w1118-h929-no" alt=""></p></li><li><p>以 Facebook 為例，由於貼文和留言的 ID 都有經過加密，此外也有許多複雜的參數需要輸入。當我們無論怎麼解都解不開時就只好開啟網路爬蟲的大殺器 Selenium，但由於 Selenium 是實際開啟一個瀏覽器來抓資料，因此效能並不會太高，而且網站出於使用者體驗、反爬蟲、廣告等等的目的會設計許多與用戶的互動機制，也會干擾我們的爬蟲工作。因此如果能單純的透過送出 request/post 來獲取資料，就不要透過 Selenium!</p><p><img src="https://lh3.googleusercontent.com/eiKIXh-EPDrU460W2-cyr8nPhi_ehV_c8iooJxylq_keH5Z9Ri3SM6O9lWFI7zIO8EyMm68aNu-ZuDyxoAD8CPlXc-mYlfEkxdPjKcHVChhg6U2CD0LW_m72vnLAQ5j00aO7FsEJm7I7nPXjHcMLypVxgUnbTwXFUTjeBNb5y-mf6V6sZyfK2XsUxcyi7Z4iRuCpuGyFCqVybHf-K9wt8FdLLslUNz3yy8PftjIm-NIEXmgttPcV6P_WLcyB_ZRmPBItFwP4XVS5qp2Nh5DBo3UEoz8Gqc0x44FaQgftWOY4seKQ2uQMZIg5iwI7qAjuxjMP7dVr4urYbAmxNDNTWL0QCnH_Z_J3Sr-d25FMBrwqCwJwi3xoluU5SOglEqdWq1dwnWDsuZE1QXzuhlE1-4jFHAjyY_hdYFgf1fNdd4qdZclIKZWKqTZfNyqAAcddWBxGxWdLF6gRewd2ah1BizOrF3TGOAJrI8klQ9xU5-w46d5iBHjuLbsK9vfKNjHODx1DvCHi-BANZNCJiJTWi94_aurriLFDlUPwe6vM_Y3V457-uxtVJskmJoCZpZrEKtE1WFnBKNm7UUh5L_rZekKX-q89hNu-9bnEdNpeyvOdBTL3cVVlCtVsBjbtnuMiipgeg6ZXM65DfqRHiMi-ZLz3t_xzxggMyl3I45_IjTqrp-iZ8ggIjCsIBp8el27VWcwtQINcrD6GGIIEMO9YkUfV8Bl59ZIoixWuFDU-ZPkQ7xEwbfetpZI=w1116-h929-no" alt=""></p></li><li><p>但是 request/post也會有缺點，因為網站在互動時會有許多參數需要發送給伺服器，比方說告訴對方的伺服器要抓哪個粉絲專頁，每次加載新貼文時也要記錄上次加載的最後一篇貼文的ID，這次要加載幾篇貼文；加載貼文的留言時也需要告訴對方的伺服器說我們要抓哪篇貼文的ID，上次加載到哪則留言以及這次要加載多少則留言…等等資訊。</p></li></ul><h1>抓取貼文</h1><ul><li>我們可以透過 Chrome 的 F12 的檢查功能追蹤網站在互動的過程中背後送出哪些 request/post來跟伺服器互動。下圖是我先開啟粉絲專頁，開啟 F12 的 Network 頁籤，並且不斷網下滑動頁面，加載更多資料後的結果<br><img src="https://lh3.googleusercontent.com/lquFOWxtMzAtvZGGTbhRikbE8PN8TVr-0OQB-f1uzsIT6AK7ccVnXCzksZ8wpmclIxYA3xrtB8KTB_LMR54876a_SSaFXo2ib7U2iR77KJlOlDN6z5nFz2CAFI8KoEBkudFf_nBQM_Ey5Ekddc81FxAfWfTtvSZHQeHFAiYVHZv05SOB4pisYJd4BwuNSZ05AIfwzXhjp_9ckhFhPlHlKCr5MGjm7m4GTrJVaFzNBMEylPhJVCbgsS9DM2sffxsQkmPgnDWwijM-pRxRsJSHbo9bYtVngUQN2fnNirpCAJUT06nKX-dK2s6VjtQDLHiR6766jyqbOz5ZV3f-epzJuG-HD5n5MwhwpZLdnZQGbYRjDYFtW70eqF8d7T04kRcIS_-jWH6eXiHtuEEm7GouKWPwJ6vGhHstAqVRxK5C0xN_sOALw59O0dk1KaKyP8aKekeWtl12V8P0rLV_ENfRwESfyRzaaMPuZupx046gPYLsJiNguwmibBAsgW-CYc3awDtftsTdGKGKvhWn_1ooh5uumpb2iE_dUdjaQStMqchtDdYfhcoXA2xNVJWWnc54PxORWgae6qUwlbszAuHiH76Dn8wEmgzZLn_iQko220tAFT3ff4TTHrVm7su4oW391rhSfqb5w9GN9nK6Ds-lQVx9r8w16uMB75n8IDHFH5AvNc7RzM5T-1zg0PYN8NsX2Bcp6Rtfy_C5iHYwsPpeeOoggb9HjQ4ijGIdgNauhcTsvxIEt5N0KPs=w1698-h929-no" alt=""></li><li>點開其中一個請求(request/post)後，首先我們可以看到這裡是透過 get 的方式來取得資料，，但是裡面卻有相當複雜的參數(如下圖)<br><img src="https://lh3.googleusercontent.com/BGtXQX5cUyobE9-22bHmY5YjaniiSG7s1HJUV3c3xYZkIXoQP1YNwoAgjRjAhElqmZ4mUShOiTCNGquMTqeXgoCr8NeCYz-5AOW1qTse7L4ZfCAHRIofWSKtb2SlzAyx_KlVyxV0ZvblhFS5hwwcp3FoOUNUUb7MQ62p-XBowpeceqtJRUDAWpqbcLl6YVX5-TaEvkeA01uxpUvLxlvyTeG43QZzc8l4rTk_KZrdnUFSg6uhdnho7Ld-djHI8TFNW3BTcvEzf8Ml8uMmzjlBMNllQDc4uamTloGAMFreDsoi8AAGgT4RVYSV--iknw2gwxNXGRVc6q6nJNZY6OHogGIjziXZuoegyosI-xLpwIcaolBPTxbUu7dSIuoSqCEeE0F22wpulf7GhDnHzX6dJb1EDuFtDdpU8YS37NzCEDz3hibyQpDkvkze62h1veJIm27itdKsT3szO-yOovzghD5KBLI0G316FCFNVKEENypCFetw8SDKgbGQ80tiegcKFw3qGkJOd3q4Afl7Mg-7OsDweRj3iYXSLclsQueYAW1qLGXP6RxZES04DqynUv-2MJgaZIcTUMEhYE3hJAoowi6QNnWaeUakL8HmInunDZlFt8jq3VkVQqsuuqPOeFpnwqlDHB-5IB5TkWqaSIcNp8fInNrTW1X2I9JovJ7v5zHp0Hq0YLa-S1uRivb2TSrVSbdllnK1NNaQc7reS5xx7aATV816cupqokN5CMFbFGry9A8KEW2Znm8=w852-h929-no" alt=""></li><li>如果你跟我一樣看到Request URL中的參數就眼花了的話…別擔心，我們可以把畫面滑到最下面，這時候我們就可以看到比較乾淨、整齊的資料了，在這裡簡單說明幾個重要的參數<ul><li>pageid: 用戶在 Facebook 的 ID</li><li>cursor<ul><li>timeline_cursor: 前次加載資料最後的貼文編號，這次會從下一篇開始繼續抓</li><li>has_next_page: 有沒有更多的貼文</li></ul></li><li>unit_count: 這次要加載幾篇貼文<br><img src="https://lh3.googleusercontent.com/jX2Tdmpmcfp83g9JoxhTgE2daBeAbHYFJzjmsRhZdCYjFSt1LE4rbJX54wR3T_W5F1hS5Y3EkwFmWdIU5c5QVKbDRYtdaRP-nCwY2CKUcgJE1DDULskChYoCEtqg602oBZ_Aln-flFCxxl_CK7OGew9A1y-rGVT5fKHIsl7pR6XUi-GnWnZLFqwUhTHnrwuNJvRoHq40Edpzgb3UTjXOwk7qiae6vl2_k71lJpgWWsqBCmJS7CrUAmGUaNISXZmbsluTBbmG__KHNardGSIHsllOaymXCFearms1WUrtk_bGyQtluIkRLw8vCkldS37PD70SRuUHPR3O027Y4wCXwtq-WmdfAQcZjDG6WI2cCrS8z3Gr1EGe8veZN4wsszmzok9b8iLmElaeMrOcRxlUqNH2RkSABSyzwnrjzvZSXwKqBwgzczW2w5hqGqRRRo_pxusVgjvgdgtMiA3Bu97o42eyS2lM5TWt0W4mFsuvYH-5kB7DwQd9b_I0HGxIIcUA1traf0dU3K9ptA6bVmADBonxIqhh5Di11Azt-k-spp9RIExlq3Dn6uRtheqR6sm9w05u9l-WhP6W2rxOD_p8FRLvp1fVVpbZS4rqo441velV1RAPEXSeEkM7YkN7ASPIih3oWhjCgDtzU1HgyUHQ7GEk8_0ZJHZS9LRbxldlrnAY77xQ9tCrEEATzvpJ1pjTwfVbLcs8Vak_b5Cz8uhwon8m7d4XH0rmoMuy0PR_U8tiDAlvKYGBuSo=w854-h929-no" alt=""></li></ul></li><li>接著透過反覆 get 資料，並且更新 timeline_cursor 的參數，我們就可以一直抓到貼文的資料囉!</li></ul><h1>抓取留言</h1><ul><li>抓取留言的方式也與上面同樣是透過 Chrome 的 F12 的檢查功能追蹤網站在互動的過程中背後送出哪些 request/post來跟伺服器互動。下圖是我先開啟一則貼文，開啟 F12 的 Network 頁籤，並且不斷點擊「查看更多留言」後的結果<br><img src="https://lh3.googleusercontent.com/3kBse9YvxO25CYuamJQdL48fDv7evBfZhQTrPh9rfXpoVy8vN0cjVB3GXVpou2OyqDbgglLGo2ltC04A0V3sE2nS7HzuykxnI6q673iVgABBsMX9Fgk_OyMUqAVn0zskSuS2DvTOJQ6IfACmmpR9dRHxASCKXQliFwpkpn2AMgAVoswMoRFJMw_6RkGZbAXwiN-hr_Tw8ortF1M5bdjcMquReIs1aSPewaFs3Az3eBygrFlBb33ZqdDrJ_KdpEg74HN-oIfBlelAxjBwJnOtFTGfZIPpxZS1GYzeqEFjlUvFcuLni9PyxnNp2M9v0dWWwTBz4kButXWh8_Xyw5saBN0_z8jBnkxXIPQRZKZ9tx9lm1Uccy-ts2hja4YzXWYCr4klSwgdOLgD4tpAMt0isE7I0JGzlMJCAMY7PdQW_sc-5v8bJkH3J3W2JTyhLxJzLL3aKa8uV2V8BMwbjP-D-xIrqGBq4iZaPeEzAcx0zC6znWx94w8lu3ioZwUfJHXN51YLmlYswCwCpeo3r9CkDpgHdDLaWywC8r_wV0LWAn5tmYjay1vagdJ9ktQtv-areKBpebjOzAuWy4sLrJo65RYeaTkn-Zs9A87G8rTcvU_dPdn16pGfbhmCTBlpA7qruN4gLskaBisal3aPZYNOH_DlwFpUB_kRYW4j_l5tVqv1cRILClcgTyZWOaBZhzYnCq1ZCgPqqeh8vcujW_xh8P0ARhUkD7P5YL9YeqlY5T074JrZdhPFjEA=w1696-h929-no" alt=""></li><li>點開其中一個請求(request/post)後，跟抓貼文不同的是這裡是透過 post 的方式來取得資料(如下圖)<br><img src="https://lh3.googleusercontent.com/nKPUa9qKvfQTzzyHWtaWfKnOwzROswD85miOeKIMcnvVM1H1O0IHltMxm7J0z19gR46mAIIofhLkmXrLZlewtwU9lDYhzUbIfKpmCG-x0LAE0s01_4GXp6-_w3qGfLQJlHgxN0nRTlhOFIels2sToMgokJYnVYK_lpRol9LSJe8dNKbMP_RNfZVNgUdZn9fU-yIO0hyI85XF1hrP4e8PqdRxgguPoMFGavHbkW3b0-sA_vLLZkhGcPY3_Bs2J80PlQRzKJzMce6l_D__EnqebQARPKepGKRMSwOy79qU5sTTPiWx3qCv3Pmlsta8mdboJgiZh_dir87htEYk7_YCQ6UhAHOvuYmt7Nnv1OmbuA8a2Ejrc-ZYQiknLlSshQUAhkhAFkDuJdiFhHv04KrCIEizP51u4eEZC2lV3cYdbxwrthbtbVbe0gCz-SEUwNlGOdYWXExYBL_k8accHQGDznwdvZTU84KnKAzoNimx1ENDzWeg2vcuOrDcdWUSh-LjcdON5MCiY9EW0eRscADxtrB6ODstEcNSW3-ALAUcOIzK_OupcY67mTaQChwpyjCgL3m6tLB4NJwE-tv1lczbleOeZarUfX1QOsPPcnV4pdpMjN3WO5eTaeIdeg7j7lGBJYBGnJhTQ-jK3m_Pnf4mEXknAOhYKxNKx6Pu6ssuW0RZ-1IYG-vCDTzPkj5aN914FY0Um0eu_sN5YvFxMODI5dCvLLCeecHlm1CxgjYngVLkhlX-VPnLno0=w847-h929-no" alt=""></li><li>我們同樣把畫面滑到最下面，看到裡面也有相當多複雜的參數，簡單說明幾個重要的參數如下<ul><li>variable<ul><li>after： 前次加載資料最後的留言編號，這次會從下一篇開始繼續抓</li><li>feedbackID: 留言的ID，如果要抓取這個留言(Comment)的回覆(Reply)，會需要記錄下這個資料</li><li>first: 要抓多少筆資料，最多可以設置50</li><li>viewOption: 留言的排序方式，有相關程度、最新到最舊與所有留言，預設是相關程度，有需要其他排序方式的人，可以自行透過 F12 觀察要輸入的值是什麼</li></ul></li><li>docid: 設定成畫面中的數值即可，沒有放會抓不到資料<br><img src="https://lh3.googleusercontent.com/HwVy6hgv1cIaV-bsqpggVKIbioHOAAvFHDYRA5nwaRSCERE18fthrIEliwvdPMpnF8iyQCdUt6HsC0nT3mbFyXVwBm9tFULzN0oD7O2oSjrMLBUr7LKlKzl_GoGOCIVeEl6ct9jKo8eO78hSiFgr698oPBQyO6TQxUC3qN6PZNxVaT-DKWjQWenLodkrblduc94EXnwEMqT98Xja0nprsFSm_FJKQd9ctweJcrYqm5Ndv2fCzAcIeavK0o23ng_BBiTBXi6GFTsEQo2AjfIBvp5huTmG2ULDcvLqJFzFCJwOtCoEcfq_vM-qP8KOyju7KB35IYOsVNY9Q-o4jjFyGZ_ullXxkPOcr-eiWOcdo8_7HycA_4gL_EY4phA1549igwtI6T1Bt1xYWZnEcLJQgSdufO-VkP2HwZvLTH9-DxBMFSEXHUKXdDILfsCUwrNKreEoprgJwxHwve7sH4mQ8DgIepMhcmxiy-vXxq6hzy52gBFoA4Uqah6Aku2zTctdYCCH5g5c7I0VX8wIWeNKqZYbZ977iJFyMY4LJPr1I99cJJg2du2UQV-21EENhTFN90xbKUaS2IpLa-8xXGJlMr157V2SJSIqsojRfSJ42yXoxFfHAva9iaSPvW8K0maS8s8dTMze-7hwp825PMM_ft9c2_BlbhBKbGDbsVj1nKRFjFuLr9C0CZ2y7fx29gC3C2byE4eVP9wTtchw-XVm5-4_1uLukfOz2oqXjQrkdl2MkgtxxVVj4wM=w849-h929-no" alt=""></li></ul></li><li>接著透過反覆 post 資料，並且更新 after 的參數，我們就可以抓到留言的資料囉!</li></ul><h1>後記</h1><ul><li>當我們成功從對方的伺服器抓到資料後，我們取得的會是 json 格式的資料，屆時還需要再利用 json.loads, BeautifulSoup 與 正則表達式 慢慢剖析取得的資料才可以將資料整理成DataFrame!</li><li>以下是我抓取台灣2020總統大選的3位候選人(民進黨的蔡英文、國民黨的韓國瑜以及親民黨的宋楚瑜)的粉絲頁資料，時間範圍是 2019-01-01 至 2020-03-09 的貼文和留言資料。共計有 1,700 篇貼文與 355 萬則留言，欄位的說明如下<ul><li><p>貼文</p><ul><li>NAME：姓名</li><li>PAGEID：粉絲專頁ID</li><li>POSTID：貼文ID</li><li>TIME：發文時間</li><li>CONTENT：貼文內容</li><li>TYPENAME：貼文類型</li><li>COMMENT_COUNT：留言數(包含回應留言的回覆)</li><li>DISPLAY_COMMENTS：留言數(單純回應貼文的留言)</li><li>SHARE_COUNT：分享數</li><li>FEEDBACKTARGETID：貼文ID</li><li>ANGER：怒心情數</li><li>HAHA：哈哈心情數</li><li>LIKE：讚心情數</li><li>LOVE：大心心情數</li><li>SORRY：嗚心情數</li><li>WOW：哇心情數</li><li>UPDATETIME：抓取資料的時間<br><img src="https://lh3.googleusercontent.com/SDCISaMhAgQaA2Z1E8eLyZKBOiHS8CyZbOeDDAPvKvcsDtGLt-A0islLxu8c38oYoNkPvO8uCjRKSQnxQILMlmhAtTCShOWVMomEdz3gQunhMgS_rsXYQxASvMJj4KcUq1Mvm0bZK_e-o7i0y0iFGvqpscKydrgIqiCE1QeYkCqmPH34F7GdQqNwfqVm2wz6UYuMhsgYNNZO0Pc3s9zJwQzQ4ZlV1Egcf25awcIbSJq57OZP_94EpR6VgJ2XmSdktjsHtvK86FYWQMiTTVDnLizLBhaICuFK75yK9rWGkhysY0tyNpZ_fZovIgTEoNGV1mUHGr3l8stJSq1yLei8Cu6GkUp6Vd3o_yhYPaoBJwc5R_5h7IZiECIuzSJ_Fz5cuBetNhxZRhN6ZWtwZw-QHh65J_3obaYUfi-E1tEiWtXDySY1_iewisn3D_P7yweBfUOVEC-OKE6tgQu5qRJIaqUYLSD_vo8I90Wung51YOHDCBVsKjr7HtwcP5HxGblCAKBN_iD-7coXIkjcX3nNUa0R1MWT83MzO2T7S5yECzJBzC4YV7SCg0eDTTUzTQDKPAyFmIkl8fVVZNuwJVnOxkOTsZ9x1QL39Pac3whhEnxOOopY62x58iGFsMGcpFp-QG41l2zB2k47Db5R2gJrNnW_cf5clq7Ky_4DULAiGlTb5tj7uLGNzFf69PB9wy5KBo9J2JFtlYQT6q-F7rSJCSNOyNYZ_8BWK52coliIbTgQW06dk5jVHbo=w1847-h822-no" alt=""></li></ul></li><li><p>留言</p><ul><li>NAME：留言人姓名</li><li>AUTHOR：留言人ID</li><li>TIME：留言時間</li><li>TEXT：留言內容</li><li>TYPENAME：留言類型</li><li>DISPLAY_COMMENTS:留言下的回覆數量</li><li>FEEDBACKTARGETID：該則留言的ID</li><li>REACTORS：心情互動數量(Like, Wow, Sad…)</li><li>COMMENTID:該則留言的ID</li><li>POSTID:貼文的ID</li><li>SHARE_FBID：該則留言的ID</li><li>UPDATETIME：抓取資料的時間<br><img src="https://lh3.googleusercontent.com/Khu49E5W45JG8gvzz1YD5An01Rf_iq9kl8FHLFt0Xa3Hxk5LZmtZzVtFjYLT_0CC8RsfUs0pbwWNzGz6IJ-bjH-XcJQ2EF1A0hXE_4upfhRxSGq-o1ZK25z7Hnw5JwYvft6cCi-VOIQZaUMHnN8PvHLzDr7Ghv8dk1O8Q61joi5p3MdyUpo9ISrKNPBr8_D98Jl8grYgDeeGyR-wt_jfWabZstR9dKuCmb8COQC5pVvgmliev0oT_lvQ2lbRTWYvgbm9IxI2E-JhToDdTMdDAsuW-5v_QePy82g4jKst40Yw-311LVoBnSMAd2UH6zs5kXNa2CelLUiSpI_AQCuoR7ivb8mkBq7tE7UDnvOhduzNetayY9G-u_adgYhJqKVJuANKM_mV_sMzVQcQuvnAI-jNCMiqWJSD7b5Ne1xg9RY14YEBynh09H5G7TXkP8PmejVDwyRR4HK9box9mdvZBVam7bGvozedrSpDRynCoE2NOr9-rmjeeBiwBUOekJv4HF9Yl_2KTkFBG6GY95onp6cHkn-2U09cf1Tajq0sqrMNfBTxXG96wW2OxzeDRFeW5YbeY-dvWr6-AsGy30tS9wn3txHTZ9c3Yjb8IyBv1ODXz4x4iqkP9yrktH4lQHVvh5qhTozfbZC6-y7U8kYG5eY1VKEHxlnBxFiUiKwKQ27P2nzt7fgiJC_ZKccIF1ObEQDgcdZhqbXkhrPd2YFFXiOY5kkt6JerrDR5r6Em9nJnHkler9PH5FA=w1845-h830-no" alt=""></li></ul></li></ul></li><li>由於抓取這些資料比較敏感，就不提供程式了，有需要的人可以直接下載我抓好的資料，有其他需求的人再請來信跟我聯繫。<ul><li>由於 Excel 有約100萬筆資料的限制，因此我將資料存成 pickle 的格式，有需要的人再透過pd.read_pickle 的方式讀取資料即可!<ul><li><a href="https://drive.google.com/file/d/1iWHXZmab-D46rQDuiSGcLeBBTrnHfavH/view?usp=sharing" target="_blank" rel="noopener">貼文資料</a>:1700則貼文</li><li><a href="https://drive.google.com/file/d/13URXpiEEHefjIt10-rAAoGTt9Jcg6tp_/view?usp=sharing" target="_blank" rel="noopener">留言資料</a>: 355萬則留言</li></ul></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Web Crawler </tag>
            
            <tag> Facebook </tag>
            
            <tag> requests </tag>
            
            <tag> 總統大選 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>台灣政治人物 Facebook 粉絲專頁清單</title>
      <link href="/2020/02/27/Crawl-PageBoard/"/>
      <url>/2020/02/27/Crawl-PageBoard/</url>
      <content type="html"><![CDATA[<p>最近想要做Facebook上有關政治的社群網絡分析，所以寫了一個 Facebook粉絲專頁的網絡爬蟲程式。但是當我寫完程式後又出現了下個問題，就是「政治人物有這麽多，要怎麼收集這些人的粉絲頁連結清單呢?」</p><a id="more"></a><h1>文章架構</h1><ul><li>確認目標網站</li><li>觀察網站結構</li><li>開發爬蟲程式</li><li>後記</li></ul><h1>確認目標網站</h1><ul><li><p>經過一番搜索後，我發現 <a href="http://page.board.tw/rank.php?tagid=24" target="_blank" rel="noopener">FB專頁儀表板</a> 這個網站已經幫我們整理好了這些資料，上面共有 742 位政治人物粉絲專頁的名稱、連結、討論人數、變化趨勢與標籤，因此我們就不用從頭開始手動整理資料囉!</p><p><img src="https://lh3.googleusercontent.com/JNckJySxn8TVQha91Sb-kSDm6b4_fejRXYyqJBtKcTynzgbVczC0F1xH-_d-Yjt81gkigrFXFyLyXOrgLgcHynx8I8UDdMz4a1qxdOKut3B6M5LNS3gavs7LIbTy8JELgmas_nyg9lqpA3qWERkDYxorItvcrb8OQO0RkcfdIFUUJbXoivDgsGg3dNbxe9i_lScmtrYeIHGZ-rGYLHOrgZJ1La6MZi-gHq7D7qUJkHRXelTpxXN3q-tpduYkvm3WQk4u8dxghVjvzdiIyfr1b5E0BKCM2Q3HIMzpUBQyKH65VL6GwcYf0ASvfgB8wjTPttrxQrpFklgRxsOh0IIBXriqXQiPri4_WyzdJ4mUC23gervKE_SRaqVT88s9tOJWRWg0vy5EPQ03VhPrEsxLgmYOu8RtoiT6NRyJX2NwbzbNM0OQI03-kcBbgQMa53v-L7fwskUT757P0ffFX5rPtsFf38zcGMCKNsv1pdUHdqbWEm6DKPCCrsCFGF94-Jdk6NfiLUNbEy2pvjhwdA7ZT9BV1jyZurbJ6khpCcXLTSK_s_QRtQLd6ChqoAIgolCPjLOSNZE-pwMnikF3PGNzkzDDeLrMEBG-P6muC6v5TAhipbrnm0pFTv9I_mNWwVI3r-NQAN0LrPPv2qayubCF06oW-NlBmulGmQjjcf57qmzXMVi8UxJT5JOs4bGrngEXA5E0XaPmRQjZT-_s54TPEdl_woR07uYUEvv04uWv__udrwYI=w1358-h978-no" alt=""></p></li><li><p>那麼下一個問題就是我們要怎麼透過網路爬蟲把表格中的資料抓下來囉!</p></li></ul><h1>觀察網站結構</h1><ul><li><p>載入使用套件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure></li><li><p>送出 request 請求資料</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'http://page.board.tw/rank.php?tagid=24'</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line">soup = BeautifulSoup(resp.text)</span><br><span class="line">soup</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/ED9wGezykG9ACLoWe4ZRSkwFsS86RbtxQFcyp-M6ekL9CT_2cczxghBCKp_YSn5BFi6wU4FNeDQO9hM2mbVytDtlXmNHIj5EA0VCbnhdEnaEdOVBMcUxzX7_--ByJtwhOmoi9geUz1wSlrim7kuQun2Rh5U1PO3sZBy-qFu3VB1yHKVFcTYHQAKOaQCZnG3B0PRFy-aGm7Elr0_0APM40bMz52Q1bcpzO1mW7zCUf83_WIVfpQPzW1V-_JRS3mQE4hBEvj4OVL3VLUYskS-9QhgxedyCkIqhbU8PnWvh_Y22RF7u3TWUThHh-f9E1ZYK5H29o6LKYd48Z97z_c6Ik5GShYnXLMdDVk0gFzwpsUiM8ogeKGsZtcOiOXDCM1sfFvnI8KUhWnvvOc0nLgl2Okuc-FaMFPEciZalXL2hO70KC-3BA8whw7EGoo2K_h75GXO94KvLQX8NSi47u1pVl58rMq1m8-OXSEF7RneUfzaG0nBHngFqcosJTBYE-rcUyPRZxxfRnar29GfCGU2JIuh1GaxJgMEjRBgkANvsk3_4mLcPRSXJgoBXSykDZ_D97UuF6tlT5CBpqeXLeYzxMd61DezK2F2wjOxForG_2q3LClTelLSpobMV3ohl5skTCdAcfEf4SQUQCWKV3u78uid1VkLbhZLm7_e9QjRXDj2U8lq4f2Xehd5tArIVI8ZLqQ3Y4WYiJp4BsC6Ewlet_7DrD84We1UI9nFZRh4e8sGF9F1R=w1852-h764-no" alt=""></p><ul><li>結果發現網站的結構非常非常單純，資料也都放在 tr, td, a 等等 element 中，因此我們只需要逐一取出 element 中的資料就可以抓到我們要的資料囉!</li></ul></li></ul><h1>開發爬蟲程式</h1><ul><li><p>因為網站的結構很單純，這個爬蟲程式非常快就寫完了，具體語法如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">df = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'ui container inside'</span>&#125;).find_all(<span class="string">'tr'</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ndf = pd.DataFrame(data = [&#123;<span class="string">'名稱'</span>:i.find(<span class="string">'a'</span>).text,</span><br><span class="line">                                    <span class="string">'連結'</span>:i.find(<span class="string">'a'</span>)[<span class="string">'href'</span>],</span><br><span class="line">                                    <span class="string">'討論人數'</span>:i.find_all(<span class="string">'a'</span>)[<span class="number">1</span>].text,</span><br><span class="line">                                    <span class="string">'變化'</span>:i.find(<span class="string">'div'</span>).text,</span><br><span class="line">                                    <span class="string">'標籤'</span>: <span class="string">', '</span>.join([i.text <span class="keyword">for</span> i <span class="keyword">in</span> i.find_all(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'ui teal tag label'</span>&#125;)])&#125;],</span><br><span class="line">                           columns = [<span class="string">'名稱'</span>, <span class="string">'連結'</span>, <span class="string">'討論人數'</span>, <span class="string">'變化'</span>, <span class="string">'標籤'</span>])</span><br><span class="line">        df.append(ndf)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">df = pd.concat(df, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/nB1_aowklDYVNF2K9GLQARx32UGE18JWg-sRzFNpwYWTqAdn5IvZo4LkKfM4LL60KcOaahZt-W0UHxaf6Ju3qwAzq6HmmTblOsi8fmOqqBKGWV6DPHltOQX0QAkrue3QBcga3B_S2yArBoxy4xHJdeOn0UAZRA2LlHNR9gxHGgBTwEahixO4phEc3R33sOtFKpLbNHD_WGvXrl1dG2x5wHBvAwxhFFHfxdKKA77_4bwjfiNn8H2VfKqNQHG5NotNw6-owLsjaajBlx8GnyzQfyMNX03emXqo2t0M_Sb7q5iSY74yAXCfIbfyXweOTGA88VxBngNYIDQTSDvI_l3HiFrvN4umf2fq3t_6y49tmUAmQzrFNResJYgtlu9Z_qV35UZft2d2LTcPJXDaeEsoNQKivNU4A57PjW-4bXH0-n57qHWafejy2511gbMsy5pX01nwWCzTkWAMds89j_LGMWOjf8sHZtWEALFNEyKlLUAkb7LXhclFFT0H1lnZsvgzIFozfbur5gnoIbWfvYf7nzZLueQTS0zS9N8cleBCnWP81Nxkvv7DgLU6yGwpzHylis6rXSjjSSfoZzadMOEiwh2HISw-Y6jhaPK6YDq2r-oqUUVnLWZOzeWSs6-Tcr2HeXzoFq_gwL4a3EaRA-Ll01wVZULSOtplvIIzTEWMVZEe0JWs_4D_fp_HIQh-kZG8hf_rRRtz9QoNiUFnYo-c-X_Pyr46BAVRPFF9RboRYseeMrbN=w1199-h705-no" alt=""></p></li></ul><h1>保存資料</h1><ul><li><p>將資料存在 Google Drive 中的 Colab Notebooks 資料夾</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path = <span class="string">'/content/drive/My Drive/Colab Notebooks'</span></span><br><span class="line">os.listdir(path)</span><br><span class="line">df.to_excel(path+<span class="string">'fanspages.xlsx'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/Lgi4vg4DHo5gB6-fP_DnJAGc4tTlp8UudTWo5sKA8ozcQyPV9EOkgAKAjyhtOi9VSnroXNGz35yr4pxpaa8bkBhPhq1Z3aAS-yYNW45zigeVXvwf28hmL2B6Jjrsk8S-_B2QQ6INi3riUERnyKar4fh5tAV1-zvh63OrAXPrVDCq7SGuMeYp82oPGuzYOZQhj-nz6FFDwdjVwUs5As0aLsV3B4GyqLB3mydA_7vzeF6agrl5kezDZyHrY2xvv3GXUKnOdBNm6CICPR3VZqWBN9H-f2A2CxaTU36q395ol_ph7mTqaTb8CYvAgzxkg98FvecZwijF8iZyN7ioiJkNot9iewqAkow3aMxwMm-Ot8fshcS32ce6-yBwFAJpSynzh4Cv8jXJ6VqO2HcyKd4EZKfINAwdDdfLRF5sD_utpUbSLJiS4DuqkIk_FGU0OQj2abQYg8b021th3hbeMUZoDVelGh_K3ySrzlT9WI8yWu-BCWfpbf5jDP_Mn4uDH9bLkpd1OGAedbkJxTCLxXE6lpKkRL3D-BedCGEJllXvo4Nq2N7K9vWIEwx-gi36try6aJUUD2ovFxLsQ6dflftK9kIn3vF49PhsWnJFRxtdyuIOFpqFRTF0S0F1xmDLFXtdHLpzBg868xBfTy3s-pSfTU5uRtte52kuhzWyROipcML6YiF4at0Ix-cBq7hCJstPVzLH4fRCvJnitfr--GELa0pnCYt0w5hem7z0eo1zfR_iNNsE=w933-h978-no" alt=""></p></li></ul><h1>後記</h1><ul><li>沒錯，就是這麼簡單就完成了這次的爬蟲程式~</li><li>有需要這份程式或檔案的人可以在這邊找到<ul><li><a href="https://colab.research.google.com/drive/11xN8QftibAGf_jdXj7BWf4h40yhXnLlM" target="_blank" rel="noopener">專頁儀表板爬蟲程式.ipynb</a></li><li><a href="https://drive.google.com/file/d/1-6BL2EDs1LpE-aiWtV32RffauMGHAoHW/view?usp=sharing" target="_blank" rel="noopener">政治人物粉絲專頁清單.xlsx</a></li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> FB專頁儀表板 </tag>
            
            <tag> Web Crawler </tag>
            
            <tag> colab </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何抓取 Google Trends 近1月熱門關鍵字</title>
      <link href="/2020/02/18/Crawl-GoogleTrends/"/>
      <url>/2020/02/18/Crawl-GoogleTrends/</url>
      <content type="html"><![CDATA[<p><a href="https://trends.google.com.tw/trends/?geo=TW" target="_blank" rel="noopener">Google Trends</a> 是提供我們查詢關鍵字搜索次數的網站，如果我們適當的加入時間與空間的維度進行分析，還會發現許多有趣的事情!想對 Google Trends 有更多了解的人可以參考 <a href="https://growthmarketing.tw/google-trends/" target="_blank" rel="noopener">Google趨勢－你一定要會用的四大實用功能！</a>在這篇文章中我將示範怎麼抓取近一個月的熱門關鍵字與被查詢的次數!</p><a id="more"></a><ul><li>我們直接來看最終的成果<br><img src="https://lh3.googleusercontent.com/XPlul5dcUZBSkRoue3UjvUhLOUCf-Yhr6EqX3wEDLlo3HDp3WNyneHkuxj8fpbP0k8UGhRVYRNvNy3OM8wULyafiBvU9beY0ntJepD52lqy7LrXfjVBfHNLjNN0gunFlRvpXVOeJRdIfgtVnqaFqQ1b5ulUBinBza_XZBGmSS52QkaLrot6s7vWFs1Lte6Qhijj0gDUW_5dSVQf8xQ_Zlp0bpy9hCBQmuEYrZcGiXTUn52gyRWs-ANecQBIGh-WsDGHhfqnjB8q9sL2RVQ_8a22Xtv5YsLjy_IZ64jXpi40dylQq8emegRG617DSGFS5MtDIZkH_TgvD3VM7s2DHgweqsQgrFkRRO4SZ4S48o4lB20vxtbzy44vDiU2JpQui3LbvH0By3qyni9pw7p6wmH-9OooqpHUZutZsisCt2eVmK3If0dTeZXqd23yUaWvg2aXXH9ywE7VfCsaMzyC3A4noJ2NIB59oupZuoFiBEUpRMYc2xCwUo8vGxu2OWpeTvAyRK4-hKpK-vLtcHTq53NZSa-GGo5hhalkyCt2frVAn-TiUqfhq5vkHM9yuxPvm6oprhzuaMVsVNZfEhOVfk5H8lOw8fuOEg2cIJqkGlorWw49SuxNQ_qEffw0ZsVGhkhoMrQEX5lx5V0rVR1GPL2qf-xZ-IrvjY478rRlPoDTr4pt0Z-gJ31AAkfEJuOkky0ac35_WNqjVjiPyQYAC_jQOZdmJHol2gawivTiCcvlZOWZE=w1785-h978-no" alt=""></li></ul><h1>文章架構</h1><ul><li>觀察網站架構</li><li>開發爬蟲程式</li><li>後記</li></ul><h1>觀察網站架構</h1><ul><li>首先開啟網頁並開啟 F12 的檢查功能</li><li>點選右上角螢光筆標註的按鈕把雜訊都清空<br><img src="https://lh3.googleusercontent.com/PMJrh9Cg1a7vqk8It5OxT9NUZxeVRWkRRc6ZyZzgxJIP1KVhSQhuUuH-TfKzmEXVv4qYvTELCgi9GLVlYkbl4d_ZLGoVslmB1Kojf-Iqx9ysu87fgmojjHiA4YJis8VJGvGLLlh-Bj7OaTEB3pu2pRxrhzf6xRD2eKwicqGfb9NVCyCu8gu29xPNFEs4PNj8g0bSVfEEMc63g6fEzrCyDz5R0at_ACa6416LwbhoQrgeuOs-HC0QOXctWXyZomAL0_Ycny-S_BetUbimc4dv3efgJo4AtajqE-jnWKgCoVQ2JzOQIc1CPh17sO4q8Shd6YH7boKMyNNMG7YDxzSE-4h50lK-Z05NLHtZcjCd1ToaP3zXlaqbtMxQvE9S8W5visfYDhbO7u6OWqPQppzcrGvnGr9MNkQChkLl-IEewmZ_9QUHbOlmO0GIkXcYaTu-Tn4hXJEKicQPjCBJkZDBNKyMLzM5YWOwBqy-J0G6Vg30t5TLLc62DhVmyAPdFBu-wC3qA_oLIrva9iNen0hmG4mcHehlLcK87yDBxLEi8_v3Z_qbI7W89seZyeveBB4jmaoaybpWCXY74uzsL3Y8JuzxMf4FM8X2ksVAI1FVM_donWYvJ54Z4xQQeVNLZ4NRIFBUx41LJkLa8-chewyYxHHQI_5GlJQxKs_VwtkgUe1GbtpbfmiX73xli1AZHeU9yJUx9ueg9EepqDTk5jLTTbZe6Ps8aN67CTZjvl3SHhgY_R-U=w958-h523-no" alt=""></li><li>接著點選左下角的「載入更多」，並觀察有點的網站活動<br><img src="https://lh3.googleusercontent.com/k2RQaMPgHaY-9WU0PeOY3aV8ynXmKd7w-eSLwcs2DnGqPX86znmLTeHz9HQijVhq-vgsqImggkUcirYLW6Hb8epYvz4bzJCAI16HsLZRJO8wMVO6e7GS8S82r2HBR2IPLrFKzHZxL4cF6i5MKch3vuo-OFcni6y9A19jkV6hLfhu9SfI_Bjvj5fGr0IGE-5eMbzKoKVP4Ho1DhLO3SNMCwuK_VZcvg97BqDAfveTFS30DaEN5DnvVGoSVI6S_fs8KpEzymOUBy1TfK_IkhFME7N5_9N-NQPkLxivw8627diAhQRdqGvNstrD_urZ8EWLTzD-35oxxrNbnicJlBGAvYPTviOSa7fe2kJNKPmQpRxMrLxncXRcWyPaI3zfYb2GZEn1BzM34iac9PSoUIuL4BS5bnRbfdyxo_F6CjQv3Zu8fP7V0Q7Q2kmvs_9ZxOkxE7pId6Kmqo27kWBjj6dwGTKUrGx_ZRbnfuZ4Q8ID3Pu0gGbcK3Pd0S09mOkFmXB7W0bNy4D5pXAzDrIaI689RgCpbzfWZ_rw8US0-Q99rmWut_s4DLdWbOjO5gM2myuY8CkytwNBhcbJKImQn4sI3L2oOBn59ATjDegihP263L_lINzBbo83scCRUWCYcix1UVtr626U9gFTjwtP7e0feCoa0wpKSxV59HRZ62dfMrFl37Py50Z0sp2s1CFaPByGJz7dgF61X0aF3sYtZap0NeFBjc1enV4yjwJ58aoMFJNp8kC1=w958-h524-no" alt=""></li><li>點選後就會發現加載更多的資訊都在右邊送出的 request 中，而我們只需要解析這個 requeest 的回傳結果就可以抓到這些熱門關鍵字的資訊囉!<ul><li>有些比較複雜的網站會送出相當多的 request / post 來干擾我們的爬蟲，遇到這種狀況時可以善用 ctrl + F 的功能來搜索我們需要的資訊</li></ul></li><li>以下是我多按幾次載入更多，網頁送出的 request，聰明的你以下就會發現我們只有ed=後面帶入的日期在變動而已，因此我們只需要寫個簡單的迴圈相信就可以抓到我們需要的資料囉!<blockquote><ol><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200216&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200216&amp;geo=TW&amp;ns=15</a></li><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200215&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200215&amp;geo=TW&amp;ns=15</a></li><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200214&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200214&amp;geo=TW&amp;ns=15</a></li><li><a href="https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200213&amp;geo=TW&amp;ns=15" target="_blank" rel="noopener">https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200213&amp;geo=TW&amp;ns=15</a></li><li>…</li></ol></blockquote></li></ul><h1>開發爬蟲程式</h1><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">import pandas as pd</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import datetime</span><br></pre></td></tr></table></figure><h2 id="先嘗試送出一個-request-並解析回傳的資料"><a class="header-anchor" href="#先嘗試送出一個-request-並解析回傳的資料">¶</a>先嘗試送出一個 request 並解析回傳的資料</h2><ul><li><p>送出request嘗試抓取資料</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 如果連接掛掉可以自己把ed中的參數改成今天的日期~</span><br><span class="line">url = &apos;https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=20200119&amp;geo=TW&amp;ns=15&apos;</span><br><span class="line">resp = requests.get(url)</span><br><span class="line">resp.text</span><br></pre></td></tr></table></figure></li><li><p>送出 request 後會收到以下資料，前面有一段干擾的文字 「)]}’,\n」，而後面就是 json 格式的資料了!<br><img src="https://lh3.googleusercontent.com/pCb4Vf_u_fuZ0ppJuH8ERypJJCczItrPwlyxmZEqwRbGgADWjAuKY34gbqUGAds0H5Nhjm3gAY8yKEsSmLM0FHuWlR4SrNUTKth_cZrdQW5rWpZQJ2sKoZKOLjqVHopEAlFSUXRvDnqaaIMD_CH29gbRqJUlACx0AABUfgTq-FiRYjxeSBr4KbQJuS2wHdzJSv-gK4JTbvTmitMsFqKRp0YI4kv00zC115ZrGyCtFmjEopebN1IBDHcDkWC91fD2JDSGr-eRPOrJLBVH02KwaJPfUlfxED0_sXPICsewdML-BJckvWowh_zUoGYXRdt1sodZAPYENe6vo_bOuQRs3vx88EcDECGsjGMGI5ABv860auDCDUg-WvWONDC-zsNlD2a_n4jFj4t3AXV79j5VWm0InjX-s3ksuZZQTEokKHEHvOh9pEMyp2FHCPn2rBrsiCcU3h-dxEOEP0Q-AXx7d6YISMdEdDwlXWhgoLb3xvmOW05y9lU1xU-ZMR3LK9AB8kNG1ysIbJ1WlrnipZRZ1elKLT2adsxznZKRz0aIIs1mD3kMPe_xFsxt5sKjEX4Kw4M8JYE04Sj_kj6ZOg1Vcx5z1IHb8x8f-Gj2E3JLSWz1a5He6QXMH0pDVeegB1Hv6fBXRE8Zx7e6AHaF5PZSdFXJdIgrSSJO1NU_8QHdGIZyoDigrVVsWQDro28oGioSiYxgJmbwl1rd1qCATGKlFyvPES6s1v9XJQhKvtU9L5SHZJoM=w885-h59-no" alt=""></p></li><li><p>因此我們可以用一下的方式把資料整理成 DataFrame，中間還有一些簡單的提取資料的過程，因為比較簡單我就不多說明了!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame(json.loads(re.sub(r&apos;\)\]\&#125;\&apos;,\n&apos;, &apos;&apos;, resp.text))[&apos;default&apos;][&apos;trendingSearchesDays&apos;][0][&apos;trendingSearches&apos;])</span><br></pre></td></tr></table></figure><p><img src="https://lh3.googleusercontent.com/yI8UzXTPawvb8fMg5F89vVCxFjJDduWFjEoVJfhv7ZE6zthcO8cRDgCTgxDu2YTsI4NhcIECV0fVqyQt2JMR4Y-gMiXuo36KMMhfNFAsOdzwNMeGc66hUGXy4Nr0P2W73lfkNMbxk_IW1PMfjeREWpke4K3ubdoVyOQwaL3Cp87ZXRV30bDrhYPQw2Xh40V6l2K-kPqn_dymg97MlWZG3MP-IBoXtYQb4iPosXnCG03Vkn39bofWmZIg6gCwz5pA6AiJB8q_47UzjUvYy6_T2JRoogbhgFie9YA-PRwKq07PfCR87VEc-L9TvfqYysvhfAzoIAE4Ds5zQHwlOW8uesePDokM9UtJvpbbvRTuu427jU9eT89DeVD_uu3DQs0S2Wzk6Dy7ENBdoqPCYUaNIDyIA_sVvHeth1gfko_EIzxy1bCcOsrbboifHLdc-_rb40V3b4JOnImYUDjYjgrbJNGApYFpIK9NLjVVXK9eQ_Di14wLVewN3D4p3ugaIBrRJwVxAH1lSV5NhXhtZumBcAj1SGUa5n0gxbREURY_KGeHxgF8k4EE6apbsdC-jAgxLTVV6JcRTvushWqUmO8JXzdCAr7EcJDo2nmydpGrTAxs9usqsTX0Ha_HJlCOmZPfwcTyw5NsBFeAnRt0JD2YLFUd0RvkTDKls0mq9Jl4odxw4As_lA8P9scplDo77Xl0AT90rr5-i-fytBSuBkAGlU4hF7r_jO1rRm0gguPE7N3OVO0D=w1854-h706-no" alt=""></p></li></ul><h2 id="透過迴圈抓取近一月的熱門關鍵字"><a class="header-anchor" href="#透過迴圈抓取近一月的熱門關鍵字">¶</a>透過迴圈抓取近一月的熱門關鍵字</h2><ul><li><p>因為我們在前面發現只要改日期我們就能抓到資料，因此在這裡我先設定兩個日期分別是今天與前29天。接著就透過迴圈逐一的去解析資料成 DataFrame 並 appand 在一起就可以了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">enddt = datetime.datetime.today()</span><br><span class="line">startdt = enddt - datetime.timedelta(days=29)</span><br><span class="line">df = []</span><br><span class="line">for i in pd.date_range(start=datetime.datetime.strftime(startdt,&apos;%Y%m%d&apos;), end=datetime.datetime.strftime(enddt,&apos;%Y%m%d&apos;), freq=&apos;1D&apos;):</span><br><span class="line">      url = &apos;https://trends.google.com.tw/trends/api/dailytrends?hl=zh-TW&amp;tz=-480&amp;ed=&#123;&#125;&amp;geo=TW&amp;ns=15&apos;.format(datetime.datetime.strftime(i, &apos;%Y%m%d&apos;))</span><br><span class="line">      print(url)</span><br><span class="line">      resp = requests.get(url)</span><br><span class="line">      ndf=[]</span><br><span class="line">      ndf = pd.DataFrame(json.loads(re.sub(r&apos;\)\]\&#125;\&apos;,\n&apos;, &apos;&apos;, resp.text))[&apos;default&apos;][&apos;trendingSearchesDays&apos;][0][&apos;trendingSearches&apos;])</span><br><span class="line">      ndf[&apos;date&apos;] = datetime.datetime.strftime(i, &apos;%Y-%m-%d&apos;)</span><br><span class="line">      df.append(ndf)</span><br><span class="line">df = pd.concat(df, ignore_index=True)</span><br><span class="line">df[&apos;title&apos;] = df[&apos;title&apos;].apply(lambda x: x[&apos;query&apos;])</span><br><span class="line">df</span><br></pre></td></tr></table></figure></li><li><p>執行完就會看到近一個月每天的熱門關鍵詞與查詢次數囉!<br><img src="https://lh3.googleusercontent.com/62JSXkDW_YJjCI-jXB3rGTxKTRdapXBbACDu4Fzmg4u-8kkONHm9MD9-FozEFK95299xnZn-vnScTrM2f3Od_BJfI2SYERFfog8RwnIxRPS2lqKTZ1EjUsNNy_W4jFGnYwHdMGL9nrQMQm9wIVi91ihrWjY3WWD4PlPLzgTQge8F3oqzqpGrTfcjlU53R7iPzcjKLPuWJT5aseeQkcAjewZ47rtn3WTGP8u5EDufP-ug_AIfCpULIWbK-TXD7QzysmpbtEZcvN-Bxp6E5bCcN_szIYDghL473CaVYBnKskxNJWGNhip1UYi5rWfT29-4q0YyyeB4-zgieCPRa6iPQ54cZhun7MVfK_UEY5iXL3hgQaECv8lrIxPEWMb3VJBSV8yBRxdsBGjIQSosr_mKTn0D-THRyqJW00FGc6KGBJzJ7H0gnsi-Lmhf25gVjA3a0223Gggdvtyo3pdjMgTRLnCW_jHwTprWByr5Lwk0tzehJXBIfdy5YL7xxj7i338NbglHd7M7mDmGiMQeTyCrGhmBtnK5JzWavjQRe3IFYCVPLzXHg_oNQjObG4rRNlBRqklEjUb-XL256XVChOTkTxKUWOdiluM07YCA00DQpqD2TpacIv5ClqPiX0URRlz4wRdVuf_9xyeY3VPPH5DxVx7YFpDq5XUtrc9pc4eeU7fnPjC3MC2b9L42WBX6MwCsFRii3TBuOoL1WNqZIfA54KVgMeDG_CSwxhooxOyFglqXwMtZ=w1830-h743-no" alt=""></p></li><li><p>最後再一段保存資料的語法就可以把資料保存在 Google Drive 囉!<br><img src="https://lh3.googleusercontent.com/zV4IrTuvOQV0wcZ9mJVREcULr0zS6AA7ZYIbsowu3pUaujpumJvtO9BCoasJ0fyaHgkjTLfKWFWkCWZ2-2vueus1OC0UVXfCMuHDEkYZ_9vBmI7CjGEzka5_Ws5OfQ7gv62JTkYEf-nGzHccpE--etSmXRMZPUXHjELaeE7ogtTouO6mbNmy0nwuuORwI_jRKxvwWILPqIBd5x0eOAlbA0gYxHYMdxkm9bXJJRg5SVvvXCMSAvt7wr12vYacYvAZXpIVSufC_fUL8qLMRcMJYnffDL8Mf-1GSNBJSX9GK20wjG0xR9_GD81R8lPsRpp_urPbIWUkdgyhk6qklq9BlG74cegCQ2SWWjgHS956VcwBcvgbXihRxtuglo-741GmE4r9hnnUm78AVcFCMEX8nybxKKvw43_gyfLxCsC94YsKyM7YVbglCR-1Ln_24uxw1bAKCUoJMY4WKoOjnT_kgxXayql8EJqeAezpPWa5cNuZLA2NHjqBgIWMk-7Js_2nu4MpVvUiimAukSv8XsfdoG0gIVKC6USa7wsUlAgpCEqYEYFH74F0Nr5xfOdGV3GkN4wKxeQKrT27yL6Kf9Qhnr4hmcOpS9mJ22NQYmkxVouWMGXagW11UBNXU1b4LchW6OtQC3Df8jkfIuTXZ9T4z5jfUL7bPeFLfB-fJe_VBw2fMp1uv2j94mIY9Gzjx6IJYYc1lWlLyys0zxkn51Ln-z39cvwsR-onIQBtgHRdC1vbVOsv=w1522-h544-no" alt=""></p></li></ul><h1>後記</h1><ul><li>Google Trends 提供我們的資料是近一個月，每天 20 個熱門關鍵字，沒有提供更多的資料了!<br>(如果一開始有注意到的話這篇文章可能就不會誕生了XDDD)</li><li>有需要的人可以在這邊線上執行/下載程式：<a href="https://colab.research.google.com/drive/13Ssk8v_eWF-EwNFUuwjAySg2HBld6i5s" target="_blank" rel="noopener">GoogleTrends.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Web Crawler </tag>
            
            <tag> Google Trends </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何建構代理IP的清單(Proxy Pool)</title>
      <link href="/2020/02/07/WebCrawler-ProxyPool/"/>
      <url>/2020/02/07/WebCrawler-ProxyPool/</url>
      <content type="html"><![CDATA[<p>當我們在進行網路爬蟲時，或多或少都會碰到網站設置的反爬蟲機制，其中簡單的是檢查headers(瀏覽器參數)，複雜些的則會「鎖定大量訪問的IP」、「圖形驗證碼」、「登入」…等等，不過既然有「反爬蟲機制」，當然也就會有「反反爬蟲機制」囉！</p><p>今天我要介紹的就是如何建構代理IP的清單( Proxy Pool )，透過代理IP來爬資料我們就可以避開「鎖定大量訪問的IP」的反爬蟲機制！</p><a id="more"></a><h1>文章架構</h1><ul><li>尋找提供免費的代理IP的網站</li><li>抓取代理IP清單</li><li>測試IP的有效性並建構 IP 清單</li><li>後記</li></ul><h1>尋找提供免費的代理IP的網站</h1><ul><li><p>網路上提供免費代理IP的網站有很多，我們簡單在 Google 上搜索「免費代理IP」後就可以發現許多網站了，我找到的網站並經過測試還不錯的網站分別是 <a href="http://free-proxy.cz/zh/proxylist/country/US/https/ping/all" target="_blank" rel="noopener">FREE-PROXY</a> 與 <a href="https://www.us-proxy.org/" target="_blank" rel="noopener">US-PROXY</a></p><blockquote><p>提供一個使用心得是，在使用時要盡量避免使用位在中國的代理IP，會這樣建議的原因是大陸有防火牆的限制，訪問國外的網站時容易被阻擋或比較不方便，為了避免這些問題，建議直接找美國的代理IP來使用！</p></blockquote></li><li><p><a href="http://free-proxy.cz/zh/proxylist/country/US/https/ping/all" target="_blank" rel="noopener">free-proxy網站畫面</a><br><img src="https://lh3.googleusercontent.com/XpGOGzFPJSsJCsI-MzGD_ORk0vn5NwA87omrsJVE_8HVRYoG87U3zoM-XKhmFOxIpEZQwmiBbBXNpgxwNv0tLixnBEU-Bo-h4E0Ok34onfxN9oy5O7DXxQ9nWhRaOb8ci1XvUPzE-ClXI2M-JPP1EgihddnAgri9Pb63_226MgvgsgQuyTnMZfWurLu196MauyPrMX8AcKTjQuapdowKcnQEdY3zvWIDWUuNrQLvFE3G46ZuVaseGforOsmMEW1pfmilDlHBNuOxwDcVQIgkN4HQ5Jk8jCulT3dIt9d3FuojR_pYV75193kNoMtjAddILBrLIJM7MfL9sdfWhSsE-51DF6rvZCitwTpLfBgtnwtcQeJ62fqDQ-xU9Fn6PX5xYEsb0SZ-wlQurHnO8tPGjaD7yhvp-0C1e2Tx42vfSj-Y7vRQ3iL-d4rz35dkhW4K5Qom0scT2g7MY8ps5M6mSZwamcp64k0h_7hyPaI3rmN3ripo5J3KV7pVioeCdAjsoIhHiGc3fbp6Jg6RjhfGA6OfDq_vWdU3w2M8weE6wzYGnyhBDJR8jR3EgqXgDiwd0XVYfIzOtjMrd4Ee4pRSwjp-8IXFNKmivla0firDAV66rZjhR6r1GEGL98QLQKbYqrtOWeUMS0c-4yzmuPDd2N1n97AgKWHIKmQz4VexSa3D9e8yvN5-gnd8xrO4=s1843-w1843-h978-no" alt=""></p></li><li><p><a href="https://www.us-proxy.org/" target="_blank" rel="noopener">us-proxy網站畫面</a><br><img src="https://lh3.googleusercontent.com/HF1yxLSPCmhAmznlUHSYZvtCSJxq3j1iFUCkUAcV-bwrg68VdWjTJ8-yftiPk6ORvTRLqPXcc5OLZlIrazmEbn7xanaR-rThHD3FLGbyicPlzrQFPyJdSDp4MKxs7SFnQUYMPkZLgmtUvxhqUjZ3L4tDkgS8SsGlFmyRctMhDswo54WVildO-8uKLSZqr4LfADtvcHtZyN3wCN4lMCve2HRQNJ0gXksenMO1zEZRrkVQRn-JBGoKVWfJr-hWyRbHW8Y5FJ-lYw4jFBsh-BuSMMOMxnY8gopsKMKMuXuRawq8dlyXcr51XE5oMdgET-NAYeOHDiIPoPnyfzSUgYyGxTk09YmJaxwEa9vizZbUxGFuPjQbjG2o8HEMTbrJvRjy2t4nb8tmZo_TV-poF02atWOgOD_0crjuNh6KN-EDZQtIzngiUIKYiFymIJVDqNttoxrP4ZrD0nradgiZyRizE2LM_vqvxlcNi4Am99QqXQXIkOCwkwXu_GVnORYQDKR-cyBLjxAEEnAfxtkN1eJkw_AUYJMc3USJ9OjssS-fb6LuW0fz_NoDPt2nvY_sQgGgYWV2JvYzRko1xhyTX6VXIcT0FtALdSTI-0AhmOmYK2qnJYgGu2ptcgU_cMGP96_JddKo3X7U9nCQv0W-LpYNmLCbUmdp2sgkHDN-3xUpX_CaMyLfIBqlWSNRR4PH=s1847-w1847-h978-no" alt=""></p></li><li><p>在網站的畫面中我們會看到表格中有許多的 IP地址 和 通道(Port)，稍後我們就是要抓取表格中的這兩個資訊！</p></li></ul><h1>抓取代理IP清單</h1><ul><li><p>在這邊測試的網站是 <a href="http://free-proxy.cz/zh/proxylist/country/US/https/ping/all" target="_blank" rel="noopener">FREE-PROXY</a>，考量讓大家能方便執行這個程式，我是在 Google 的 <a href="https://colab.research.google.com/notebooks/intro.ipynb" target="_blank" rel="noopener">Colab</a> 上進行開發，文末也會附上完整的程式，在此大家可以先看流程即可！</p></li><li><p>安裝與載入套件</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">!apt-get update</span><br><span class="line">!apt install chromium-chromedriver</span><br><span class="line">!pip install selenium</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br></pre></td></tr></table></figure></li><li><p>設置Selenium參數</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(<span class="string">'--headless'</span>)</span><br><span class="line">options.add_argument(<span class="string">'--no-sandbox'</span>)</span><br><span class="line">options.add_argument(<span class="string">'--disable-dev-shm-usage'</span>)</span><br><span class="line">options.add_argument(<span class="string">"--disable-notifications"</span>)</span><br><span class="line">driver = webdriver.Chrome(options=options)</span><br></pre></td></tr></table></figure></li><li><p>收集IP清單</p><ul><li>在此先用迴圈的方式讀取1-5個分頁表格中的IP</li><li>由於IP的格式是由四組1-3個數字組合而成，並於中間用「.」隔開，因此可以透過正則表達式來抓取資料</li><li>具體的語法如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">IPPool = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    <span class="comment"># 用迴圈逐一打開分頁</span></span><br><span class="line">    url = <span class="string">'http://free-proxy.cz/zh/proxylist/country/US/https/ping/all/&#123;&#125;'</span>.format(i)</span><br><span class="line">    print(<span class="string">'Dealing with &#123;&#125;'</span>.format(url))</span><br><span class="line">    driver.get(url)</span><br><span class="line">    soup = BeautifulSoup(driver.page_source)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> soup.select(<span class="string">'tbody &gt; tr'</span>):</span><br><span class="line">        <span class="comment"># 用正則表達式抓取IP</span></span><br><span class="line">        <span class="keyword">if</span> re.findall(<span class="string">'[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;'</span>, str(j)):</span><br><span class="line">            IP = re.findall(<span class="string">'[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;'</span>, str(j))[<span class="number">0</span>]</span><br><span class="line">            Port = re.findall(<span class="string">'class="fport" style=""&gt;(.*?)&lt;/span&gt;'</span>, str(j))[<span class="number">0</span>]</span><br><span class="line">            IPPool.append(pd.DataFrame([&#123;<span class="string">'IP'</span>:IP, <span class="string">'Port'</span>:Port&#125;]))</span><br><span class="line">    print(<span class="string">'There are &#123;&#125; IPs in Pool'</span>.format(len(IPPool)))</span><br><span class="line">IPPool = pd.concat(IPPool, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">IPPool</span><br></pre></td></tr></table></figure></li></ul></li><li><p>執行完程式會看到以下的畫面<br><img src="https://lh3.googleusercontent.com/CGeuFhe8iXsa5pjHhebVPdhvuXLxFZEME5K55ltXIrW7R0cs_5vNtqLOyFAodq3xKjnQ2Jru9e8LyUwOEdL0HB2EInDmFSmINibDbxAqDYLAxrVFkC5VQE0oHn4YGnG_E_dtiilFno68oAUETCGl5fvA3_w0tGc_xYmZt63w99k43z3_ezcdVtI00NIA1eWo5NciUw1ZTyGP9P5Booa20aesNixxjxelc7mvo85Piou_trbzxiZXF9kyeeo6MNZEMHZrmSViih8gtlddX2cX8sHgjWK75M1-JFtNfIHYzKIy9ixVbSdk4BKgIMNZSFEKYA3pgkxlAeVevp3Kb5DcdZ685Lql2hIbM8k1ITlMeZjPrDMMIX4wlr51RvgGbwa5ExjU4oJRPVSULgXqZQm6ZETPzh7w8cgFgFhxhMQoUKhh6g4hbH_aLFYxRaogGOBgClRHzvyf7_BG2mS5OkR1nRsoaUG_IQeS08jA_UvwAozD61Qr8FVwo4MgES4XCx7TJWyfkHWFfqaKwFDs2Z1yTjp8CNnTMqf32_agdCp_Tl5wetd3oWePWmCirxvkYbizsTf3Mdh4tjYsV7la2mVeHAhrlLZPxaYcIw9TGC4G9NuD5ro5AmAankdTleDbhHnobISJSac3e9D-XENBbUFmjw0gG5_baxw9TquXqvwXLb6Lh3yVKar3ZyY=w702-h588-no" alt=""></p></li></ul><h1>測試代理IP的有效性</h1><ul><li><p>雖然看起來我們好像拿到 104 個 IP 了，但別高興得太早，因為當中絕大多數的IP都無法使用!!(畢竟來源是免費的xDD</p></li><li><p>因此我們在這裡需要再多一個步驟用來確認這些IP的有效性，而測試的方式就是我們在 request 的時候透過這些IP和Port來訪問網站，當我們訪問成功後我們才把IP保留下來，反之則丟掉</p></li><li><p>具體的語法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ActIps = []</span><br><span class="line">  <span class="keyword">for</span> IP, Port <span class="keyword">in</span> zip(IPPool[<span class="string">'IP'</span>],IPPool[<span class="string">'Port'</span>]):</span><br><span class="line">      proxy = &#123;<span class="string">'http'</span>:<span class="string">'http://'</span>+ IP + <span class="string">':'</span> + Port,</span><br><span class="line">               <span class="string">'https'</span>:<span class="string">'https://'</span>+ IP + <span class="string">':'</span> + Port&#125; </span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          <span class="comment"># 隨機找的一篇新聞即可</span></span><br><span class="line">          url = <span class="string">'https://www.chinatimes.com/realtimenews/20200205004069-260408'</span></span><br><span class="line">          resp = requests.get(url, proxies=proxy, timeout=<span class="number">2</span>)</span><br><span class="line">          <span class="keyword">if</span> str(resp.status_code) == <span class="string">'200'</span>:</span><br><span class="line">              ActIps.append(pd.DataFrame([&#123;<span class="string">'IP'</span>:IP, <span class="string">'Port'</span>:Port&#125;]))</span><br><span class="line">              print(<span class="string">'Succed: &#123;&#125;:&#123;&#125;'</span>.format(IP, Port))</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">              print(<span class="string">'Failed: &#123;&#125;:&#123;&#125;'</span>.format(IP, Port))</span><br><span class="line">      <span class="keyword">except</span>:</span><br><span class="line">              print(<span class="string">'Failed: &#123;&#125;:&#123;&#125;'</span>.format(IP, Port))</span><br><span class="line">  ActIps = pd.concat(ActIps, ignore_index=<span class="keyword">True</span>)</span><br><span class="line">  ActIps</span><br></pre></td></tr></table></figure></li><li><p>執行完會看到如下畫面，如畫面中的內容，大多數都是無效的IP，最後我們就從 104 個 IP 中找到 11 個有效的 IP 囉!<br><img src="https://lh3.googleusercontent.com/mpJ3CTXL5OYoijGnd95bm8wFJut98UpcDT_gvwSuVrs9-ydMWReaKtgZZ-vza355xH7SndLwguEMpUite5HkGrUx-WxZPTzfIElrDmEAvyDvGQH8EdOCCDhgzKypMwX_2W_o9v-IMGoV3xooKL3QA-hII3HO-iO88n9ZprIrEdg7ib0oWAKk668XfTHbo0DAWGz2p7DrAJ9tMDzzHvc7tzhcRnRr7QU3caDLaqk6PeSwOppEyl-fRYDKiFrzD5HEzj9GFISTDsHoi5GX4cJBgCi4PvOgN1aCJNcjqlGn6Pn-YEuc2d6pYOX6YEWem_BbaLux-qEKRxtBM6AG9oCGcQVw8VUH1tP4H9cSceHtDoQTQgIqoNrepE-CJTXWX64bpayXQE1umVpWheXIeA_zbHl8YC0rs8x52HEkBrZoIoQ3ZisQF-JS2hQstQXrDFef8hOtTZtKOXk3fh-ihIzR-QGSFXhkJzdw4zhrjo9Vxp_mJzmgL5Az8il9KFsY0kbLhJAY4G9LhR6hkMvE_1NsPgdxwLUyAzwZzlTs3CagQppyxehkJCx67SzWn2L3VGbshqSC9q8_OvZbc-IRpyvgjdH9mCBe_eFb4f90KyUByZDudiviIb00qInmNFfjPaP3Q0QJuQ30QyvIeXGC2T7LDLIGlOPisbxlsEfYOOlg1vbRnoL2k4UI2l8=w394-h820-no" alt=""></p></li></ul><h1>後續</h1><ul><li>之前在透過 Facebook 的 graphql/api 爬貼文時，會發現每個 IP 大約開啟 1,000 則貼文後就會被封鎖 IP 了。</li><li>當資料量小的時候，我們當然可以透過手動的 開啟/關閉 手機的飛航模式，讓手機重新抓基地台的訊號，藉此達到替換 IP 的目的，但如果我們需要抓上萬則留言的話，就會需要透過代理 IP 的方式來爬資料了!</li><li>有問題歡迎在底下留言，而程式可以直接在這邊執行/下載：<a href="https://colab.research.google.com/drive/1PgALsr6iArpUE4NHt4GJw2TkiGH7we-m" target="_blank" rel="noopener">GetProxy.ipynb</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Proxy pool </tag>
            
            <tag> Colab </tag>
            
            <tag> 代理IP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>中文斷詞的新詞偵測技術</title>
      <link href="/2020/01/05/NewWordDetection/"/>
      <url>/2020/01/05/NewWordDetection/</url>
      <content type="html"><![CDATA[<p>談到自然語言處理(NLP)的中文斷詞技術，我們通常都會直接聯想並使用 Jieba 套件，網路上也有許多人分享了 Jieba 套件的功能介紹與使用方法。但是當我在使用是卻感到非常困惑，困惑的地方在於，隨著時代的演進會不斷產生新的詞彙，那麼我們透過「過去定義好的字典，適合用來分析未來的資料嗎?」</p><a id="more"></a><h1>文章架構</h1><ul><li>一定要新詞偵測嗎?</li><li>資料來源與說明</li><li>資料前處理</li><li>新詞偵測</li><li>展示成果</li></ul><h1>一定要新詞偵測嗎?</h1><p>在做文字探勘的分析時，首先的任務就是需要將非結構化的文字資料整理成結構化的形態，其中英文因為有空格作為字跟字之間的界限，因此可以很輕鬆的進行斷字。在中文的文章中雖然有標點符號可以切分成不同的句子，但句子內並沒有空格能把句子斷成不同的詞，因此怎麼把句子斷成詞就是一門很大的學問。</p><p>Jieba 就是幫我們把句子斷詞的套件，而斷詞的方法是透過事先定義的詞典來匹配文章，因此詞典的好壞很直接的影響了斷詞的成果，我們就直接來看看 Jieba 預設的斷詞結果吧!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">sent = <span class="string">'蔡英文強調，希望其他產業也能台積電一樣，把台灣當作研發與生產的基地'</span></span><br><span class="line"><span class="string">' / '</span>.join(jieba.lcut(sent))</span><br></pre></td></tr></table></figure><blockquote><p>‘蔡 / 英文 / 強調 / ， / 希望 / 其他 / 產業 / 也 / 能 / 台積 / 電一樣 / ， / 把 / 台灣 / 當作 / 研發 / 與 / 生產 / 的 / 基地’</p></blockquote><p>如上所示，原本是「蔡英文」被誤斷成「蔡」和「英文」，可想而知，這會讓我們在後續的分析成果中出現英文時沒辦法區辨出到底指的是語言的「英文」，還是總統的蔡「英文」。同樣的「台積電」也被斷成了「台積」和「電一樣」(?)</p><p>我們再多看幾個例子就會發現，斷詞的結果仍有許多可以再改善的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sent = <span class="string">'法人預期，本季台積電營運表現可望淡季不淡，預估比去年第四季的營收高峰季減在5%以內。'</span></span><br><span class="line"><span class="string">' / '</span>.join(jieba.lcut(sent))</span><br></pre></td></tr></table></figure><blockquote><p>‘法人 / 預期 / ， / 本季 / 台積 / 電營 / 運表現 / 可望 / 淡季 / 不淡 / ， / 預估 / 比 / 去年 / 第四季 / 的 / 營收 / 高峰 / 季減 / 在 / 5% / 以內 / 。’</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sent = <span class="string">'法人預估台積公司今年營運將呈現高成長，營收將年成長15%至20%，並續創歷史新高紀錄。'</span></span><br><span class="line"><span class="string">' / '</span>.join(jieba.lcut(sent))</span><br></pre></td></tr></table></figure><blockquote><p>‘法人 / 預估 / 台積 / 公司 / 今年 / 營運將 / 呈現 / 高成 / 長 / ， / 營收 / 將年 / 成長 / 15% / 至 / 20% / ， / 並續 / 創歷史 / 新高 / 紀錄 / 。’</p></blockquote><p>如果我們想要透過文字探勘來獲得有價值的資訊，那一定需要立基在好的斷詞結果之上，而 Jieba 則是透過比對詞典的方式來進行斷詞的。但是除了最一開始曾提到「詞」會有時間的影響因素之外，在不同的產業、場域(現實/網路社群…等等)也都會有各自的專有名詞，而且這些因素彼此間還會彼此交互影響，這些因素都會讓情況變得相當複雜。</p><p>那麼字典到底要怎麼新增與維護呢?最直接的方式當然就是透過人工的方式維護字典，例如當我們看到「蔡英文」不在詞庫中時，我們就把這個詞增加進字典中，透過不斷地循環來增加詞典的豐富程度，但是這樣的缺點是需要花費大量的人力成本。而看完這篇文章的你們，以後就可以透過統計的方式來幫助我們找出那些字可以組合成詞，讓我們可以輸入透過大量的語料庫，就自動得到一份專屬的詞典!</p><h1>資料來源與說明</h1><h2 id="資料來源"><a class="header-anchor" href="#資料來源">¶</a>資料來源</h2><p>收集資料最快的方式當然就是透過網路爬蟲的技術在網路上收集資料囉! 這篇文章中的範例資料是我在 <a href="https://www.chinatimes.com/" target="_blank" rel="noopener">中時新聞網</a> 上爬取有關台積電的新聞資料，有興趣的人可以點擊此處直接下載 <a href="https://drive.google.com/file/d/1rw0QrW6_LnNM6trH1p7tbLpBx-cmmycd/view?usp=sharing" target="_blank" rel="noopener">台積電新聞資料</a> 。至於爬蟲的方法或想要爬其他關鍵詞新聞的人請參考 <a href="https://tlyu0419.github.io/2019/10/30/Crawl-ChinaTimes/">如何透過爬蟲抓取中時新聞</a> 這篇文章。</p><h2 id="資料說明"><a class="header-anchor" href="#資料說明">¶</a>資料說明</h2><p>基本上我把截至昨天為止在中時新聞網上面有關台積電的新聞都抓下來了，時間範圍是 2009年11月 至 2020年1月 的資料，共計 15,455 則新聞，包含以下欄位資訊</p><ul><li>TITLE：新聞標題</li><li>TIME：新聞發佈時間</li><li>CATEGORY：新聞類型</li><li>DESCRIPTION：新聞摘要</li><li>CONTENT：新聞內文</li><li>KEYWORDS：新聞關鍵詞</li><li>FROM：新聞網站</li><li>LINK：新聞網址</li></ul><p><img src="https://lh3.googleusercontent.com/QrO9iVtj3wRRD5XdFthCmWSBgt9M6RL8F7Vybl5znE51klcQ76gXvCIXPXjVVawWFEdOkqrTXrFQgrP3Elaxsu6AFrdP6i9EHlod0cfMWvfZ4074I6Y72c_CL6b50EnezeG1HhoB7EiXjJYfUgS1Q-KOAL1V0n3g_QCb4IA6n2BmvchzK3PfIvj1GEOfouEPNqnx_cXNg6WD0gQsoMd6eQ8FhPtGuNIrpQk3xaPbBYrGf-eePIu1VdfVxZCt8-JvfI4W3k6fURWm_1l9oYqCraa4kyGc42hIT3ZgqunP88RhQixMiqncpl30Wtitsf8e_gTRvee6KRO-ZlnNkPd3XA6UjL70yf_GJfzO7Elzfv3bo4Augg-VKeEOI36Q8zvNwTshZHFH1eXsCXhOAZNBUJoCe5M3sMa1XPGGTevder5rvAgbiJVyskWbQvTtzRHlcH_Zs3SM75Vdhwoc6DjExPl-OKqNCZrP5W21dKAF0ztB-ANhDvyfvOfOhj0Snf7Jqma3ZS1LHf-68BmLbYVJCvhjwSrBwO1Q2UfuMgYm78CY2zOM2SZzq2m9o3vwZT2JF42jjSQpSVwM_1twCoNAKU_3eQomMDZ9Exe1Z2HYUn4eF_f-fxRBskgg53Dql9xOgmRKORdsaO3Na0FImqGVi4Sln3JNxDRAgVqPrVwYEVfEWo6klfNdnuGnb6mL=s1739-w1739-h978-no" alt=""></p><h1>資料前處理</h1><h2 id="只保留中文字"><a class="header-anchor" href="#只保留中文字">¶</a>只保留中文字</h2><p>因為這裡是想要找出中文的新詞，因此在這裡先將新聞內文中的英文、數字、標點符號、emoji…等等的資料濾除，讓我們的語料庫只單純的保留下中文的文字，而中文字在 Unicode 的編碼落在「\u4e00」與「\u9fa5」之間，因此我們在這裡透過以下的正則表達式來保留我們需要的中文資料</p><h2 id="按照年月串接新聞"><a class="header-anchor" href="#按照年月串接新聞">¶</a>按照年月串接新聞</h2><p>由於在新詞偵測時會需要使用到大量的運算資源，考量記憶體的負擔與運算時間的原因，在這裡我將新聞按照年月拆分，並將相同年月的新聞串接起來當做同一篇文章進行分析。因此在後續新詞偵測的語料庫中，共有 123 篇文章(123個月份的資料)</p><h1>新詞偵測演算法</h1><p>這裡使用的演算法的概念、說明參考自 <a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="noopener">互联网时代的社会语言学：基于SNS的文本数据挖掘</a>，說明如下</p><h2 id="提取所有候選詞"><a class="header-anchor" href="#提取所有候選詞">¶</a>提取所有候選詞</h2><ul><li>我們的目的是希望在沒有預先定義好字典的情況下，透過統計的方法得到一份字典，因此會先透過窮舉的方式列出所有可能的詞。通常會先設定詞最長的長度(建議設定為6)後再進行搜索。</li><li>以「天氣預報說周五會下雨」為例<ul><li>長度2： 天氣 / 氣預 / 預報 / 報說 / 說周 / …</li><li>長度3： 天氣預 / 氣預報 / 預報說 / 報說周 / …</li><li>長度4： 天氣預報 / 氣預報說 / 預報說周 / 報說周五 / …</li><li>…</li></ul></li></ul><h2 id="檢測指標1：詞頻"><a class="header-anchor" href="#檢測指標1：詞頻">¶</a>檢測指標1：詞頻</h2><ul><li>如同以上的範例，透過窮舉的方式我們當然可以把所有的詞都找出來，但同時也會找到許多錯誤的詞，如長度2中的「氣預」、「報說」、「說周」等等，因此我們可以透過統計詞頻的方式作為第一個檢測指標。</li><li>可以預見的是，當我們文章的長度夠長時，「正確的詞」出現的次數會遠高於「錯誤的詞」，因此我們可以將出現次數太少的詞濾除。</li></ul><h2 id="檢測指標2：內部凝固度-pmi"><a class="header-anchor" href="#檢測指標2：內部凝固度-pmi">¶</a>檢測指標2：內部凝固度(PMI)</h2><blockquote><ul><li>以「電影院」為例，如果「電影」和「院」兩個詞是完全獨立的詞的話 「電影院」在文章中的出現機率，應該會與 出現「電影」的機率和 出現「院」的機率乘積的值 兩者會接近，這時候表示兩者可能是偶然拼接到一起的。</li><li>但如果「電影院」出現機率遠高於「電影」的機率 乘上 「院」的機率的話，表示兩個詞並不獨立，也就是這兩個詞可能是一個詞。</li></ul></blockquote><h2 id="檢測指標3：外部自由度-entropy"><a class="header-anchor" href="#檢測指標3：外部自由度-entropy">¶</a>檢測指標3：外部自由度(Entropy)</h2><blockquote><ul><li>除了看內部內部凝固度之外，還需要進一步看外部的表現。如果一個文本片段能夠算作一個詞的話，它應該能夠靈活地出現在各種不同的環境中，具有非常豐富的左鄰字集合和右鄰字集合。</li><li>以「被子」和「輩子」為例，我們可以說「買被子」、「蓋被子」、「進被子」、「好被子」、「這被子」等等，在「被子」前面加各種字；但「輩子」的用法卻非常固定，除了「一輩子」、「這輩子」、「上輩子」、「下輩子」，基本上「輩子」前面不能加別的字了。「輩子」這個文本片段前面可以出現的字太有限，以至於直覺上我們可能會認為，「輩子」並不單獨成詞，真正成詞的其實是「一輩子」、「這輩子」之類的整體。</li></ul></blockquote><h2 id="綜合評分"><a class="header-anchor" href="#綜合評分">¶</a>綜合評分</h2><blockquote><ul><li>這裡透過簡單的加總凝固度和自由度的得分來幫各個詞評分，接著我們就可以按照這個評分對資料集排序，並且自行決定合適的閾值(Threshold)，來選擇要選幾分以上的詞作為我們的專屬字典。</li><li>實際在使用時，可以再自行調整這個綜合評分應該如何加權計算。</li></ul></blockquote><h1>新詞偵測實作</h1><ul><li><p>在網路上已經有人將以上的演算法轉換成具體的程式碼，可以參考以下網站 <a href="https://zhuanlan.zhihu.com/p/45745963" target="_blank" rel="noopener">python简单实现新词发现</a> ，我在使用時僅有稍微調整這份程式碼就可以使用了。</p></li><li><p>以下我們分別列出分數最高與最低的 50個詞彙，要強調的是這是我們沒有預先定義好字典的情況下透過統計方法自動計算得出的!</p><ul><li>最高分的 50個詞<blockquote><p>醞釀 / 泡沫 / 犧牲 / 垃圾 / 槓桿 / 貢獻 / 余湘 / 蟄伏 / 駕駛 / 妙禪 / 禿鷹 / 夥伴 / 烘焙 / 泡麵 / 苗栗 / 辣椒 / 邏輯 / 肌肉 / 呼籲 / 野村 / 尺寸 / 犯罪 / 挖礦 / 漏洞 / 麒麟 / 蓬勃 / 敬鵬 / 古蹟 / 咖啡 / 徘徊 / 干擾 / 梅姬 / 螞蟻 / 卜蜂 / 硫酸 / 搜索 / 什麼 / 灌溉 / 鳳凰 / 侵蝕 / 陰霾 / 掌握 / 餐飲 / 烘托 / 紓困 / 瑕疵 / 霹靂 / 暑假 / 魔咒 / 雜誌</p></blockquote></li><li>最低分的 50個詞<blockquote><p>復五日線 / 轉機題 / 逆勢收 / 能源教育 / 規劃未 / 中東緊張 / 負債表 / 險試產 / 目前多方取 / 決議每 / 台北時間 / 受制歐美 / 關鍵地位 / 也會是不錯 / 等類股拉回 / 多芬第九號 / 開發案 / 信心指數 / 去年度財報 / 長黎方 / 全年毛 / 統一及長榮 / 兩地 / 貿易戰休兵 / 低接 / 除息前 / 貨市場上則 / 統一超 / 後市不 / 鎳價 / 主要受惠 / 李秀利表示 / 度因工廠而 / 援極紫外 / 與股后方 / 上市櫃市值 / 外銷訂 / 年以來 / 交大 / 創歷年同 / 鞋與 / 本周納入 / 魏哲家表示 / 受惠於國際 / 無限可能 / 果砍 / 重啟協 / 運底部已 / 小幅成長 / 統初選</p></blockquote></li></ul></li><li><p>觀察一下應該就會發現高分的詞的結果都很符合我們的常識與邏輯，至於原本被錯誤斷詞的「蔡英文」與「台積電」也有成功斷出來(只是藏在後面一些些的排序)；而最低分的 50 個詞中則出現許多不合理的詞彙。我們可以把分數當做是否成詞的信心程度，在使用時還需要選擇一個合適閾值，並保留閾值以上的詞彙來當做字典。</p></li></ul><h1>年度關鍵詞</h1><p>以下是我透過 tf-idf 方法計算得出的年度關鍵字，如果對於 tf-idf 還不熟悉的人可以參考 <a href="https://zhuanlan.zhihu.com/p/31197209" target="_blank" rel="noopener">机器学习：生动理解TF-IDF算法</a>，考量篇幅的原因我就不對資料預處理的過程進行說明了</p><ul><li>2020<blockquote><p>表示、去年、產能、資金、開高、不過、營收、第一、走高、晶片、投顧、高點、華為、貿易、處理器、加上、全球、美元、法人、預期、台灣、設備、表現、三星、帶動、美國、買超、產業、早盤、技術、今年、指出、行情、成長、漲幅、電子、製程、投資、半導體、億元、大漲、新高、持續、市場、奈米、指數、外資、股價、台股、台積電</p></blockquote></li><li>2019<blockquote><p>客戶、去年、漲幅、上漲、以及、明年、貿易戰、營運、美股、帶動、全球、加上、震盪、投資、影響、大立光、蘋果、資金、產業、晶片、指出、目前、美元、公司、台灣、製程、技術、法人、表示、預期、成長、電子、持續、半導體、表現、新高、美國、華為、買超、今年、早盤、營收、奈米、市場、億元、指數、外資、股價、台股、台積電</p></blockquote></li><li>2018<blockquote><p>美股、大陸、全球、股王、賣超、產業、獲利、投資、去年、半導體、震盪、下跌、買超、蘋果、影響、開高、指出、族群、權值、鴻海、預期、奈米、加上、早盤、股票、上漲、新高、美元、公司、大立光、暫報、表示、目前、美國、方面、台灣、成長、法人、持續、電子、表現、營收、今年、外資、股價、指數、市場、億元、台股、台積電</p></blockquote></li><li>2017<blockquote><p>企業、帶動、國際、今日、賣超、產業、整理、影響、漲幅、獲利、奈米、預期、開高、加上、半導體、震盪、族群、暫報、美元、投資、指出、買超、上漲、權值、公司、目前、蘋果、美國、萬點、股票、成長、法人、持續、表示、方面、台灣、鴻海、大立光、新高、營收、電子、表現、今年、股價、外資、市場、指數、億元、台股、台積電</p></blockquote></li><li>2016<blockquote><p>族群、震盪、經濟、大陸、加上、可望、整理、全球、獲利、資金、奈米、蘋果、今日、投資、去年、漲幅、賣超、權值、美元、上漲、金融、國際、公司、鴻海、暫報、美國、股票、指出、預期、目前、台灣、買超、持續、大立光、方面、成長、表示、新高、法人、表現、電子、營收、股價、今年、指數、外資、市場、億元、台股、台積電</p></blockquote></li></ul><p>雖然還有2009-2016的資料，但就不在這裡繼續列出來了。從結果來看2019年爆發的美中貿易戰確實有被偵測出來。同理，我們可以透過分析這些關鍵字來了解台積電在各個年度中發生了些什麼事情。或者藉以監測公司的營運狀況、風險…等等。</p><p>今天的分享到這邊，我將台積電的新聞放入這份新詞偵測的演算法後，得到的結果如下，有需要這份字典的人也可以在這裡下載 <a href="https://drive.google.com/file/d/1jHyeq6jBQp3YMdNcbWZ2Kk2-RpCkvvfE/view?usp=sharing" target="_blank" rel="noopener">台積電專屬字典.xlsx</a><br><img src="https://lh3.googleusercontent.com/PQF2PmC-umwOwqeOuZfEg8tNyBFvTw2ibXq7B0JsE6VPYQFbUW2phD-oG2GiDYE5JFIjmwaku4NMKCBx_tPh6BaqjJTzTupcCVL_J90zG8-eiEUdc1mDEokcyVtpeLRkNXvR2fFfb0iOZQ25Fw2grPsl5EoXv50EZlvyUZziX-xDfLefyBq6oN_r1IZVQwoDErjelva5OI1yCrc8kNtRWHVncuSKaaNtkSHLeL-mNuj7qjAg9Xhnp4_ovSXOIpYGHw7xouEsyYL0xS_2j_pYRUAHvmhiwU9hbij8mlqIXAeGUpvjECpVAFgGdywjnmrbqg6WrTxmPY8z_IdEScKVJ0sCBDBApn6RUBXxQSrLwr6j4R6-c0awNKnzi04gDssFV0RkIJueZmRNbAAUhGpnCW4ZGJLezFyNtLdfUhmyS2NsevOSdPKGyQLvngcde2zPCHNOqi9beLUF1UkFaVoeVJPbfRsM3XFxRuY9NKdnM1No_PA2AMypeOKsmnxviRH193T43GLnBHZ5d8CQw7yjzKdJv2eOJzeqzS1DZtwvSoGhsaUYCZqDdXC1xnLtpJwJcbXxfXsxdT508IORqnHHEuPrNIRXd4O2W7ZRUHG8fIeSLoe_A7m2bOiwv1ybMekCw0FV-YUZKSJ3_XUQn0TNFZIIZLuSTBGOwV4R9JNuCMrX1qekELJP7ro=w1056-h978-no" alt=""></p><h1>附件</h1><ul><li><a href="https://drive.google.com/file/d/1rw0QrW6_LnNM6trH1p7tbLpBx-cmmycd/view?usp=sharing" target="_blank" rel="noopener">台積電新聞資料_2009-2020</a></li><li><a href="https://drive.google.com/file/d/1jHyeq6jBQp3YMdNcbWZ2Kk2-RpCkvvfE/view?usp=sharing" target="_blank" rel="noopener">台積電專屬字典.xlsx</a></li></ul>]]></content>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 新詞偵測 </tag>
            
            <tag> 中文斷詞技術 </tag>
            
            <tag> Entropy </tag>
            
            <tag> PMI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>透過網路爬蟲抓取1111人力銀行的公司資料</title>
      <link href="/2019/12/29/Crawl-CompList1111/"/>
      <url>/2019/12/29/Crawl-CompList1111/</url>
      <content type="html"><![CDATA[<p>先前曾發表過 <a href="https://tlyu0419.github.io/2019/04/18/Crawl-JobList104">爬蟲_104人力銀行工作清單</a> 的爬蟲貼文。當時是透過 Selenium 來抓取104人力銀行的「職缺」清單，而這篇文章則嘗試直接用 requests 爬取 1111人力銀行 在全台的「公司」清單，包含了公司的名稱、統編、資本額、簡介、職缺數量…等等內容。最終抓到 1.1萬筆公司的訊息，並且我會將程式碼與抓到的資料放在文末的連結，有需要的人可以自行取用。</p><a id="more"></a><h1>最終成果</h1><ul><li>按照慣例我們直接先看最後的成果，這樣會讓我們更清楚的知道要做些什麼</li><li>目標是將左邊的公司清單的資訊轉換成右邊的表格，具體會包含以下資訊<ul><li>編號(1111人力銀行給這間公司的編號)</li><li>公司名稱</li><li>關鍵詞</li><li>公司簡介</li><li>產品服務</li><li>法定福利</li><li>公司福利</li><li>職缺數量</li><li>聯絡地址</li><li>行業別</li><li>行業說明</li><li>資本額</li><li>成立時間</li><li>統一編號</li><li>員工人數</li><li>公司電話</li><li>公司傳真</li><li>網站位址</li><li>相關連結</li><li>連結網址</li><li>更新時間</li></ul></li></ul><p><img src="https://lh3.googleusercontent.com/SyD93pdVXMVzztxyDaSSpDTVH-MxGheXLvlSyKHAkoZI4pxj1WlbqLgB2Bd9IOY97MZ347L0G1zttYG4RhuhPbmKsVr0R_Q80gmCMYAxQOBAgfJQGFtjNIVKVbAgl53cunP7b3ZsXvi8SXMGUmLx836OgCtgEz0Ud09I8FY4jT6blNEaHvGR1_1KHisvgpaCsRYLPQvJuvGRpR5HOEbQ20bP431wLGVC7joJlF6jyk5EF-LK3xtHqba-2g_alqwlhOiDhXxIt23uqSGQKHE0WDLRlgoGAzkSuhBsQ1Skx20euttJBmZGDkTdzcw9KlMmRRghkIDiKHAD9TFuoZjJlhjzbGej6CcswdoIGFD14crawL9YiR6eCBajCxxdRTq44MnAphbMxb3EKKtSkZLpjTjoNdCL93vXyF5D2FAmnckepdw1x8kjc1Pf2fjYKG8auo8CcMEvrMGkutyBdaUi28rWeFZnfVDDnTstRne1CZ7JQKo6-oLWaDR80vkCvoLYUvAOSbxbKFdwcV3wTQ7s3nzBKKmLi2dA1YIgTleZw7SVpMJABc0Be-xH3XFIDEBPT_WRebTSK0M3kxsmVvLFpKIr_PKQtWpZj2rhKTMs9gAjuu2Kzwz9wv1o9t6d8dDHYDNiMpuuvl7HZiBuqRAMLXX2GvRSnNK_Z9CLGWVLuYksOtQcSZXkX6c=w1739-h978-no" alt=""></p><h1>爬蟲流程</h1><p>這次的爬蟲會經過的流程如下</p><ol><li>載入套件</li><li>偽裝成瀏覽器瀏覽資訊</li><li>收集公司清單</li><li>爬取個別公司資料</li><li>保存資料</li></ol><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p><img src="https://lh3.googleusercontent.com/1Ap_T16OA0P1CBJoC-h32JFe1MjhmsA87CV7Dn8kf2-10P2WykSP5-p3Y4_dyLiCqkGgb9Zw6RV9S88hENJF-StCrsOFKWBuIq1njBasbXj-N1Z3UeXqEUENiR1YG8S_VVmgbW4C7XMnhwaRr0Iq9JmzryW-8wP2Oi9T1_GROTLeE0ITRz1k4Fy6JbCxfSloqN5vK9Zs2Bj63oqpwqKK6eMxqEOsHgl82LhrSiRdVgW0327tHTJR5ku9ri2oY8mbxv-RFi24IsLGB7vRSF06vrKVYdRo9saC4sEV_OoSGAeojtdtPynJD5nf86b55hgzSCEmAOo_rbmbS4iYIDvKkpcqKZXqZOA70b1_lUtPoT-pDTKgklVncZT79hxGfmOLrVtdLfYMXkhiQUBt8Pzn2zvWfpV-IFiLe1YiehzfNLJtbU4yLmE6aBhy_angvYR5fb9fm0v5H2dDAgVZKttkzTtnlG76XGSr_qcVK4Rca2CxYGTRXSs4KCXMz_fWC2RwuaTK1pL-oDgsdmakO5VVvT6sruf2NekkPhlkfgHeuVR6S2ugoe1e2Nd-6Iqd-1NFFokxa1mHqguU02VtiXEYXLNTb88ROk_2TMwV8hpxG7BFlnYSg7nwNBaHrR7MdOjXL4iiLvCR9FnqTU8PoFeAlHJiThqz9ir7PGfjS9ijYvM_xpE80AfgQU4=w1820-h155-no" alt=""></p><h2 id="偽裝成瀏覽器瀏覽資訊"><a class="header-anchor" href="#偽裝成瀏覽器瀏覽資訊">¶</a>偽裝成瀏覽器瀏覽資訊</h2><ul><li>有些網站會偵測我們是使用什麼瀏覽器來查詢資料，如果是沒有瀏覽器的查詢有相當高的機會是用程式在做網路爬蟲</li><li>因此我們在爬蟲的時候建議養成加上瀏覽器資訊的好習慣!<br><img src="https://lh3.googleusercontent.com/SEzOhqCkMkmcloJ4-wirqfazDe6M-MxHx7iszsuZK3vtUJpQlJoDogH6C13XAYk_yK73jjSeIAazjat-GK557Ka1wMUflS_kxLYbuwj5ShTDPqvfUm3vmHPvVEz1j5TYwPGltgkLzwSy4FVMEBtKJ02IC6ySnLBTY1Q4HvYj8vkamR4sVq7XldEWsGAanRt-sYe1icZ0AIlyh-WNY2Kb1-OHDK05anB--HJck88PeftheLCPMxNVuniO5zN36eUC-hAh7hZ5zLiExwFb_5y6ucMkYCPq6gI0nHWNUvUtYkdk2fyeRl3xwQhcLvhKCQnXJ89rXHnOHPLG-jZ2yCzooyDyChLmGMp2_oYn_OB47smE8IdrukZoU8O_l6QLOyMgE7sQzM729BJxlMsC7aci84w9JfNgcvjxVgEofA9GOfqquBjajpSqWZBgnZLSIK0GobAYpLjD_-Qtg2SLhA-hc7cNaenUYl0-F7Xh-w7ErhwNw2Os1X1ssgFf9sW7vn2s3l1auvvexSoFx2c1to1TFqdhN1CVLP_oxRJxJwoRgZzdFt9ZBxW9YgjHI_k-jqP1ddLnF2Ulzr84v0Y9F79Pqvbzhs61CnGFvepHh-XiWHCDgXszq6eIghthqK-1dXQSIcbhlYgEIy9TqjvG-_pumihI9iNt7tx_gEkzKJ5_oaY1VLSBPFShZ_A=w1819-h65-no" alt=""></li></ul><h2 id="收集公司清單"><a class="header-anchor" href="#收集公司清單">¶</a>收集公司清單</h2><ul><li>這邊可以依需求調整要抓幾個分頁的資料，因為我想抓全台的資料，所以直接設定抓200個分頁<br><img src="https://lh3.googleusercontent.com/69HgjwzpY9cHuZass9lqLxVYS9UXcnyym-aFtOglqdHrEfuPhXdoZRXh_v127l0d2LCE0Wzi0Fjp2Ke8n6GrDSjOxLuPSaXw0zR1xyrd8nsRA2sem992NrgjwP__DxTZIfJErnMZXdSfpalYq65JWvW4ZQz_A3fTP8IUE2FLajZlKwbkoxshcvI2WtMR8r27_8VZkQAgEqOwR9Y1MVFOS5E9nKP1E0PqiT5pUZF0vydn6PXdW3eMbVIHUxpwPa1XN21jbfiuSQwlk7YTtja-I6YDFWyMDKkSRpJ4yefFFMGfefnzjzubjIbUfJ8_GzITWub27HQ2pNra4w48tuEhFOVUR5E1Yy-IeAkBa04EHP5MttYjCu2Fr3-maVpujMIE6SslLhFBFGtk7djGXEVVKUz8BMoFvkAFvDYXbwgepZPrOwTPL9Py1SidSzj_B5shfg3WyYbGCMtj45piEp9PB--JruYToyE3MRwbWo5VRjMABMIIkBBvkBxDz4CQaVX-_9uRXQAYP3lXecrBEv_-tQ1Z_Vv5hULqn5KnN_ykTLm9UiyKnhtQ3UZ7KjqbcvhwwkYDuS8m9gzMlYhZ-VG3-Yv7SrPaNsMvKQo-v_a-_BOXOWyQeK1-wVozCsZvq9A9VWDctZxoCURpV8J7AbxL_3QzHtEPjNeHDChmlInjF4Xveadt865ud8o=w1820-h297-no" alt=""></li></ul><h2 id="爬取個別公司資料"><a class="header-anchor" href="#爬取個別公司資料">¶</a>爬取個別公司資料</h2><ul><li>網頁中有許多資訊，而我們抓的資料可以分成以下幾個區塊<ul><li>基本資料</li><li>公司簡介</li><li>產品服務</li><li>福利制度</li><li>職缺數量</li></ul></li></ul><h3 id="基本資料區"><a class="header-anchor" href="#基本資料區">¶</a>基本資料區</h3><p><img src="https://lh3.googleusercontent.com/jMIQYJ9rhX7b5YymoMGIh4oByrjEcpbtDfnGsrf_7M62Bf7V4Pifu9bwlr_9YwDH6rCuQSeiniBqP4t7sXfnQLne74_izfuyYoogs8C7OzXCHgBmTazN3nd7rW84NWShgfnPSfogiapWS2bDUkZWUB1ifd4G4S4KPWFjlHLfye_nPUTtR0S4SXcCC9I34VVdTXVncCo-I0XQLda_Nav7UMPECc8OolB_aAMkfbVsVqvfZUQuhhTf9FkDwqdJPxMfbeocLavddxi1Zt-HAzes0ijyrFWO2LLzGnJzWBQqr-EjkWgeoqBAqMVV3tFzlILYGsgF3M7XE78LXosDCyqD6w1Kaytxzf8pZL1roaWIV83-mtC0aMnd0s50-YSscqoJ0R3QVZNNSqVSSUC4Zlj3BKSp5v0kzcYTJjQZVJCHfsxWjlrfLKg0KjwdBzfc6wzV19nn_Fs18Wv3bb0hpsB5EV4-qrf6BiY5lUo-Gs050_ZyuSHjWOPCbC7YJfukPA0ld6gu6TOCUndqt1FpS5rvrNphNRHMO_6fHLwqF3MWM5x2kI9uYzD9qFSZtk4SQdnoD3FNSU_wAOTgteTsgz4bTl_SOp4Slc2E2JAf-OoLS6UvvdidXolMEjpJECrC49T3bkNdXD6teLctwDytdi7yly-iqcgQBos3LONhjI9XljIkPK7WMUOanI8=w1815-h849-no" alt=""></p><h3 id="公司簡介"><a class="header-anchor" href="#公司簡介">¶</a>公司簡介</h3><p><img src="https://lh3.googleusercontent.com/vN-ay9lbeBF4f3K5RM1ChkwbeZjvxtoa8Eoi-AeOcrq9Ep6EHtvgfSpKD70kPP3K2y8ms60zmEwo0cIMrrikUuq6g29vBC3YkvMfJ1OZUv1mpHcRt_tuQiEVaJ1Jfv_I-bsH0mEPWo5AYxo3oFSHikjwPbjf-hZvaJFjmSh2qH7GboFZTT5I4E30lE8LEw68SCZK0L5RFY7img0PIUCEDtdaV5Lf-kCrfjJuczpdhYUHbiMGW-wBzY5IvdxaM0YllQPcqZCOmA4YdNX8uu9aO2b1M5izAi3GtmIWcVIOSOs_czdTIh1ulbx_yMdCh81Oz6hMVfcrJyZcaa7jig_74TyFxSNed5ST-0q4qQnCslkbrOfRXAaFfuFAprjaFxAkedL_Esgcm3qJ0NWlDoxOyED-2hBSUQ6vxoA_9yq6r9P5ONGFSMmckkPRxOF4fHclHkLTnnozNA3T3GKQg1J-_pgy3ZDlQLqJhx_O6nQyXVmM0b7uUhvq0X1GLqbUzNLLXRg_4ZOCPUXlKJziMgPdhggOVRc33Kq0uSmuRKI1PnpeVIQMLSb6GXjI7oICg--e-munZhKb-j8b3KvQJlrEEPpfum5BF8z0k5cX-b5Pw-GpFz-gUkRmc9mDO8xaI9iGzIUnkmj0zw2pxPRnTYLODCykfd22f-l63vonRWnXUL7-eBiQqWZJ6xE=w1820-h82-no" alt=""></p><h3 id="產品服務"><a class="header-anchor" href="#產品服務">¶</a>產品服務</h3><p><img src="https://lh3.googleusercontent.com/O9-4A6uX3HKy6S9Y6o3_5enoHxiqzl20dLKAEOCWNB3sktAZ1NhFW3-D93TMH00LhKwL9ZykCVMy1asw0uPuq5158uzHRpxC9HFVoU-DyRxuEZm5FKHUSZizesy6p2srzPMVpwgJFApb4bZsXvG-TZVwE66L_CfEgzWDhkNWujfp-ldXWEpsf99OKD3AS8VNZE1fZKAyM8UfrJX31occncdpETTCTpMHJU09a5L7ev8c8dEryf9opo-Fiia6GcbISZTiymNk_I1sSJrIFK7cgd_rnLQTd4aygBQX_a7FQnCrTRuYwtrknf42iFiCWYe9MleiDMlmCJysR34cC6W17RANr6WrFw4WkSt_yJqiGg_0ugoeVOAiY7Ryilzw-eQp99OKZHOQHU4EMhLrhIsCSN0T1vLy87-G4KyO4aRYpuT7AAT4XuJHT90-VGt03_Yd3Hxak2wT6i2MEMgtXdIiXFJbZ4DbTPkQcV6jILB98CxZcmbN1Y-S0AqNWdhL4_cdf8mW3uEMXbjl42MElNFhj82EsLqGDny38quNQ_YSh-renX0VMwW-sM0JZ8gxkXxJuPpj_0wUI7KVzVEqjjhDumbOV_bjbXauYdTQYmoISj-543YjAmzMj9LwtqNB8TN_hEhYlGMrSZAO23OYOQATg8ICIFzKVaEopkADa1cVlFc9RlJPH0jwo3U=w1814-h81-no" alt=""></p><h3 id="員工福利"><a class="header-anchor" href="#員工福利">¶</a>員工福利</h3><p><img src="https://lh3.googleusercontent.com/eDqaijATxVUqJjWvJtAeCHhzoMTC7J-yRcbDa7LMf6Kxt3J7KtiiqrowPlf3R3ptFtdSM0U5PLzk7IiZsn7GzavzSyqtV23tNKIQWFCutNawgmpuon1u3QfHlyxxDhKaz1wuPDUgAyokYv0EGY5ry45T7W54LLzN3tyy7YUrh5vdLr8bKQ4iZRuTbuBkOfM7j32zR_LHghKlFb9the_MmymQlvAnsAzUOFK6k_7ii8xBJolt2UXW0Qg4ZTpcd94O9azkAuIpp4hPTy_dyfTBJIeeb8KRFgGOn76SlF6fPoNXZX18BDzJm1rQ85UhF54kCe93dTwoChVQgrgQfIufcFVNAtS2o-MUpRWR9IiAeMWRMifITGJrPH00nBqFMr-8HW_RyeKTbMfzDL6Eyq9HVOtWkE5eaDOPSjhn2AyR4-tgAOFu5EzgkYPo1YL6M9sH93UvVXX3GVrRZfrkPrXU6tH9_iAw4EZgxoChYY9iapnrnjWmH8ADasSYxYYi24yP5S755fJhAM-GDwwvFprm70d5ji4Zc8nsUXnAl1MiETtKPj6lJg4d24EohCBR2EKLJo1l7LtL1EkUx6SnufR_SquaNIKwxnii54055Z6dZAXFG_SzV3BJjz7DCc9ckzCGO8JQ_7zxlWgy00HBE_wiy9DwUEHoBlfuVU4P4V9qw_-fUJPE0R4Q4O8=w1810-h222-no" alt=""></p><h3 id="職缺數量"><a class="header-anchor" href="#職缺數量">¶</a>職缺數量</h3><ul><li>因為職缺數量是在另外一個網址送 requests 查詢回來的，所以在這邊另外寫一個爬蟲抓職缺數量的資料<br><img src="https://lh3.googleusercontent.com/PJRzhqtZFIp3J7yxKeRlVAUD-rkF1XquMCJjd7Ox26xTG-vcM443vxmnjJoY2uoe61o8thMkrAF6Df2NaCnrdhxxxdDZU6VHg0ZgEBnT-9Qhdy2vgK7Jk9KDzg823MWyWI42YvvRpaVGS7sEa-M3stNNTYqEFCffOuXxz0P41OAddSC0oowlH858eFV05x_tJ2TDCRmRGZzURk317IJlmeEa0ammt8pfaNvOfHXUOK-tYJ95slGb-TZQ1y8UtqUsU8lc6sUO7xdfhl8ZklvbMu-Eqn_htxKiDyU6IaNk1A1_kf9zcmCOL9uEuMuFVJ5_lvK8vr1t-Ny68Za1OyvRkwJwB5E8P3q2D501NMtSrUP-TDHM5_pVYCa8yFZ_ybdVUzs2F_UkU8PL9M31nz2yteSZLBXfZxXJqRD3h-EZw1YAk3U7X8FRKjnl_Npzn757gWRH4XVgRWbRTA-WpUxHtkIaxZhi5XSyrp8QGEiupNiqe0dVFuJZxSWr5sqzt8CPgwAXCgwJ1HrN910E9r1Px7UkTU5atHcOpjvMhi3qCDvVpZS4Gc71kPn_8x1a9iFcOqqjPBlS-sTbWLs6n4Yohsg7yCT5wK3wxKhMeLiM-yFc-zlMwTerZyPHrBwAEmo-0KsiOpXT_k3EYBHzbAfkRRYfT-yILoP88jVTiGP-WvP3SNIl3V3I-IQ=w1819-h177-no" alt=""></li></ul><h3 id="組合函數"><a class="header-anchor" href="#組合函數">¶</a>組合函數</h3><ul><li>把以上函數組合起來應用，讓我們可以輸入網址就輕鬆整理成表格<br><img src="https://lh3.googleusercontent.com/8eawvb0XTNKq_mo6z85ayEfi42wFDkHo-3_lfBfkNtC8uk5mhDiTcc9FMArDAVXOyGIHANPeiBjlNLjnINQ9ypgW6EpZnS7FlczzOzyg8MqFyP5RbtA6mfkj2uTaMgeGMO5hRNBo6KQ0CvSoh8bRu_IJdAE7dWRgV1F2wPZ-sAFnDrtDzxoiQJ9euDhGSSxkutqt1URnCFX0-8JladAqfFV_G1au7OFBn1SnQkG3RmEaytfIlyvpbLmsNt8zypiOSSwMGKg6btMdYktpwya70HhMrN8ovOj9WkqIgS39K-Y4YdxwVG8qnZFAaQy0vE-CRSpiqASXNumA5up8zMAg9yX7otl57zFXKRN0u4Fp7ncqEtcqRSo1dQVVc1Hlg3ZpRJ9W5PxZr0ECsSU3BR0yqW0r86zWwl29OczfsK-P8KOI3-6YD1ZF3XJOaGScawGZRbC5VlWM6cAilXukXdYoR4DiomUAfHgLWKDoAg2wq4lzZnStgypNpFxlOM3sL67uUjljk6qx5SbOLaWpqnYRNmpNIJwi62E2eIvnXJBDM7acXbP10qNyUPleQcz6TvpxHaLGOrJFnfauWRBCpWy3liQ3OlyihIhOwry8ac4Veiz6Lyzf-e8ZduosI4ICwRC1QbqRGsBd796HLQ2h4A27L-J4Y2SLewQQqiTpCuxhj38X3nWDzyb_FQg=w1841-h857-no" alt=""></li></ul><h3 id="執行爬蟲"><a class="header-anchor" href="#執行爬蟲">¶</a>執行爬蟲</h3><ul><li>將前面收集到的公司清單逐一透過組合好的函數內爬蟲</li><li>實測抓 1.1 萬則公司的資料要5個小時左右，如果資料量大的話建議引入多線程的方式來抓資料(加速爬資料的效率)<br><img src="https://lh3.googleusercontent.com/5gcpxpdR30GEJUEnmUgCQjLJHarDCUDbJKqKzgSIpLzvy0fTBvNVNbXCJkutNt6srzT2W6MatzmyJYMl5O-iRB3ku-t2zFxK2d0J7LLAf9tgP98-N9ZvKrbXdmJdyCdHtou2_I5SeQxPBtA0qQgBCyHf1GLotlaV9JrV3Gz-iR9s51x_N4Cdhg1b3lbZf7mu1HQ0On_7jqQ6D91cRqtNmtNMKhB4JGEIrhTu0t98kgYYz-mlqF-rJHVOwh4jBt0hlu-2jCXwA-BdKgWpnRxX7aI9XG3HaSXRw6r3WzQ1PA_UiMzeO4iwCjrk2UrtMODcrvXqxDO0ovyi_UtGUREPAS3Ms1QyFh_09qou7kx64iX7fO-RYqURq8PL35OMmW26yTw32MI3EMOVD6q4MpITcN7l6EbA8DMafpM43FyXzxjqVI9kLnKOxrt0VNht1gJU85eDGZS-aVrE9sMpNWgC6-Rau8Z558AXUetCPt2stzh1jGyHTPMfMT_BfRZwD_oC9qEwEvHb9XgRWBC41RZAGclQ70-6Hoe6vELVCTsUUL3w3l_zcO-Il2_CYHsri7q-Behw2hTSXtfpG-9wLDD0Gx0qq_WhrJgehqtxtRKeLsO7DObxMkGfNm-nD3Db0t1KLr0qUu_1611bEutBtpwloau2vjXrOlEnxhz1hQqijvRCtXvGDPzJyV8=w1819-h344-no" alt=""></li></ul><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><ul><li>先跟 Google Drive 空間串接，再將資料存在 Google Drive 上</li><li>資料放在 Google Drive 上的 Colab Notebooks 資料夾中<br><img src="https://lh3.googleusercontent.com/XFSD8XDUc-6MHpYrKAn7_MS1Sll0yGOtakm8yO8WzfAZWx4G5QmIehLgWLRDgpBS0TCSoG3PPtpe-xE7sKUY9nCHODD7zlSMd2LcF5cm-pYGMZSmGtm_CR3Dikm4Fu39dFhp5n-A1MhhYReAHxMMInc_weKBJzo1gxuS-X0E2vhgSTpD15iiuYViTJ-TgZrEAvlgSqUTWKzCWE40x-KkwK2s5QClL_JsZ_qC41Pm2Eo1FBhEIz_wEXWuipalVajINAjSm8jgiY3U5Y2gntGfPOu9mdChcUS4I2H1K9jjy8kKSOtkeeg9pF8qj5dtH_-B-Z75Rvdz8KhSpqwqKYFSgo4ejY0lkBrnYioQHLThWurN2BRUu9iuzh2F-FPpKQ51MdWHezEmeQgNgyhd9VUY0KmyiqDXIaM9m_DeSQ2l-nN6-TqEhRbUthCSxgjUwAVz9YpZWogbQSHrrME4cv61GjxFUDGzmMtobLQVw1AqmlukDxei1ppIntKZJ2YtEhXQzTehyEGwYxqJKem6Obw24tv8mQA_7_wVmSau5stZpy7B1DIxZ5ysAwLQwPnpDWIHuDGtW4nhrPihswZg8Jr3V_UMstWM0QXv2ufBHcarYXU6RNQAPswS-ewWvjM-2kjjO5-tsxPJ-UmaUz1akGBGwzCMjKIYQ8rYnhdmGnlDXHKUkM-TcGCjli0=w1827-h239-no" alt=""></li></ul><h1>程式碼與檔案</h1><ul><li><a href="https://colab.research.google.com/drive/17GzHUJfFuYxVE7m66Zk3zpU8wXlAimQb" target="_blank" rel="noopener">1111CompList.ipynb</a></li><li><a href="https://drive.google.com/file/d/1scDcy9lIWDK2DHaN4TqOHFgeDeeClbyZ/view?usp=sharing" target="_blank" rel="noopener">1111CompList.xlsx</a></li></ul><h1>後記</h1><ul><li>其實這個網站並沒有特別的反爬蟲機制，最花時間的地方在於怎麼樣在網站的 html 結構中找到我們要的資料，不過花一些時間找一下就可以找到了，並沒有太難的地方。</li><li>因為 1111人力銀行 的搜尋引擎有限制 150 個分頁的限制，所以如果你想抓的是職缺的資料，建議可以按照縣市/區域切割後再分批次抓資料。另外因為我目前只有抓公司的資料(約 1.1萬筆)，如果要抓更大量的資料時(例如想抓職缺清單)，可以加入多線程的機制，後續有機會再跟大家分享。</li><li>當我們有這些資料後，我們可以進行以下分析，如果你還有想到其他可以進行的分析項目也歡迎在底下留言跟我說<ul><li>各縣市的職缺數量、就業需求</li><li>從職缺數量了解各公司的業務狀況</li><li>公司名稱與模糊名稱(關鍵詞)</li><li>…</li></ul></li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 網路爬蟲 </tag>
            
            <tag> 1111人力銀行 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何爬取 GooglePlay 的APP留言與評分</title>
      <link href="/2019/12/26/Crawl-GooglePlay/"/>
      <url>/2019/12/26/Crawl-GooglePlay/</url>
      <content type="html"><![CDATA[<p>隨著行動裝置越來越普及，各家公司紛紛推出了自己的手機 APP 來降低跟客戶間的距離，如果我們想要了解客戶對於 APP 的想法，我們當然可以透過發放問卷的方式來收集客戶想法，但這種方式的缺點是會需要花費較長的時間。</p><p>另一種方式是我們可以直接到 Google Play 的 APP 頁面抓取客戶的留言，如果再搭配一些文字分析的技巧，我們就能更快速、即時的了解 APP 的優缺點。</p><p>在這篇文章我將示範怎麼透過 Selenium 幫我們自動抓取Google Play 上 Foodpanda 的客戶評分與留言等資料</p><a id="more"></a><h1>最終成果</h1><p>老樣子我們先看最後的成果，我們最終會將左邊的客戶留言轉成格式化的 excel 表格<br><img src="https://lh3.googleusercontent.com/Qx862wPuasoREv2KBhW77KXhaNphIO3q0jgOaKfOyNNZwRbhaqF9PodY_biXwg4C8aQU1C6P3_hdJ8EbulOM50IUWlc2o743L2UTaenjmwFnZ3WKnkXTatim9LZTMkaQ42njQ8yuMqvPnxI-gtpbMl2pnFsfEOWB_Lckg7XdZeGQi8x_SADH7HShZQE_C6GsHC8J6su1zeaePL1mfAqvPnTOaKPv2DZRYO-AcRFOhtdRKadaInOGj1_s4wY2qPnwW4D433R8Ec0LU-xgHaCNcvBS-5gsgzODlIVhhg9gsY1Ziy6vybtEO0dIXWEAltizfPG0BSX8SDaQhlYjPjDz6lFb1EpNaqAXcZjm9yg69ZFg1stpZcCq0MF3HnOnlpgTp7aPaLRVJA6u7ZJ8Kz3bzJdsfkLeycWaFPxCtVcjUgSISgshA0THJnI1whko7K9BCnLjrxj9sirqT_waymA-ZIXu-4Zm4yCuBMeNKc5Tkbsdsda6xHEGs13gWliLSEJx_Uz18_3COMV73H9o9LAlP0KgE_4FM8OVySFTuuubrNaMNi7PRgHAO9SHlYF-36dsCr2ybizL_vPDk8LYiFWOhsEO8zt036jf8pz9DED47tvb7fEKIZo7w19IptHvAztqR0qRLaodfFWTA3uwJ89THJkwh4HCx111gcxIpH0ggSy2JhOr7-HcCJk=w1739-h978-no" alt=""></p><h1>爬蟲流程</h1><p>這次的爬蟲會經過的流程如果</p><ol><li>安裝套件</li><li>載入套件</li><li>選擇想爬的頁面</li><li>加載更多留言</li><li>檢視完整留言</li><li>解析資料</li><li>保存資料</li></ol><h2 id="安裝套件"><a class="header-anchor" href="#安裝套件">¶</a>安裝套件</h2><p>這次的爬蟲會需要使用Selenium，安裝的方法如下<br><img src="https://lh3.googleusercontent.com/m6PjNPESUqsilH1DNfaURaID1HaNb3anLF-uaT-XCTgycg9oLqB2lQZ_NcmQKFRfpuHMl7YUCHtH3BH0i0HNLKMkJyFfPGu5t1fPk1PIkdTxHDSSDV4NZQ4-hhEEMc8LAxQ_G-hdgHP6HBY9nssAUL7y6_J-ku-V1ascTZKYCXo3-edYw9sB7zwkSm10CWC5aWp4hnmArynEN_d1aleig5mEznwNf7EaZBEYcSlpsc69z8hNhDBUEOJe9aWwy-xKEsqJpblp0uYN8Il1HSxJ0FgO4nhipl2ccOBk6CWAFntPTERNwzLqSwTD6kRGvQ33k0MtMQYXX6LLD6LzNsrxZnqCV02aq0L4-esbKSiHUZXaaF3tUiAWS-hlgl8VDtgk5AYnbFQdaOKf4pUm-r-tezaLx1nLy-HmF8v26t0PpynQ5gU7eJFqzHRxMxdarxC3yCocA4Y3J_a9vDIorMZS5B0lgS4noUWs3gx0AKx0JjBdJ31PQogVQ35f4bmaCTY8YWRsL2eHD7sN7tlWffRt0b6evN7TzTz-GFm_lcX-oanvBYrEmRDvXnk_pm2eMfzw7NGVB43FHSLLnRYuZL5Z-wTd6jwNN8raTLE9uYIPNSMPnWFoNcJCq3JrjfmLJFyaHNS8JzK_KvJuTJJMTaay6ZPkO5YmrLqXco13aeXfAjh2f7fCiU3bnWI=w1509-h90-no" alt=""></p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p>載入需要使用到的套件<br><img src="https://lh3.googleusercontent.com/Q-aLaTmFZBwybQc6pPcSivze-ZG2E522Cw_BvGeZVASDwhkGgMjZ8n7vqYrmaHJcvMnvCs-6ZA7fk8_sQ5AVl3iHlAaw_TRphrbOxKXpe2PY-BDG2LUdm9DSCANE94tcYbVmKM9bzp3QDEI4BgDwpAYM2A4qx6WEV-EKA9Kekn5lq9ei2nOW8XS5YaaDIYl69IX8W3m1jlGzL-3lHfGd9naur1ZO0UyWMI1VRv3NHq7vqtIksvFqzjq7lMNlvJdM_Snz3Zn7WT2uUGhaIKnM8v1_1WP4epC8mBQ288J4Beb0jYtE7_vNNBHkl7yKz07DMHTek6TqGQQ-qcVkUIGXwoMnZax3WeopWI8TpEzLi-_UnyDaZig34D03FNsnJ1W01OEIkG11VBFaGXcQiqCb7_EkTCIOxK9w5EEe_UQSzOOXFlf4ZnEXz74DCpB2tDZtkFS6tsyb__l-r1hTpHYI97fFfurMwKZsmHbfSqQPrT4CBesCe59L6h8omWRKztlo4wfgcwU2n3U-Zl5Gejr8PWdJ0yZw3qhla7lcFtl6PqGJPjRYVlIwJJjj7QcqjhSrbkW_AZKptRJ0hEif0OaePX6-Y2X4invdY2BhNZH-iJjwxNXWDRW0iILf6C7oOX-c4vCEjXUkJTaklQQQk_c3Dge55Oz7whEuSn5TG8h1uscvQKE7eMSy7_4=w1515-h145-no" alt=""></p><h2 id="選擇想爬的頁面"><a class="header-anchor" href="#選擇想爬的頁面">¶</a>選擇想爬的頁面</h2><p>在此以 <a href="https://play.google.com/store/apps/details?id=com.global.foodpanda.android&amp;hl=zh_TW&amp;showAllReviews=true" target="_blank" rel="noopener">Foodpanda APP</a> 頁面作為範例，想爬其他網站的話自己更改連結即可!</p><p><img src="https://lh3.googleusercontent.com/iK91grOv8nJJe4FDiQ4EDMeSB0Dg-yDioEz-LMSjaQbXukez5NMFgmabHlK1SBvUlwKvKnr8mUE8CKP0ZSz7bwuuOW8Nu6E_2uy6UTaUX8RTtsAiq_A3wWQMzYJ7sL3S_FKRWQW7FB1JrOtpGXLMExfVf5jnqSYbMvY-WNx5yLG_pnuGQrGyaXFsDARcOB1JeKt-AqiXnHOTdLBk6kZU7HoTsbJF1o8SyX60weQT1UJxGCqeNri_2wAsvezEPBk42dAr3WYFW2mEXjYMPShYIYRlL1eXZt5BaY1FB3QGj4dTgQRX0v9tPoinE15TLs9Zo8batCY0ypPWmLcnms-W6PVpgpbHwxQeBfgJmrgvF9QC8cOM1maLW0QSUbMLvZyIq72_3TVpd89mKmxdTfgpCHeoY0ks2vsnGTt2pfdk5Ax_hNsEgno4Kq08qZR-V1fZS7SkRvyErn-GwJX6ZZNAtEvWAw3PSsJb8Lx-aiXZusEo8NfPMvtwpISPWtsK11D93FgoBt9SxljPYKXlJZcgaLa3imS0gxW-Z281lyrTlo4u5OR2IdGRYKZ0Y6AwOBuN2gv8OjYAwgjIaE7Cv2zouISx2nzW5I2y9NZFRV4HADtw8N_y_KCNsPuZBjixl17-ycpZ561DvcgYxNrx9fVcX9nf_He07Bl93q_8uFNbeEpB5nfBVREcmGg=w1739-h978-no" alt=""></p><p><img src="https://lh3.googleusercontent.com/CPmCuDKJAP0lKEsfhV3lyzFbmMqwGFYpTF7BKQQhfSeR9W_711fAYqdc5kw_1Oq4eDf2EPaHk0tD9rH8HkEBdzIC9X34COP-rMNcQSHhk4zKiOkcJv2of6XIdSJkDiVD4vaxVT0lNB0vYiRk4GfYzPTxXtSnIrKw_u1fsQbByIkHpaMwJw8mBX9yAidHabYigCr9M0dYxJdQEU1d2MXY1VkVyRCqdm_KFa6RPtfFKny59VtyVp673hP2h9-J8xLS6f2T36d_F3IgDSIRlwi_lcKTtYfqn5jT4ZcHxNKNOlSjS56xmSZhIKFE4h4c4LdKVMbaV4sUPJK1YVA5xYGyXK6AvCLeai6p5-kCUz2lzvZ-_gaVPx2Kg_aWZmMJ8xkZ7k23mIxi09HD3cciqPRbkaNSFz0MVjRQ_b5bS1sVruCWL8tOicebg1PxU6TlWPWLLUUhf-Gykca93JeTqb0HFyGt1_JJQOtwoyytn7mijt9NOq6r0-7cFcFWfkjaRenMtTaChMaIZdwb7ginzBERQ_-FRLcEKCkLSnxKokFZlCJz14VF2ma3c1Df1MTnj5NofROUZytFsc_FrASzcaofbDMYg73rshXlStpVZbWJnOg8-s6SOlfw0zmTfTZsO6YC2iRSyv2G6pCrmC_K4d7oLTcMbjeYlhzQjQTipbr0_PNeAbwWJUhYwBc=w1507-h295-no" alt=""></p><h2 id="加載更多留言"><a class="header-anchor" href="#加載更多留言">¶</a>加載更多留言</h2><ul><li>透過觀察的方式可以發現，當網頁滾動到最底部的時候會自動加載更多留言，但是每加載5次會需要點擊一次「顯示更多內容」</li><li>另外需要留意第一次與第二次之後的「顯示更多內容」是不同的按鈕，所以在這裡使用了一個 try 函數</li><li>在這裡我設定做 10個點擊「顯示更多內容」，實際上可以依據資料量的大小自行增加或減少<br><img src="https://lh3.googleusercontent.com/UvNjhu7xwKq7RLikJcYCBBGrb4Wm39RkvznDeFyie1BpQ2KrIde4jTHHRP8jqzBUfZ4X8-V9fUSAbboYHamony1jA0D7fvReTQsvyhTcPHXLOIscas7VIRktvVYiakbEs7D-K_nGKCxkJ-GaT3q7P_B70W3sg7bb0DWDXGRI3m8V0ea4go5v13Il0haW4xmrPSaRMj-sAnvruTXHCTBo8gp_P1MXWefqC3xv0FUqN1jCenbQD4eKsUYvcrN-KCzzMgfYEBxfdY8hCCvLegs-lXF3o80OOLXdsYaFPIa0U9Ft0inFzrFKlSrNl8-10JNns6vjXaGXYDarC0QvE6XQPd1y3YbgpUKByYUPmpD5w5I7JpaObmr0HuoW-M8OZEWcVVF3RfrhcSZrxoN0n5_Tl1lqIuFT2P6DnPtFbJpjeqUftuGA2NBsy-C9nM8BkbZwaey5YZ0IU0QOI1PsH_2-ErjGKvpR3xmeT7JOUBjtB4SQNtizVOYEndFA12f34rY2NFj_v6tOMjAXGTiGREBulOoyLeaPk5VSm8IKO9iCJEQxBxEABU-yMFmd-M5QTYz1SemeQH5vxDQe_Y4dsc8UavCTklEOY-wpMYXLIaIf3icU47NvSCzrSYcOEwlHL0kSa_i7Ppo_7MnXgW1TIcCvD2DsY-KMw6cmyBdp1C55HSmkDUP_70n6c4c=w1514-h242-no" alt=""></li></ul><h2 id="檢視完整留言"><a class="header-anchor" href="#檢視完整留言">¶</a>檢視完整留言</h2><ul><li>對於長度較長的留言需要另外點擊「完整留言」的按鈕才會顯示完整留言，顯示後我們才能抓到資料!</li><li>這裡我找的是頁面中有沒有「完整留言」的按鈕可以點擊，有的話就點擊將留言展開</li></ul><p><img src="https://lh3.googleusercontent.com/YKSV2CFs-TGvnbnBzWP1PvJczTioKJzUyJA_x7eQqli6G3pYL2W2rTrtwyGfc2wYprI_dLKngQroECIGbDtZPiWu9xnTzI9_w28iPGlVZfmJIgVgaHH6mRjzkB6ADzwF43KaOxdFAUK4mW6xsF0wimz8VD4w8UEeGGkuv6-AixwNZ98WVpQpEu5caBwnENIg01XJb2bzxJrpCJjCFN3U2qCkrqizcTSAmsGYuEgLekX8IE2TomJiONQ-oHB_6FRNb5Wx9WjwOlC1_xwaRKrZvAQyzKhXrASiOdHHHLKRsh_VZqPGZrEYyaq76XaXob4fqQPzmyBSZ3hYaEMi_2cdYPL0ms4OU33l8_AxQhfS0mlOrdQ6vM04vXwJBczOGDTbmTmlBUp5qWQiLDxf4InOn05_dz-G1AWFiEtA9lVQtX-ElErhQcc3glerWarN5e9UE3zxMf-xp3Qgr0HXqj2ynRM4mb8YONNUnJYkJbWZ-kdNeofsgeC6t9BOQjk28K1KtP4sYABZwOKkDg_W_X1L_m2EIOfW0Y16pLvTjBAzBnf5xTFkN6lsa0lLn77za9aYZOwL4sim_nT2bSLrdimF5Ay5i8x58Mrx4e-B8MrDxCGTwhPS9V7ZcAuxSunO8yavq_1T4z9lrkCWSFCE-ry0uMoXUnE96y5LHu15D2lFsMTUR4HEzYuaRGM=w1739-h978-no" alt=""></p><p><img src="https://lh3.googleusercontent.com/QJzwab2aj4SjQsxIbBOoMslSEMT7MA2PCbOg_aE92OI2q84TG0hg6rF6DeM01CbGFdiEG7zwpvLIAjh0JdoqJNeifckNUfjG_vQ6rknESmueaIAk1zEnSJj639PUXUsoTW7KRZyJM9WxMXj7KE4RiWWpaEmTrTMphtNJdANc5lv_zoQXz-1NNldLQPY1yBy7JZJdXuOyJj_L4K02YiAlpLpFKKddjFAFOB_jaVi__THoBy89psEiRd-sNCtTkTiXomMljFFsIeDIRaI5hccd-qo79IbbHg4mEoFd31LsRVdY99fGL7ymlnfwRFu0zehLhxQH6AcBiR1eARxoaw0OzI2M6KB8f_r7HdnmPMchveH4j8XJNbfChP_b5muiC1U00Todp6ETwhUu2e63CO5W9tCM9iwnwPsXTCNqZU29pf5QgiAQ3er6GFDRtzmHS6wIYfspnYBy0LKkC7-NotkOpV7zWBu_zfS4RcGwKZcX1XOmqI72Rgn9G9p8Vt5v5UaVFOQQobNwxv9NKeGo0meUavK9k7XdhqZiKM6XWjd6S5kyMXeT3f1e63VzkYPt2btXNyoyosKpE4bSJPh8f4O2pHXcmBv8N1HvbREh489KD2R7PvHZliJE6VT_cNIdJ6gNl-LgSWwkCeNLfiy23ic2hNSDvH1kyLhsVbNhBZuosHL14Yl853lTM1Y=w1512-h168-no" alt=""></p><h2 id="解析資料"><a class="header-anchor" href="#解析資料">¶</a>解析資料</h2><ul><li>當我們把所有資料加載完成後，我們的資料是html的半結構化的網頁資料</li><li>因此我們在這裡需要透過BeautifulSoup來幫我們把資料轉成DataFrame</li></ul><p><img src="https://lh3.googleusercontent.com/8kbpnn8SQruf0Xfl4IfWlcyLFat45mArTP1BpKHfYvfORfxBETYjBBNSZwfldIfr6bXfYUS8vuI_DT1qGK3bnR4fbbUAnK0PPby4q7YhbiA65iwb5GkAtjJkF0_kIfCG6aH_RmHPO3MHciwm5HL7l5_MzrA36P5s1RVpIsG86SgBHbbNxM7E0aDHBe6Bao50Vq0T4dcjBvbA3gntftpCo_lUpI5718IyNwbZew4LP7SssN1wNdGCoQfY7k4EA-teS7Ob3F_tRem0CoSSL_t3q30CKDHy0iNwV6Lkxfv-0qXU7GYQpwh58G7HSdBffew3zoZcyAg5hhU1v5lmKeKvY0rPnnh9ilhOJ5JgsVhV4mv7mhbf8g6jrRV1HW2I_ahu6uvcgl-8EbRqApN3xbtR-J47V7qBMRLozhHyBP3ylvB-1RgvLSRB7nIRZqYJLmDXAjc8b2HZ5HvtKJ3qtnA5Hq0Ja3A6bE48m3yBARewfD-AQ5QWktrB0MfhNSpnMGSAgDc_pzkLy1DqTrQaZt56pQURsSxOuud09Wga_tjG6WDxBCFZxu9p93hqN4b44kd-Qdq9a51LT9FjOlkDb95tXsZU7OoqmQoZIVgtYCGl2ywm5EBMxjZ-DnShlB2OypWegn-Jzrs5Z8rGYGDqXEk7caC6VqvpPSmAJXJqG6Ju0EJQf71L_bBG84g=w1510-h478-no" alt=""></p><h2 id="保存資料"><a class="header-anchor" href="#保存資料">¶</a>保存資料</h2><ul><li>在辛苦(?)的抓完資料後，最後就只剩下把資料保存下來囉!</li><li>先跟 Google Drive 空間串接，再將資料存在 Google Drive 上</li><li>資料放在 Google Drive 上的 Colab Notebooks 資料夾中<br><img src="https://lh3.googleusercontent.com/1RY9JXc87whTGNVpHY8GNm77RarneoCpsxulPb5vj6wfXeVyWIK7fZBDibHwqNOzCts_l01rF4cRfW4DT_PgQw5TTiy2GJzPkBOHVUF5Se406acjFQeu-5aIdJaSPN2BfZUe9IcH8jsejbAkBIKV-bnLT3iSwDcQDrEWWPShM-aY6Ik-ZCVgg0ru5gEzl69wz9jxslwry5qIlm1bqfFgsr9thO8KxVdWK7izA4PkHv9KtRW09_wmPr7XxfomxanTKhe2l65vpe_oTs36w03dz8huj3tKWHo9ZafJ2N4gSqdPfiabN47tKetfHlgRY48nV99oVeIQTMSehsIm9hS7ppdygWz_h04W5oR1-q4mvIREU3wwbpojD_9ae87F4aTIiLEVHKi9TOvJ9RXkgKI-z-pShLKrrEgMcSdb0XxHMXWEQNqLSFZ3u39GRG2yJMDMmF0Fr_Tj3kj7RCqRCezX0c4OsgR81NMV6Wt5CdB6NvbxbQX2HOxfc0txXze0hdGjgwaTsBf7dFYWFCw3iKBVqlVbdE0bywKFqky1i2GnxVBWOIgTijG-B48BHy_tDhWpvxB3bUnYGtfotQsMV6HExX2T4iii8EdAfpV8zHdHEAvY3yOuN_lDUfz7Iw67VZQjVx9M7M5i3tQPYJLoOCf-s8xNk9V_XCzwwCNsYsUVfZNXK8-4CzdzGGg=w1508-h67-no" alt=""><br><img src="https://lh3.googleusercontent.com/vbM-BfMd3-qRvcXEo0ROQj_xubD6sDgxRenFirWnqQyMZKTCDiOH0R3r6ygLNNzgs32iUgHYy-TotV2HlaI240JJMV4NN1dcHZvRVQtVeQx8iQsiO5yEZ3P9wwXN1PKit9q_yR0i_RcL4MR5UAa3o0VuEqPqHKKhAg7orRZu0S6_C0lFtJFPfyTRNqsiR33iFkWvZDPzOuvjjzQXWqCue7ufVALDV_B0qU_6Nx8dkNcbPJ_wLI-DEriNX8zqTVa8hU9D7rZ-zaMjNO3BtNpK7uW2x1bi-xK97eHBmK1qENnups7kIvUb4ukyKZC2t88w3thrO25vwlxgc-oyZvAXB_23CiUeE7pbGvBwypdfAqg76WDm31-RZMb_HrlhB0LaR2cXaj3Kp7JUjZNNMce4dH5WppTuVI_j5BXikdkaW_igKv0MO6i-s9Ikuuq7Dt6pJxzgjL3nWsypQAMwhGiSzoSFzYVJmLyHz85WTBbWxp5xQcX5b4YCOj3qk0I9B3vbClvj5utdmYl8XKjtGrNL8HMhwY0FqjdMXirhfFmftH4XL-bip9fU6xM__1DT0ofUgh-xo-io1hfsqWz8VutTDixxv9ry-SXwvAIOliMD9WKH2rzsa4iRo6KhpU5RvsJI7ZI8136j3AlyNMdGjY07Jx7ghy72QrGZwWMmn1JOUf36mAy_eWPUE4k=w1507-h85-no" alt=""></li></ul><p><img src="https://lh3.googleusercontent.com/XqGbKWIZbCLvd8iGRR41WLodgwts4h9B7NEEWBh_Mg2Xo1k07Fm8FP5i-K_si7_fq1-9EuMtyTa8a-UC7_jupFpdtg8ePU2NSZgjtxya5QV115eKFcUrQeytCRqteC-XJ13otanJFZOkbm7LwG8tt0gRrSF7qVaWaOwcG2XTLVdVrJvvAnxzrK2f1G3eQEz7gZ9zLOyZMU_X9dnMh-Tx5I7ajmYgOFs1ulghQH5S54eLD_hu62mUyGOg8qc06xH7ms1K2TTNnoifjshDMfOzU8eTbfakyoV9Wt3SiZlplLxpQ-1Hxps4SAeNrHZB2MdP3J14rR5Vjz20EWscSY6t2Z1QESIYbBS8RZO9qZG3g83WnVGA9r195OiKTXKX70wVjlPGSzz8cLPE05QXpOM9ZcL_YZAgZc5eKtI8fjcFUsyiEeUyfbVluEdYNScPOizNLsm4ovhkuzROfmTwBAo_we3qysHbqHgvvRTYOq5PKV4MjiwD0Ytmi4cvrccpt_YRzMvDCfxKWmxwmbiaEhTZNVy7nQ4JH0xxDM3akaIfu34voRjUovMfalsns28I4gK-a11Ux96saA9E1VY5AG4PY_Gy3xXKXEIYBl6dznEFW7IBJt3Wtk1x18tmlJNpW7bsnI9qBuqIyNyM-gm0NtffNhxKoV2C0ayEpLnJ0tD-AKkzPOfbGOCJWvE=w1920-h878-no" alt=""></p><h1>後記</h1><ul><li>在這裡附上程式碼，如果你/妳有想要抓取的網頁，可以參考這份檔案 <a href="https://github.com/TLYu0419/DataScience/blob/master/WebCrawler/GooglePlay/GooglePlay.ipynb" target="_blank" rel="noopener">GooglePlay.ipynb</a></li><li>這裡抓到的資料也挺適合用來訓練情感評分模型，因為客戶在留言的同時會一併評分。並將訓練出的模型套用在其他沒有標註的資料上(如 Facebook 的留言)</li><li>如果有問題的話歡迎在底下留言或寫信給我，最後祝大家聖誕節快樂囉🙂</li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 網路爬蟲 </tag>
            
            <tag> 評分 </tag>
            
            <tag> Selenium </tag>
            
            <tag> GooglePlay </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何透過爬蟲抓取中時新聞</title>
      <link href="/2019/10/30/Crawl-ChinaTimes/"/>
      <url>/2019/10/30/Crawl-ChinaTimes/</url>
      <content type="html"><![CDATA[<p>以前我們在收集新聞資料時，是透過複製、貼上的方式逐一的把將新聞的標題、時間、內文…等等資訊從網頁上複製進 Excel 的表格當中。但是當我們需要收集上千、上萬篇新聞的時候這種方法就顯得不太可行。</p><p>那我們要怎麼解決呢?其實只需要引入網路爬蟲的技術，就能透過電腦幫我們自動瀏覽、收集指定網頁的資訊。今天我們要透過 python 撰寫網路爬蟲，練習在中時電子報的網站上爬取新聞的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊。</p><a id="more"></a><h1>先看結果</h1><ul><li>下圖是我們將新聞爬下來之後的結果，我們會將上面提到的新聞標題、時間、內文等等資訊整理成Dataframe</li><li>有需要下載的人也可以在與Google Drive連結，將資料下載到自己的電腦中<br><img src="https://lh3.googleusercontent.com/N8gNwNU6T7-ZGiKIyCee4wq87LsHDPv23QWgou6AzpqOm-8bkkeuCG_DB44Jt6mUv3NYx0VCH5mKMRZhvI0wsGznuwTlllNLwem5faa94G8i8GO_W8xAHNNVvLY6eYNxBDfMaXEwkFOtGSeTxrd1pqF-bFKWWvnPQGUPBOYHA2CZxA657AYw1sjGIXQMMwqZCFRmzviU_7bTPuHPV0vYR71S-5sKPk6ZL23Suv5RcV5s3z1o3JbOJI7Ou6TBSLOcxskn7Y-i3t8vly1DiQAF85SaRB_8SI5hI8uO1FJnqVUuKpGKcSOkdFB3gIVV9gMxNjFlVN7m4Xt3idCZYm3x_nQ2yz-CnpAA-tXlU5-5Jjjc4aQEJHEKB01xj4vgkjb5Anc9vqA498XJsGMFT5ypGgra3wx97mfpxtFS08QViz_ogQ5DLLHQkRx8jVWqoWMUtbQIyXcCx5YwCJESJGEX7J6tNu-MDVIa2La5UiSpqC7gPFHyiLpe5N42p4aMkXOoIjQ0T5SGomzVF-kFdqfzB4NlPd8rwR0WZID4Ay-OgOg68LCwMPjDPWChbMJvTU85znuWSlCYre-WLkuB3c6nCzDVuYNPQUbaQmVurZyQgZwRP--JD-3x4HZm-OqK20zzlQRYtaaQvs6RmwEq1bYyfuSquLn1mCHztBruzxkyLSD9c9fpYonA96M6C0LcijLlJZ4pV8HMAsosApx1znI-YN6qAc-lDBW6HwHGfM8q2zus0RnY=w1840-h413-no" alt="新聞爬蟲結果.jpg"></li></ul><h1>抓取一篇新聞</h1><h2 id="解析網站結構"><a class="header-anchor" href="#解析網站結構">¶</a>解析網站結構</h2><ul><li><p>在開始爬蟲之前，我們需要知道怎麼從複雜的網站結構中找到我們需要的資訊的位置，接著才能透過爬蟲自動幫我們爬取資料。</p></li><li><p>那麼在上面提到的標題、時間、類型、摘要、內文、關鍵字、新聞源與連結等資訊又放在網站結構的哪裡呢?</p><ul><li>以中時電子報的新聞 <a href="https://www.chinatimes.com/realtimenews/20191030001004-260408" target="_blank" rel="noopener">英發動大選歐盟換領導人 歐英新火花受矚目</a> 作為範例，打開這個連結後會看到新聞網頁(左半)，接著按下 F12 或 點擊 滑鼠右鍵 接著選「檢查」，我們就可以看到網站的架構囉(右半)!<br><img src="https://lh3.googleusercontent.com/6bTMD0hNVHYTNZwkXc0WzpT_Y46ZlJILp2M9XeTOEPlFJDD0ktrZVC6KcQpXFfvhtE_nO4lCka4FnMMWWX0MVLiDZK-lkOpOmliJfjSRgt5vUobV3Dm5QC46fmRt18MPGRO5cvY5BHsp4RwZIo-ppTaUmIwkpD5dPopTHGn7ybFo8Ijm8tpCIiYbOZEc4g9q0VEmVtYKSEb8KPh6RpawkDMqF-s-nMNUqJxUplMBZG02IKibjLeeaeEOv-gWcQfoF4dp0rHJrraeruZLJB15kYDJ2GJTfF7IuYFRtgcAvw318WLdPFeSy_L6GtAmktI4ufK-hHUpYsVGmY--ESlAjVRIAYwkXDK6TqTdP8ZFYKgEEpe5xBYu2mksYmryl0Hi5iVARC0dSSAsRJ_nClYp6QbFsSEHKo-vQkITGSwLcCvCTvpD1Qwx8h5db-CEuUj-FjFwNXLL6vgIYme5zTmIoKH019D-pwUpomkgCUKx5A9vV5uBGYpxGljfKsOYwczquCYPl3kjzHL93S7vBZK5MhOspgqzl1l1gA0FOYwI22OAHpG97M-BND-GZKxjx_GLQ2GreG7R7PQ22WdarBZlBNpWN9BuS1C6NPCrGFzyzP9esLGdCU41k_UqLLNOnkH7mVIXNRUhBKXpYBLJZiul9PFrSh3916NQ8i0_lvfn4NSIDARnaaHFj0CkRavqBY31GW81zY5SwBEO9Yn_zqq7sqAuQr9T4mpl82eaJ694C-pf8mJo=w1739-h978-no" alt="網站結構.jpg"></li></ul></li><li><p>其實我們看到左邊這麽漂亮的版面格式都是來自於右邊的網頁結構中，也因此我們需要的資訊都能夠在右邊複雜的程式碼找到</p><ul><li>以新聞標題為例，我們就可以複製左邊的新聞標題，並在右邊的視窗中使用搜尋功能(CTRL + F)，就能夠找到我們要的資訊位置囉!如下圖，當我搜尋新聞標題之後，就會發現標題藏在名稱為 script 且type 為 “application/ld+json” 的元素(element)當中</li><li>另外我們還發現新聞的發佈時間、新聞類型、連結等等資訊也都在這個元素裡面<br><img src="https://lh3.googleusercontent.com/MY8vc8gGbLzMt02Ai9FI9hhTZTfu_hMmGBAz-0n1ZkwSuoAe_Wh3TUjQBkx0sPq609thcEwP1tKPUD_VuNmaIJPBfc0OeaA0REe2ob0jZgNmwTVNvwRt78FhaZUqjnYVazz3pdDxZT4bC1TQJVsmqeLf6FFnoY1mA_dgS56rHSn2dLTSmajL31dkl0CRYEM_CVTWHQZ38cb91t16DTX_aPWCsXN9j0Y-1AtW5IlcXAeXda3hIAkGCkm-ojFlB66U0fFjnUXDcIW3XBSnppC8K__Knjpn9ZjywDNxAwmX2t8QUmtVhuazbgczbStaqZmeAhAdXAZcuaOkgcMh1eV9HppzG0BiRxXdIj5NSxDJKs2nNk8AKaWhcH1NJM5IVAm7K4X7NtrPImVblqUB--L7vQHS331EuQvamcJk20Y4w9f9sQp-DE6hqox-wBDa8o5XqVsuz1LTvkqXgjJLvUShArXqXAYE7gyITn39V4ufg1G4zkuZ2chPh-Xq4VCQAHxEDHCJW1uHWL8Ag-4AApvZbQyr4tAYpbNFrgPLb3whGoxvv6hMW74zkqMuelDCwI1vXNvuc-xMbohRPjHMcQEO3unumYZK-XPxjjkgwWI7cmC2t6rO_27GD1hYAo2nSImdlxccYDHZ1qtKrQzXRE2cAFvJ_MI1434LsvLLAR6abs6U3n6McDmQvwbIqrO9Heb8TRM_8Tmy89jwZ5LRpxGd1XvW8yR_aHGFOYmLb14IZtxK6Dhl=w1418-h978-no" alt=""></li></ul></li><li><p>經過一番找尋後，我們會找到我們要的資訊分別在以下的位置當中</p><ul><li>標題： soup.find(‘h1’, attrs={‘class’:‘article-title’}).text</li><li>時間： soup.find(‘meta’, attrs={‘property’:‘article:published_time’})[‘content’]</li><li>類型： soup.find(‘meta’,attrs={‘property’:‘article:section’})[‘content’]</li><li>摘要： soup.find(‘meta’,attrs={‘name’:‘description’})[‘content’]</li><li>內文： soup.find(‘div’,attrs={‘class’:‘article-body’}).text</li><li>關鍵詞： soup.find(‘meta’,{‘name’:‘news_keywords’})[‘content’]</li><li>新聞源： soup.find(‘meta’,{‘name’:‘publisher’})[‘content’]</li><li>連結： soup.find(‘meta’, {‘property’:‘og:url’})[‘content’]</li></ul></li></ul><h2 id="撰寫爬蟲函數"><a class="header-anchor" href="#撰寫爬蟲函數">¶</a>撰寫爬蟲函數</h2><ul><li>盤點完我們需要的資訊的位置後，我們就可以把這些資料放進 DataFrame 囉!</li><li>在這裡我寫了一個簡單的 GetNews_chinatimes 函數，讓我們只要輸入新聞網址，就回傳資料表供後續使用<br><img src="https://lh3.googleusercontent.com/1O-4wLHHQ78GCOzmgAK71p3mHasOtfBVEwI45doSG2T_N0_z5ZP1d2U143osp7oL1lFi8d5UWC53A0WtMokhuleNfLwtTAilQzKaQUP-zbHqqFVgfUN5zraUyEG57MifDVCdRlcqg3xlm40LAz3TtlCO40RjaF_S-BUvKRq2fO3UPVldfFoLuF1wPZW8jGFys-JR9roOd-tygiUHXf8RaESWszJvoKyyxhDJFrttGltTRlSmFpm5SFX5iwCUcyL3cvHXthXt3j-YfR3gYP2ndx3CJE63tfzHOAubKNrC23TQbezeOec0AhSXtIZRd-zOUDZduqLQcfBi53eRm5bfmVc4ZpgcOnqFq2H1VUDIQZUoJiG_pF95ps1GdY_uyW4NqS9GchwQfVTj9JJ5_EEL1dBS8sf7YMyuh_m8rfzi0B6Aab9-5yn17yB5nghjaPmhpG7yIDJU84m4p7lEwYBJduJSHABsyUjhm95QsShyzTS16_nLRoiSDc8B_LJMZlo0tQvhoZa8jq_QjQWzmkGl2d_yjL2EJrsfIqRAB5Y_G1xOKvAzjHyxcUDVJJ3dk9Keh9YVkxFRarsPLqdVevH_aguIrPUO7eMI9pr4DC-RA1nYyeY5yN7GO3GQog4igKFOX5oE2gV05QFszZuVB1P-O6EyA7PB9eJ6CkRjPQF7KXRW-JDgI0w5yyzKrqnye5udEhag5z7_xGgiAj6Eh-e9qUc7G3qhLRAl9hHj-IxqNqOuyfBi=w1861-h526-no" alt=""></li></ul><h1>搜尋特定關鍵詞的新聞清單</h1><h2 id="解析網站結構-v2"><a class="header-anchor" href="#解析網站結構-v2">¶</a>解析網站結構</h2><ul><li>雖然上面已經寫好 GetNews_chinatimes 函數，但仍不能滿足實務上的需求，因為我們不會只需要一篇新聞，我們想要抓取的是大量的新聞!</li><li>因此我們需要運用網站上的搜尋引擎功能，幫我們搜尋特定關鍵詞的新聞，並將回傳結果中的新聞連結都抓下來，而要抓下來一樣得先找到新聞連結放在網站結構中的位置<ul><li>這裡我搜尋的關鍵詞是<a href="https://www.chinatimes.com/search/%E7%BE%8E%E5%9C%8B?chdtv" target="_blank" rel="noopener">「美國」</a>(左半)，並透過 F12 的檢查功能找出連結藏在 h3 元素下的 a 元素當中(右半)<br><img src="https://lh3.googleusercontent.com/JwEFSUZGq6nPNiZidVByMG6eyVPUt9U8uJVARkd4D9t7h9VZTPLbZOfTgsbmLh4TOBLrM08XevDHbc7UW29fX0tMW4tpds9Q0TTSKEiJYzA8zgeB3h1CbsqN2TfXc7SzL9sPB2YrUMzf-D5jvX25JsOCHSOW2yNhFQyc0-0343asVUDJCeOBibIiARPuxz3QLGG-zPAQ6tUHtx3j-9VmOa0nL5dRmKb2c_-uF--GGKv7utloxgiDnoD0OqdZbhfxrVSTCzHftAjAbVg6JWKYSWwqcaHe8_0fsRBRPP2NxfM7HSvzZWKvsahkZscTiilyGwblN0fODArJxZjike1xCCizpocBTbkjBv05A_LXn3-XqcPwdtC1RV2R_a7sPLNKDPF1boy3X40qG7nMWSoYTQPCJ4oT9As4ZMnHNJ5w5wQXlmfA4g-cKGesxVVtLJ-r9cN3ycz9NmpqAgF6H9xVjL2oXmQF8Orr-OHKpHT1hMHHV0BTL4FjhkNN2BQq9bgwKx8LsEcIy-D2MLIdy0o9PqoMjSsowEE6cAD3S4I4622KG_0ja10qSAtvI0hfxztS5vRPl-rVE2kT3yC8M1ssu2afnmJjdsM9xhGNwlnsZAUwFpAHejP1AdTXTZLprAo4DgbKLxxOxnb8qjfHQv6l00Jcf1iSys-gGF1FfKjXDtDl-sftztM0glqXTPjdEOJOXoUpAVNSru1PULNZ-IssPuoQm2L3nMpUTdhOB8_5N9VWK9rS=w1787-h978-no" alt=""></li></ul></li></ul><h2 id="撰寫爬蟲函數-v2"><a class="header-anchor" href="#撰寫爬蟲函數-v2">¶</a>撰寫爬蟲函數</h2><ul><li>要抓取連結的方式非常簡單，只需要找尋 h3 下 a 的 ‘href’ 屬性就可以了!<br><img src="https://lh3.googleusercontent.com/KgL3_3_7QDzaPngbVCTwN33x3ShFgiaZJM_ZERhfc35Nnr7eEwuD3Tf64TVsSLD0f27EnvzgiyFjWrGeNEUgWm2zLKFGHsFv9z6yyI2FEQQBspN96T-klIgEpVeAHpmot9Ci8HKLXlOX2gkHiko6dd2Qw_CC7fW9w52j9sJqTIxP8vNpKzo_fcRdWEalvvuZlNhpBmzuo-cbUBpqA-6_MkMLpdGqh7poC8HB-o6lvP1Hcpd8EatSamX6cZ-ppb8KP533NfXmgRvLB95vKxrYuWmB3rWHxdfqZ3l_xTsJuLx2qo_8QBLSqS4tBE13O_JZrfetHV0FxGvEp_Vw_ILl02Jvg3tC6meXahi0H8p2XY_lbRbQTYuoXDkNn2tkNYGncFCfniCS3Ek-viClaEhdMfmtzygwzPpR9PFlATipEHlFlj8MbeOhNHRwpqC_6w37JtNNDRGPygtMagEbatvoYQrvnJkO9_ldZZXXmhoWOCO4PY9aeeUHlajpHelhwVdnYH5VBwFtccblV2w467-BdxCDbzNQxU4ZiNnzakpCaYrx8h61MrcB9rw0uZEvDKOVyHTUYaOxWhxzkvg5GM9SwJJ2vEDsFImlDqXHpJ7uYeY0lBQXSSBMS_wfFpbwvlxxj2lpLISpHadsYi1c8z2aOa_wDhqw1Wb3GXzP4I-vNVnJMUPW_dcGECrnbA7vX54YrQo6tgpG3N9RQ2QERQYqIqtqT1hN12C4d2evRlXY0bAV-8LK=w1789-h978-no" alt=""></li></ul><h1>開啟多線程功能</h1><ul><li>只要組合以上兩個函數，其實就已經可以實現自動化的爬蟲作業了，具體方式其實就是逐一的把新聞連結清單放入 for 迴圈中，並透過 GetNews_chinatimes 函數解析新聞就可以了</li><li>這樣的方式在少量資料的時候還可以，但當我們需要抓上千、萬篇新聞時，「逐一」這件事請就顯得非常沒有效率。那有什麼辦法解決呢? 答案是我們可以運用多核心多線程的功能同時抓取多篇新聞內容，只需要運用 tomorrow 套件就能讓開啟多線程變得非常簡單，使用方式如下<br><img src="https://lh3.googleusercontent.com/Ru1GOLmFbwajDukJ7y5iu2Gi7YeDkqau6Kug_ctByP_hMNUkv6H3F4uukxOpEuyzHdYQ_DWne_DuMbnoRsZqtQncplZGBLkQH7HYiudDjee3ssrGYS1m_HnLdOMuB4UuEL5a1E4TlT7Q4EBBVqaqud5w5C_5jTUrse12z5Uego3h_Dk4nCZsHzYucNQSuPMzG7eSrWI6mk0ZaXbV5D7wwHavziP9vgElg1-B8htkNbJtoDZtuR_SHSMIAZlD3U0myd6x-tIgWQSevfNB7IGtI4ujmgLqg3KXNXsD2X2evUfUk4GQ7nIbx1Mj-IR8Z72PwpGR5k9Y9bJU6mlHQcfjzdJNdaOrpbZIjMqBXZLY93wpnOqEoU75aX3i4pg-Hxa38EI7GN8lVBvIN2EQUjAdjYYiL53b7UuPm5SLxfIlDqToxQRGFx-XwQBuZoJwozhnMpSo8ju4JFU5b6e4UXYY18WwWVh32F0fxbO3Hj2L3hoDhZoH9cEMm33WlIRcEimWx_V41gsDVMZ3jIPx9q4qFaK14obE1G1kYrGNa0J0lpFS7TVa9NlBLDT_Fys5FmNL5Dcm2cmI9oNRbOBq00RbyWarEabelG6iV3MSTA-XX3943uMTPtrQS6PmuX5W-EoyNtTKTgYDJqpua-5HcLA-KzOXqI5tqC79GeBido8C2Z1bCyzAsobpDrA7XrDhUYz6qJA7Z5oSFXX6ec4h56dgil2y61iW65fxV8XjbXk6GBjQv304=w1878-h202-no" alt=""><blockquote><p>使用提醒：</p><ul><li>threads 數開得太高有可能會導致被網站封鎖 IP 而禁止連線。</li><li>具體要設定多少 threads 數才不會被鎖?這需要慢慢嘗試過後能才知道，每個網站的反爬蟲機制不太一致。</li><li>建議在抓資料時使用手機分享的網路，因為被封鎖 IP 狀況發生時只需要開啟飛航模式再關閉，就會替換成新 IP 而解除封鎖囉!</li></ul></blockquote></li></ul><h1>組合應用</h1><ul><li>最後我們就來組合以上提到的內容，嘗試抓取大量的新聞資料。</li><li>具體的操作流程如下<ol><li>在搜尋引擎搜索我們感興趣的關鍵詞</li><li>收集各個分頁的新聞連結清單</li><li>開啟多線程抓取上千篇新聞資料</li></ol></li></ul><h2 id="定義函數"><a class="header-anchor" href="#定義函數">¶</a>定義函數</h2><ul><li><p>輸入想要查詢的關鍵詞與要抓幾個分頁的資料<br><img src="https://lh3.googleusercontent.com/EgzhP7GsbJxKBGY8-MIbyEnzVZSfTAcwRw_ITzkOthcph4pRVGHFXsUGyiPufbm6V2AXeJBwzbp-gBtlvXMuothZj4d4Ox3wwyfm2GFjNgzvHtr_M3m6-WC8eipt4BeVUemEbypkric5hJN7_-z4NLy44WmQcZJMO167NBQXqmNuFonatvmdVipfEkzJOeZwuHQdrv_w0UGeKWefx63FpuhPCbAA_LzK-iN6odEJ9cPCfqTm1nMVWlKKl6tbabaj-kyEio-rihYIMrhv8Iuao0j0cVasIhMT7zusm2Of90RQlIPGqlT1vGnn5dOMmcKPXrv9-oDPURnHqnptujePrcuiDaR_mQNfC2PDSSRDg4XZVmf68rzw0H_5UT_pQLsnnuKFmCpLZtnyFz3ubU0ibgzBU0fQQM04E6xtCAvlN9kO16sPMEZzExMS7W9hKkyJDBRaAQfbBSwdaRtQsm8G2T4WAffqanzjVG53J3GSVita_LydGgZlnxcvzzn1IrkAMa43Kb0aO7Bupei3Bv6PsrUp_i67UFNkRo5shPyw0bF_ozAfVTRnL0q0QyP5n_297wbBXor2jG7zcg3sWaMvlh7W-bPvXpkh7zyzcYhT_REqlTZYCKjcr98czUKZd0wxVcp1Y6Fq8xiQM88u9J84D71jK5cS9qqf639ck7skjY11Ak19srFASAapTAAfXmkCyI88POXQvqguekwPHB80EWkpXjRuteX1oX7gvEX0qzvdcMn4=w1874-h525-no" alt=""></p></li><li><p>使用定義完的函數抓取資料<br><img src="https://lh3.googleusercontent.com/DZu_DrjAq13cKmvz2B2vDQopBCo3uJ-U1izOWb2rkJObUZejoXxZl14ke0ETNrvbJsyrkpM3LSMKt4hNT--DMNalxuJg6774RH_9VdyNEzx4UKQd4atGw41ZhZGAoGQN4DSZNk7ihpUAfS6F-GKRtyy_pmqTBRxMXSJUVxobCTclTHTZj0YBKQ2ELHSqGJ6UkDXy2w8yVOahP2B34xv5EUvMfWSXbJ2QoFQ6dYO-x_Z9AamnpPYXu3MNIOZmiOFrkNfChu02G337g3ufdw6qAZCoXzC3hysrUpKmY69RNG1IBGs_BZj5hNQlGsGoUrdMOSbfVwezPOXPrMvrm-fFE0JqqHBNvyB-9Lm_pza9oRggCufuSdSpItCQTzfbh4D73GzT6ha9l20d1ovYru6RFwmx2SrKdBhni0rBhwkvlmHGkue83ecjKp5_XhfXRNLgIly466i9T617iil8ABpW3izK4UzrTdXd96yrsKu6aHfddX3_jET6Ib_0uT7xKs0lTN-3E2DBzXMWv97We8N22916AIW6NgvV9T1RCk_9G-ec0y9G-0wIlUieKuVI-Ie2ul1FjktP4ymcMC_k9UB7MIQP74p5r9jmM8rqaEXxohL-Fb1n92DCqWKH8iVDesWML5Js_Y2Tj6HbcApq2WlgF7d7Oe6KZX-HxvoQG3ogvTYphUW4Y5LN4K9Upq62vlyX1cjEVmU63nZIflxO8sLstkaVKmKcKcGh5hbH30Wut-sSLMJ1=w1876-h859-no" alt=""></p></li></ul><h1>保存結果</h1><ul><li>將資料存在 Google Drive 空間中，有需要提取資料的人可以從 Google Drive 空間下載到本機<br><img src="https://lh3.googleusercontent.com/2J2k1g29zujtoSpyi0gGhYRxruAd6KSkAC9ojdDBui5qDKmi6_m_RFg7-VYEl8FVDimmOrIm8SUlzuPRR8vUZ9MSb0X5wQQMMcMZArufTCoWqOS6QZavpUKbyPn_UdFw19KUrLnHWSzHebFduhfhs2UdX1SR1yPwtQk2WB4nUgYUbHPe3RQbikQ0QhzRsDFo0EVhS6Ik-fwpGdho-8IcQLH2PNoBhB2yQF5gA9FWt3Fza9g8lCegL2h3BJ8QNJa-qzCAGj2VTpbI6dUNXpqB3opixORD28rg9xWBg9DMq70G1GANij20dzpqCqI3_IgNT7kPfC-ToMPmULkjvPwMbzOA7hdPZrbSWMIsyYrdF24pmjcNf16E7b2K5cYK6UR_OJklMXxpZKkyLSweZAUFBLAtfB3jc6HMlH9iY_oEc9adz23MCzny_qsAQou--Cnj_ituBQ1DV9ObHwhw8oDeHv3bjfSUcWlsEf90EhIjeXVB7f4dS9NciVv1JZmrMOzBQAuODAmJOm9JLtht9tGJzLzkb5IalfQmH1KIK6g8PXnAOyF4skq28b9pO6Ul5KtRkdI4cfjSPMqjHOCCORE3DALvMkS4STQatXNCXZzINaCd6va4K5iSTabiMgszqIc62ylM8fYZUwyWhkOR1H-fwoU0Nu3bHZJqAr5O-nd-fgJaxh5UfdAHRwcmZMqKufSeazXehBFDhpv1M3XJ1Y2cKsf9XHiEQt6J9sCtGBK2eonBPW4e=w1874-h476-no" alt=""></li></ul><h1>後記</h1><ul><li>以上是中時電子報的爬蟲實作，完整的程式代碼我已經上傳至 Google 的 Colab空間，只要點擊 <a href="https://colab.research.google.com/drive/12CvYz3OMLgMl1dVW69KEO5IhWgsqUJCj" target="_blank" rel="noopener">中時電子報新聞爬蟲.ipynb</a> 就可以直接在線上執行這個爬蟲程式，並將結果保存在自己的Google Drive 空間囉</li><li>如果覺得有幫助或者相關建議的話歡迎在底下留言給我~</li></ul>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 中時電子報 </tag>
            
            <tag> 網路爬蟲 </tag>
            
            <tag> 新聞 </tag>
            
            <tag> 多線程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>網路爬蟲_Facebook粉絲團貼文與留言</title>
      <link href="/2019/05/01/Crawl-Facebook/"/>
      <url>/2019/05/01/Crawl-Facebook/</url>
      <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>上個月參加某公司的職缺面試時，面試長官詢問「有沒有辦法將臉書上的貼文時間、內容、按讚數、留言數與分享數、甚至是粉絲的留言資訊都抓下來？」</p><p>受限於當時還不太熟悉爬蟲，只能簡單回應說透過python與Selenium應該是可以，但卻沒辦法說出更詳細的方法與步驟。後來經過一番研究，總算知道怎麼把這些資料爬來下來了！廢話不多說，我們就開始吧！</p><a id="more"></a><h1>輸入與輸出結果</h1><p>這是我們要爬的臉書頁面，從畫面上我們可以找到許多住要的資訊，如貼文的人名、ID、貼文發佈時間、貼文內容、多少個心情數量（按讚、生氣或哈哈）、以及留言數與分享數等等資訊。如果貼文底下有留言，我們也希望一併把這些留言都抓下來。</p><p><img src="/2019/05/01/Crawl-Facebook/01.JPG" alt="01.JPG"><br>而這是我們抓下來的結果，結果分成兩張表格：</p><p>第一張表格紀錄貼文資訊與互動摘要</p><ul><li>貼文資訊<ul><li>Name：貼文者名稱</li><li>ID：Facebook的客戶編號</li><li>Time：貼文發佈時間</li><li>Content：貼文內容</li></ul></li><li>互動摘要<ul><li>Like：按讚數</li><li>ANGER：生氣數</li><li>HAHA：哈哈數</li><li>commentcount：留言數</li><li>share：分享數</li></ul></li></ul><p>第二張表格則是記錄粉絲留言的資訊。</p><ul><li>留言資訊<ul><li>CommentID：留言人ID</li><li>CommentName：留言人姓名</li><li>CommentTime：留言時間</li><li>CommentContent：留言內容</li><li>Link：貼文連結</li></ul></li></ul><p><img src="/2019/05/01/Crawl-Facebook/02.JPG" alt="02.JPG"></p><h1>過程中曾遇過的坑</h1><p>在這裡先分享一些我在過程中遇到的坑與解決方式</p><ul><li>瀑布式網頁：網頁不會一次把所有貼文都加載給你，當我們把頁面滾動到最下端時才會加載新貼文。<ul><li>解決方案：透過java指令讓視窗滾動到底部</li></ul></li><li>系統彈窗：當我們把頁面往下滾動後，會彈出一個請我們登入或註冊的視窗，阻礙我們爬資料的流程。<ul><li>解決方案：偵測「Not Now」的element位置，並透過程式點擊這個element<br><img src="/2019/05/01/Crawl-Facebook/03.JPG" alt="03.JPG"></li></ul></li><li>心情互動：心情互動分成「讚」、「生氣」與「哈哈」等心情，這邊的坑在於當心情數量大於1才會顯示，若我們單純的設定要抓生氣的心情數量，而該篇文章沒有這個心情，就會顯示錯誤。<ul><li>解決方式：抓資料要運用try-except的方式嘗試抓該項資料，若無法抓這個資料則帶入0</li></ul></li><li>檢視留言：留言不會自動載入我們需要點擊「Comments」後才會顯示留言。<ul><li>解決方式：偵測「Comments」的element位置，並透過程式點擊該element<br><img src="/2019/05/01/Crawl-Facebook/04.JPG" alt="04.JPG"></li></ul></li><li>看更多留言：即使點擊「Comments」後，也只會顯示部分留言，需要反覆點擊「More」後才能不斷加載資料，但問題在於我們不知道到底要點幾次。<ul><li>解決方式：透過while迴圈，偵測頁面上是否還有「More comments」的選項能點選，停止的條件沒有「More comments」後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/05.JPG" alt="05.JPG"></li></ul></li><li>看更多內容：留言中若內容太長，系統只會顯示部分留言，需要點擊「See more」的選項後才會顯示完整訊息。<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/06.JPG" alt="06.JPG"></li></ul></li><li>看更多回覆：除了回應給貼文的留言之外，還有另一種留言是在回應別人的留言。我們也需要將這些留言抓下來<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/07.JPG" alt="07.JPG"></li></ul></li><li>相同element名稱：透過Chrome的檢查功能，我們可以看到我們想要的資訊放在span的timestampContent位置。但是我們如果只輸入這個條件並沒有法辦找出正確的資訊。因為在這個頁面中相同條件的element有許多筆…<ul><li>解決方式：抓資料應用逐層搜索的方式擷取資料，在一開始就設定清楚要抓哪個大區塊中的這個element。<br><img src="/2019/05/01/Crawl-Facebook/08.JPG" alt="08.JPG"><br><img src="/2019/05/01/Crawl-Facebook/09.JPG" alt="09.JPG"></li></ul></li></ul><h1>程式代碼</h1><p>完整的程式代碼會在文末附上，在這裡大家可以先把焦點放在程式碼的理解<br>載入使用套件</p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p><img src="/2019/05/01/Crawl-Facebook/10.JPG" alt="10.JPG"></p><h2 id="搜尋貼文連結"><a class="header-anchor" href="#搜尋貼文連結">¶</a>搜尋貼文連結</h2><p>在這裡我們先定義一個函數，希望把網頁中各篇貼文的連結都找出來!<br>ulr放我們要爬的Facebook網址，n是稍後要送出幾次滾動網頁到底部的命令，藉以加載更多資料。<br><img src="/2019/05/01/Crawl-Facebook/11.JPG" alt="11.JPG"></p><h2 id="展開所有留言"><a class="header-anchor" href="#展開所有留言">¶</a>展開所有留言</h2><p>定義一個展開所有留言的函數，透過while迴圈反覆搜尋與點擊「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕。<br>在過程中會出現請我們登入或註冊的彈跳視窗，但我們不確定到底什麼時候會跳出，因此需要在過程中反覆偵測是有出現這個彈窗，若有就點擊「Not Now」<br><img src="/2019/05/01/Crawl-Facebook/12.JPG" alt="12.JPG"></p><h2 id="擷取貼文資訊與互動摘要"><a class="header-anchor" href="#擷取貼文資訊與互動摘要">¶</a>擷取貼文資訊與互動摘要</h2><p>透過逐層搜索的方式，逐步定位我們要找的資訊<br>在這個環節需要反覆透過Chrome的功能比對資料，需要花一些心力進行比對<br>另外在這部分也使用了大量個try-except，原因是許多資料是有內容才會出現。例如並非每天貼人都會收到「哈哈」、「生氣」的心情。<br><img src="/2019/05/01/Crawl-Facebook/13.JPG" alt="13.JPG"></p><h2 id="擷取粉絲留言資訊"><a class="header-anchor" href="#擷取粉絲留言資訊">¶</a>擷取粉絲留言資訊</h2><p>這邊要留意雖然都是粉絲留言，但實際上分成「回應貼文的留言」與「回應留言的留言」。<br>函數中的第一個迴圈是用來抓「回應貼文的留言」，第二個則是抓「回應留言的留言」。讀者可以自行比較一下兩個迴圈中不同的地方。<br><img src="/2019/05/01/Crawl-Facebook/14.JPG" alt="14.JPG"></p><h1>實作</h1><p>今天要爬的是<a href="https://zh-tw.facebook.com/taiwanmobile/" target="_blank" rel="noopener">台灣大哥大</a>的粉絲團頁面。暫時先設定加載20次資料，若想抓更多/更少資料的話可以自行調整n的數值。<br>注意這裡會透過Selenium開啟一個Chrome瀏覽器，若沒有下載的人可以參考<a href="https://medium.com/@NorthBei/%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%9Dpython-selenium-%E7%B0%A1%E6%98%93%E6%95%99%E5%AD%B8-eade1cd2d12d" target="_blank" rel="noopener">在Windows上安裝Python &amp; Selenium + 簡易教學</a>。<br><img src="/2019/05/01/Crawl-Facebook/15.JPG" alt="15.JPG"><br>接著先創造兩個DataFrame，一個放文章內容，另一個放留言內容。<br>我讓程式自動輸出目前的處理的網址，若有無法抓出的頁面也會送出一個訊息提醒。方便我們後續追蹤哪裡出現錯誤。<br><img src="/2019/05/01/Crawl-Facebook/16.JPG" alt="16.JPG"><br>跑完之後我們就可以看到抓下來的結果囉！<br><img src="/2019/05/01/Crawl-Facebook/17.JPG" alt="17.JPG"><br>將資料保存到桌面，打開檔案的結果在文章的開頭，這裡就不再重複放囉!<br><img src="/2019/05/01/Crawl-Facebook/18.JPG" alt="18.JPG"></p><h1>完整程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re, time, requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FindLinks</span><span class="params">(url, n)</span>:</span></span><br><span class="line">    Links = []</span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 這裡會跳出要我們登入的大畫面，找到「稍後再說」的按鈕並點擊</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">'//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">    soup = BeautifulSoup(driver.page_source)</span><br><span class="line">    posts = soup.findAll(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'clearfix y_c3pyo2ta3'</span>&#125;)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> posts:</span><br><span class="line">        Links.append(<span class="string">'https://www.facebook.com'</span> + i.find(<span class="string">'a'</span>,&#123;<span class="string">'class'</span>:<span class="string">'_5pcq'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'?'</span>,<span class="number">2</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> Links</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(url)</span>:</span></span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//a[@lang="en_US"]'</span>).click()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Now is in EN_US"</span>)</span><br><span class="line">    driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 點擊「comments」，藉以展開留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//a[@data-testid="UFI2CommentsCount/root"]'</span>).click()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.find_element_by_id(<span class="string">'expanding_cta_close_button'</span>).click() </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'There is no comment!'</span>)</span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k != <span class="number">0</span>:</span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> driver.find_elements_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//div[@data-testid="UFI2CommentsList/root_depth_0"]//a[@role="button"]'</span>): </span><br><span class="line">            <span class="comment"># 反覆偵測是否有「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕，若有擇點擊</span></span><br><span class="line">            <span class="keyword">if</span> bool(re.search(<span class="string">'comment|More|Repl'</span>,i.text)) == <span class="keyword">True</span> :</span><br><span class="line">                driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    driver.find_element_by_xpath(<span class="string">'//div[@style="display: block;"]//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'No pupup!'</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    i.click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'Nothing'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文章內容與互動摘要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PostContent</span><span class="params">(soup)</span>:</span></span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># po文人資訊區塊</span></span><br><span class="line">    PosterInfo = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'l_c3pyo2v0u i_c3pynyi2f clearfix'</span>&#125;)</span><br><span class="line">    <span class="comment"># 互動摘要區(讚、留言與分享)</span></span><br><span class="line">    feedback = soup.find(<span class="string">'form'</span>, &#123;<span class="string">'class'</span>:<span class="string">'commentable_item collapsed_comments'</span>&#125;)</span><br><span class="line">    <span class="comment"># 名稱</span></span><br><span class="line">    Name = PosterInfo.find(<span class="string">'img'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="comment"># ID</span></span><br><span class="line">    ID = PosterInfo.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pb8 o_c3pynyi2g _8o _8s lfloat _ohe'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'/?'</span>,<span class="number">2</span>)[<span class="number">0</span>].split(<span class="string">'/'</span>,<span class="number">-1</span>)[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 網址</span></span><br><span class="line">    Link = driver.current_url</span><br><span class="line">    <span class="comment"># 發文時間</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'abbr'</span>).attrs[<span class="string">'title'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_1atc fsm fwn fcg'</span>&#125;).text</span><br><span class="line">    <span class="comment"># 文章內容</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Content = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pbx userContent _3576'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Content = <span class="string">""</span></span><br><span class="line">    <span class="comment"># Like</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Like = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_LIKE'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Like = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># Angry</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ANGER = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_ANGER'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        ANGER = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># HAHA</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        HAHA = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_HAHA'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        HAHA = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># 留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        commentcount = feedback.find(<span class="string">'a'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2CommentsCount/root'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        commentcount = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># 分享</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        share = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_355t _4vn2'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        share = <span class="string">'0'</span> </span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        data = [&#123;<span class="string">'Name'</span>:Name,</span><br><span class="line">                 <span class="string">'ID'</span>:ID,</span><br><span class="line">                 <span class="string">'Link'</span>:Link,</span><br><span class="line">                 <span class="string">'Time'</span>:Time,</span><br><span class="line">                 <span class="string">'Content'</span>:Content,</span><br><span class="line">                 <span class="string">'Like'</span>:Like,</span><br><span class="line">                 <span class="string">'ANGER'</span>:ANGER,</span><br><span class="line">                 <span class="string">"HAHA"</span>:HAHA,</span><br><span class="line">                 <span class="string">'commentcount'</span>:commentcount,</span><br><span class="line">                 <span class="string">'share'</span>:share&#125;],</span><br><span class="line">        columns = [<span class="string">'Name'</span>, <span class="string">'ID'</span>, <span class="string">'Time'</span>, <span class="string">'Content'</span>, <span class="string">'Like'</span>, <span class="string">'ANGER'</span>, <span class="string">'HAHA'</span>, <span class="string">'commentcount'</span>, <span class="string">'share'</span>, <span class="string">'Link'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留言</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CrawlComment</span><span class="params">(soup)</span>:</span></span><br><span class="line">    Comments = pd.DataFrame()</span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 用戶留言區</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 回應貼文的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_0'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg0'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 回應留言的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_1'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg1'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)        </span><br><span class="line">    <span class="keyword">return</span> Comments</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">Links = FindLinks(url = <span class="string">'https://facebook.com/taiwanmobile/'</span>,</span><br><span class="line">                  n = <span class="number">20</span>)</span><br><span class="line">Links</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抓下來所有留言</span></span><br><span class="line">PostsInformation = pd.DataFrame()</span><br><span class="line">PostsComments = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Links:</span><br><span class="line">    print(<span class="string">'Dealing with: '</span> + i)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        expand(i)</span><br><span class="line">        soup = BeautifulSoup(driver.page_source)</span><br><span class="line">        PostsInformation = pd.concat([PostsInformation, PostContent(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">        PostsComments = pd.concat([PostsComments, CrawlComment(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'Load Failed: '</span> + i)</span><br><span class="line"></span><br><span class="line">PostsInformation</span><br><span class="line">PostsComments</span><br><span class="line"></span><br><span class="line">PostsInformation.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsInformation.xlsx'</span>)</span><br><span class="line">PostsComments.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsComments.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> Crawler </tag>
            
            <tag> Selenium </tag>
            
            <tag> Facebook </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲_104人力銀行工作清單</title>
      <link href="/2019/04/18/Crawl-JobList104/"/>
      <url>/2019/04/18/Crawl-JobList104/</url>
      <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>找工作是一件很辛苦的事情，當我們在<a href="https://www.104.com.tw/jobs/main/" target="_blank" rel="noopener">104人力銀行</a>的網站上輸入關鍵字查詢職缺時，經常會直接跳出上百、千份的職缺。雖然有很多職缺對於求職者是好事，但這也造成了求職者的大困擾！</p><p>因此我希望能透過Python的爬蟲，一次把我想要查詢的結果(工作內容、地點、薪資、要求技能、工作地點…等等資訊)抓下來，在我自己的電腦上按照我的需求進行篩選，讓我能更有效率的挑選工作並投遞履歷。</p><p>另一方面，如果想了解各個產業會需要哪些工具、技能，也可以從這份資料中進一步的分析。那麼我們就開始吧！</p><a id="more"></a><p>我們直接先看輸入與最終的輸出結果吧!<br>在左邊的圖是104的查詢系統，我在這邊搜尋的關鍵字幾項條件分別是「資料科學」、「台北市」、「最近一個月有更新」等項目，經過搜尋之後，系統幫我查詢到25頁，共730筆職缺。並且也有跟我說各職缺的公司名稱、學歷要求、工作經歷等初步資訊。</p><p>但只有這些資訊是不夠的，當我們看到有興趣的職缺時，還需要進一步的點開超連結，檢視詳細的職務說明(右邊的圖)。在這個分頁中我們就能夠看到詳細的工作內容、條件要求、公司福利與聯繫方式等資訊囉！我最終的目的是希望透過程式自動幫我們這些資訊都整理成一份excel表格！<br><img src="/2019/04/18/Crawl-JobList104/104HomePage.JPG" alt="HomePage"><br>而我們最終的目標就是將所有職缺以及各職缺的內容都整理成這份excel表格，讓我能按照自己的方式篩選資料，提升找工作的效率！<br><img src="/2019/04/18/Crawl-JobList104/JobList.JPG" alt="JobList"></p><h1>載入使用套件</h1><p><img src="/2019/04/18/Crawl-JobList104/01.JPG" alt="01.JPG"></p><h1>設定查詢條件</h1><p>這些查詢條件可以在<a href="https://www.104.com.tw/jobs/search/" target="_blank" rel="noopener">104的搜尋網頁</a>上搜索，在這裡不多做說明<br><img src="/2019/04/18/Crawl-JobList104/02.JPG" alt="02.JPG"></p><h1>展開所有工作清單，後續將依序開始爬蟲</h1><p>這裡會透過Selenium打開一個瀏覽器並開始跑程式~<br><img src="/2019/04/18/Crawl-JobList104/03.JPG" alt="03.JPG"></p><h1>解析爬蟲資料並整理成DataFrame</h1><p>在正式開始爬蟲之前，我預先定義一個函數，專於用來處理「職務類別」這個複選題，稍後將用這個函數將其串接在一起<br><img src="/2019/04/18/Crawl-JobList104/04.JPG" alt="04.JPG"></p><p>開始逐筆爬資料囉!<br><img src="/2019/04/18/Crawl-JobList104/05.JPG" alt="05.JPG"></p><h1>結果</h1><p>這裡爬得很快，大約10分鐘就抓完這700筆資料囉!<br><img src="/2019/04/18/Crawl-JobList104/06.JPG" alt="06.JPG"></p><p><img src="/2019/04/18/Crawl-JobList104/07.JPG" alt="07.JPG"></p><p>因為內容並沒有太難，因此我就不做太多說明了，不過有問題的人也歡迎在底下的留言提出！</p><h1>完整程式代碼</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import re, time, requests</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span><br><span class="line">headers = &#123;&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&apos;&#125;</span><br><span class="line"></span><br><span class="line"># 查詢的關鍵字</span><br><span class="line">my_params = &#123;&apos;ro&apos;:&apos;1&apos;, # 限定全職的工作，如果不限定則輸入0</span><br><span class="line">             &apos;keyword&apos;:&apos;資料科學&apos;, # 想要查詢的關鍵字</span><br><span class="line">             &apos;area&apos;:&apos;6001001000&apos;, # 限定在台北的工作</span><br><span class="line">             &apos;isnew&apos;:&apos;30&apos;, # 只要最近一個月有更新的過的職缺</span><br><span class="line">             &apos;mode&apos;:&apos;l&apos;&#125; # 清單的瀏覽模式</span><br><span class="line"></span><br><span class="line">url = requests.get(&apos;https://www.104.com.tw/jobs/search/?&apos; , my_params, headers = headers).url</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(url)</span><br><span class="line"></span><br><span class="line"># 網頁的設計方式是滑動到下方時，會自動加載新資料，在這裡透過程式送出Java語法幫我們執行「滑到下方」的動作</span><br><span class="line">for i in range(20): </span><br><span class="line">    driver.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight);&apos;)</span><br><span class="line">    time.sleep(0.6)</span><br><span class="line">    </span><br><span class="line"># 自動加載只會加載15次，超過之後必須要點選「手動載入」的按鈕才會繼續載入新資料（可能是防止爬蟲）</span><br><span class="line">k = 1</span><br><span class="line">while k != 0:</span><br><span class="line">    try:</span><br><span class="line">        # 手動載入新資料之後會出現新的more page，舊的就無法再使用，所以要使用最後一個物件</span><br><span class="line">        driver.find_elements_by_class_name(&quot;js-more-page&quot;,)[-1].click() </span><br><span class="line">        # 如果真的找不到，也可以直接找中文!</span><br><span class="line">        # driver.find_element_by_xpath(&quot;//*[contains(text(),&apos;手動載入&apos;)]&quot;).click()</span><br><span class="line">        print(&apos;Click 手動載入，&apos; + &apos;載入第&apos; + str(15 + k) + &apos;頁&apos;)</span><br><span class="line">        k = k+1</span><br><span class="line">        time.sleep(1) # 時間設定太短的話，來不及載入新資料就會跳錯誤</span><br><span class="line">    except:</span><br><span class="line">        k = 0</span><br><span class="line">        print(&apos;No more Job&apos;)</span><br><span class="line"></span><br><span class="line"># 透過BeautifulSoup解析資料</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &apos;html.parser&apos;)</span><br><span class="line">List = soup.findAll(&apos;a&apos;,&#123;&apos;class&apos;:&apos;js-job-link&apos;&#125;)</span><br><span class="line">print(&apos;共有 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line"></span><br><span class="line">def bind(cate):</span><br><span class="line">    k = []</span><br><span class="line">    for i in cate:</span><br><span class="line">        if len(i.text) &gt; 0:</span><br><span class="line">            k.append(i.text)</span><br><span class="line">    return str(k)</span><br><span class="line"></span><br><span class="line">JobList = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">while i &lt; len(List):</span><br><span class="line">    # print(&apos;正在處理第&apos; + str(i) + &apos;筆，共 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line">    content = List[i]</span><br><span class="line">    # 這裡用Try的原因是，有時候爬太快會遭到系統阻擋導致失敗。因此透過這個方式，當我們遇到錯誤時，會重新再爬一次資料！</span><br><span class="line">    try:</span><br><span class="line">        resp = requests.get(&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;))</span><br><span class="line">        soup2 = BeautifulSoup(resp.text,&apos;html.parser&apos;)</span><br><span class="line">        df = pd.DataFrame(</span><br><span class="line">            data = [&#123;</span><br><span class="line">                &apos;公司名稱&apos;:soup2.find(&apos;a&apos;, &#123;&apos;class&apos;:&apos;cn&apos;&#125;).text,</span><br><span class="line">                &apos;工作職稱&apos;:content.attrs[&apos;title&apos;],</span><br><span class="line">                &apos;工作內容&apos;:soup2.find(&apos;p&apos;).text,</span><br><span class="line">                &apos;職務類別&apos;:bind(soup2.findAll(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;cate&apos;&#125;)[0].findAll(&apos;span&apos;)),</span><br><span class="line">                &apos;工作待遇&apos;:soup2.find(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;salary&apos;&#125;).text.split(&apos;\n\n&apos;,2)[0].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;工作性質&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[2].text,</span><br><span class="line">                &apos;上班地點&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[3].text.split(&apos;\n\n&apos;,2)[0].split(&apos;\n&apos;,2)[1].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;管理責任&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[4].text,</span><br><span class="line">                &apos;出差外派&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[5].text,</span><br><span class="line">                &apos;上班時段&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[6].text,</span><br><span class="line">                &apos;休假制度&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[7].text,</span><br><span class="line">                &apos;可上班日&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[8].text,</span><br><span class="line">                &apos;需求人數&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[9].text,</span><br><span class="line">                &apos;接受身份&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[10].text,</span><br><span class="line">                &apos;學歷要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[12].text,</span><br><span class="line">                &apos;工作經歷&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[11].text,</span><br><span class="line">                &apos;語文條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[14].text,</span><br><span class="line">                &apos;擅長工具&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[15].text,</span><br><span class="line">                &apos;工作技能&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[16].text,</span><br><span class="line">                &apos;其他條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[17].text,</span><br><span class="line">                &apos;公司福利&apos;:soup2.select(&apos;div.content &gt; p&apos;)[1].text,</span><br><span class="line">                &apos;科系要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[13].text,</span><br><span class="line">                &apos;聯絡方式&apos;:soup2.select(&apos;div.content&apos;)[3].text.replace(&apos;\n&apos;,&apos;&apos;),</span><br><span class="line">                &apos;連結路徑&apos;:&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;)&#125;],</span><br><span class="line">            columns = [&apos;公司名稱&apos;,&apos;工作職稱&apos;,&apos;工作內容&apos;,&apos;職務類別&apos;,&apos;工作待遇&apos;,&apos;工作性質&apos;,&apos;上班地點&apos;,&apos;管理責任&apos;,&apos;出差外派&apos;,</span><br><span class="line">                       &apos;上班時段&apos;,&apos;休假制度&apos;,&apos;可上班日&apos;,&apos;需求人數&apos;,&apos;接受身份&apos;,&apos;學歷要求&apos;,&apos;工作經歷&apos;,&apos;語文條件&apos;,&apos;擅長工具&apos;,</span><br><span class="line">                       &apos;工作技能&apos;,&apos;其他條件&apos;,&apos;公司福利&apos;,&apos;科系要求&apos;,&apos;聯絡方式&apos;,&apos;連結路徑&apos;])</span><br><span class="line">        JobList = JobList.append(df, ignore_index=True)</span><br><span class="line">        i += 1</span><br><span class="line">        print(&quot;Success and Crawl Next 目前正在爬第&quot; + str(i) + &quot;個職缺資訊&quot;)</span><br><span class="line">        time.sleep(0.5) # 執行完休息0.5秒，避免造成對方主機負擔</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;Fail and Try Again!&quot;)</span><br><span class="line"></span><br><span class="line">JobList</span><br><span class="line"></span><br><span class="line">JobList.to_excel(&apos;C:/Users/TLYu0419/Desktop/JobList2.xlsx&apos;, encoding=&apos;cp950&apos;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> python </tag>
            
            <tag> Selenium </tag>
            
            <tag> 104人力銀行 </tag>
            
            <tag> requests </tag>
            
            <tag> BeautifulSoup </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲 - 爬取松果購物商品資訊</title>
      <link href="/2019/04/07/Crawl-SongGuo/"/>
      <url>/2019/04/07/Crawl-SongGuo/</url>
      <content type="html"><![CDATA[<p><em>本文章僅限於學習目的使用，請勿用於商業目的</em></p><p><a href="https://www.pcone.com.tw/" target="_blank" rel="noopener">松果購物</a>是台灣新創的電商，截至2019年1月為止，平台上有超過3,600間廠商進駐，商品數量也超過15萬件。僅用了2年的時間就開始<br>我很喜歡創辦人分享以下兩篇創業歷程的文章。有興趣的人從以下文章了解更多松果購物的故事。</p><ul><li><a href="http://tesa.today/article/1629" target="_blank" rel="noopener">松果的創業故事-第1年</a></li><li><a href="https://tesa.today/article/1909" target="_blank" rel="noopener">松果的創業故事-第2年</a></li></ul><p>在這篇文章中，我們要透過 python 把松果購物上的商品資料爬下來！</p><a id="more"></a><p><em>再強調一次，請勿用於商業目的。</em><br>這邊文章的架構如下</p><ul><li>找出「男生服飾」下所有的商品連結<br>選擇「男生服飾」的原因是這類型的商品數量較少，我們在抓資料比較不會造成系統負擔。若想抓其他類型商品的人可以自行調整類型的代號</li><li>從商品連結抓出店家/商品的資訊<br>包括店家名稱 / 店家評價 / 商品名稱 / 商品銷售量…等等。具體項目會在後面提及。</li><li>轉換成抓取變動目標的語法<br>將以上抓資料的過程撰寫成簡單的函數，讓我們可以輸入類型的代號，就抓出該類型下的所有商品資訊</li></ul><p>那我們就開始吧！</p><h1>找出「男生服飾」下所有的商品連結</h1><p>我們要練習的是SongGuo上男生服飾類型，這是網頁畫面。若想要練習其他類型的商品，可以自行修改網址中最後的3碼數字。<br><img src="https://lh3.googleusercontent.com/Jpw1KzETId2E21gF9etMcEIN9tfow7WUtHGGFIkAV5QA6tqdGGBlm1U9RFEreoJXbsRFzyBm1h3VUGGndTSS-8dRCSVudhFEu9ni3m9qZkEY3O5aKaeh2WTNDY0XnXoCJiy-XhW-tw1SL3b7Wbh_6IK70xUgSFo6El7z4ad97l6OgX6GvKe19Nh97fDl3LUenO0lbJFn7QR8p0cyQ8xE0sxHpVBZ477y1QgmKG07yW5z923DNOtBeILN7VDdS2HKNa34uFBvdBqJPCwFzG_DFPiQjEXDDL39ti1oEmx4ivGhGhZIHiNh4TxclpE-QyGKYUibcAeiEvFBwSdA1uTv4vIOtH7oxpx-qzDK5P_EULUetNHMR3MRedOj5k_2Wuumk0L_qySPxMh_Em9IvwpU3G-qYTTk8fMNXnphcE9079SgnGK--sVdBGSDT89Q89EHN8LpspujUhBhYbK3A7vT_2qJV3uuK4TAHYwtw1lWyIQLqm5puHWllvm1J11ql5YcP1HWGFjVXllLqzfy-5Ie8w9OvAmgDETdKx3o5htobHSEx9njlyxXRegvYKF88_Hk_5BR9e47vHTJkXZESv1HL98iP_PAViaT6f1PGfOVyvtps5hgCS4NwXeGa7wQgZDOKjMd-K6qoNCPotugjHcJJxa8Qh6qTenST0cWpqc7zIuguHQezhcyv2A=w1344-h978-no" alt="01"></p><p>首先我們需要載入使用到的相關套件<br><img src="https://lh3.googleusercontent.com/VGzFBH2gVJKJCNzpvclRaixSSz2FHoOLM6QLajqRlJiX_KCJJWHg1u-zlusYdACywtBD3DR3sjr0yGq95Lb4d2DN4-_DLDl8QXAI4FnfEgZCF-ZRUvj9dIN486IbQbhXTOq_KHgx8EuigcXHz6o0CQdwyLIwLEUAzouEgHRD5ydbO7SphPs7B9S_GUgMoTEZS0CBDGQdRlmScSajQ7Qgw7crnJvRSZ0WqE0z5z2oXIyzRzVWwy9_KIcRnhK0Kly3c9F7aFwlZoTl2_wDkKva_UDagq1qThXM7Ri-F3LwK1BkIv9YohsctC0NWxFAwVFdsgGQFpY3fuktHtiV6h6PbRx4nZy70d8IJMxBXibiESLh_AX7GzjLjSDnf5aworUMGH8Lv-hBRhS5DDurocRTmNNi8LPm03N4qQCQ9WvloRsazoheG0UmYzNSnxaELeZF6gPsgWH-luA2Tpdu8Gz49FwzvHrMMWwIr9gwIHo_3QGd8QQxmiFoHnKjek2nOIOMeOes2qWno5CI7cLav7zP4KIuinSiggBFTz6-5jtPCLZ0qzfcFfqi08b3LU7c9rJkXLNrmDWsTu3z_pWbhS_dC0fam5yLR1IgiwZO6ATPgb2qbATjr4qDb2oUkg6-lA78yEO6QsaglsysTt0G5qQu1Oa4YALF_LqUgQzvgdjYqxSHpuPX7qpZfOQ=w1075-h91-no" alt="02"></p><p>設定待爬網頁的網址，與使用者資訊<br><img src="https://lh3.googleusercontent.com/HxXaKaA-E6ZRdphbNFoSX21KCL-PTF30mYn6DotreKD7b7JNt7WXcZ45vqTbhPUZnKRV9cwqn2gTBI_5ziWjBLnGLmg7dtRUoQyvPfA5lIEduP81S056m-6uxewMPzkoLeTfz60y9upl-jVZfx0fBA8wyc_zFl6DO-r0-bqOP7W4xfa8GW89cXOoJadIAzovyqw1-8Y2B8UIiI5e7pBeiU6JMlIuyVQ5G6IjrO9fvP70OA_7CnhQDvrNnHb0YFxw96afJtrtDOES5-cqWvpJ29pUWxui5LmJU-gEGOv8lv4XzzEgdHTANTmBuTVFr6D-xszbMV3jA7wXEEJe0IqJmmouBTbkI3iQS8TC8g4FRUjpv7uuG0DiyNT0H4Gt8gWSG9r2ho0jdScFQJjWdVopwWM1WdJCaHUfAdBrgWEaG-gPR-uOcpvJbT6GsHw2c9ySpgOqGlfOoE5NGZJcZUPBU25XOhyZTQlwRU1AyrESzjUcfy5MtXXr_4IZClunRb8lrAPqjfdgm95A3snaCu5yz_BlqsK7xkRNRDKgAyVQSBMOiW5Wgm28F3YODbZ7cmyGBIux0oTTsaVueZozljIBdgLh3LotA3kQiDxAXFZVxB5sly_JOeHJP7Y4gm2HFW9Gutzwpb3fZZIYjB_xqdHuwJAfieq0PNMIEVDRTdQzzeeQ4edGhF5m3EI=w1083-h140-no" alt="03"></p><blockquote><p>需要注意的是，若沒有輸入使用者資訊，很容易被系統偵測為爬蟲系統，進而阻止後續的爬蟲作業。因此這裡需要加入這些資訊，藉以模擬真實的瀏覽網頁情境。</p></blockquote><p>接著我們可以透過這段語法找出網頁上所有商品的連結<br><img src="https://lh3.googleusercontent.com/mQ1Dk3nR0wHJq6L-IIxgAT0lySsQjoue0JxmL9TvmHCzxDnQTctFYsjbe42FM82GMqCbDHGp2p9HQUgAlpEXtAQjkKCff6KJGQN8FTLxqr2WrqLjFf0b71_liWfTlLscbzEj8o-ZKozfSGG8WHyZjuPD6vCW5XmoUL-altQQZvZovp7i4ADQmlYX3R9DU4Kt3qlQJ2A1qaS_1L6uwivbHFTIA7Aiqh6oQG6JWioSq-th3q0Vs6e_8pN1plZGEuh0PCQaoeBZQdtIad_4pwMaMFSsfYOqKaOrzEDC4rZWiYpYLsqFwj5UdLbvCK21I93A9Sw_eMHzqZaZEQrhAKUjMewvDhNBMmnFcQwzG9eBYJ7ccuUyNdO7b4ToVVOijtKwxWYPLBqivzunmxnVuoS0IYzN3qM4jpeUPVHlZrOec4uTInd1ukeCgYc47UXEQk4Rt8PIxBFI4sGhvEAhnUWPa6YuC3b7wjkxZ3gjS2lYM7jz-ZHuWe6Hc-NhzVK8q6efVzyLpAebhyUhoP7UBLPuQhGAFsrpTaYgoVmbsMRted67xE7XMNBkbUZmj9oGqfuL6TJEzNg4Ka9Z_-QKb4PzgGVJMqeVcrPt_zK1s9Qbj4zcLaU3AdL1jFqaEeoCqGMTuX_4eeIFz0MW35I-CYfkdF_htC61BGOQMwW6wRRAc1U_F2-G8EeEdtE=w1081-h437-no" alt="04"></p><blockquote><p>需要留意的是，透過輸入’a.product-list-item’的方式找出商品連結只限於用載SongGuo的網站。如果想要在其他網站爬資料，則需要檢視個別網站的網站結構。<br>具體的方法是</p><ol><li>透過Chrome網頁開啟目標網站，並在網頁中點選「滑鼠右鍵」的「檢查」功能。</li><li>Ctrl + Shift + I<br>畫面如下<br><img src="https://lh3.googleusercontent.com/R9xMEOy4pZNHxlffuAYaKgECW1cb4TvVCchsLMkcrpp-Hsjt2JUtV93jL6sltGCB1inPxphnHX1vzSwnu1dg_-C8saC0_-r7UDd4RroQKbzWlQHWnEYpRpeY6gTx8vlBtcF4fC0lZBZpUfNTTD8QFqUwl7KgFKq70Sdd-EbU78Uie38pMmJg7jMhPunoo__dGzTU56OaF7deNYsyKd8ambASsWwlxoNXIz3gGQZVfKAAPfYtS07SxPzHpMI2LSzMWBgC5Qcq6poanBe2VnhtACFAXB2ldDY8__JlFU93C2hM0gK9E2xx3-zdIkvu73ms9i-f8ne9h2ZMJLvC_ElFXO0igMQFTBUb8IZyMfKV_MtKEinZ3z8K67-PuzYdPJvS5znBuvNM5S8Bt80-o35bIiZvEYApaON24AjFNkff9Gx8OqTz_8ia6LaAi2t8bm_DVmmx_oN4PekDB-JzAB_ESLP97fZoOtRjNqU9lGlxJbQA1GZHY8lQiIJVrAg6O_zoU8CsWTCe86aLRjy3tEj9ux1tgRhWASB2ib1vrycngC5b4L5UPtJ-Znqr0OxWIlIZvIZboFqDK00MteOD5rlqXYl64scZ3BvkDzoB_Cl4Dz54ZfsBPxuKhkLbxeb-ELQg6fPdclZFtoJIkFaz5v7xOeRAP1IMVrmqR2sBGEjMrbRR_YmRFXhxZ98=w1833-h978-no" alt="05"></li></ol></blockquote><p>從以上的畫面我們就可以看到，商品的連結就藏在各個Element的href屬性中囉！</p><h1>從商品連結抓出店家/商品的資訊</h1><p>我從中挑選第一個商品連結(/product/info/190117048588)作為範例，商品項目是<a href="https://www.pcone.com.tw/product/info/190117048588" target="_blank" rel="noopener">【瑞典】旅行折疊電熱水壺</a>，網頁畫面如下<br><img src="https://lh3.googleusercontent.com/dTviyk_OLCkbzdQFUKmDtx2Y6W3MOLL6XMgiEDMIWUkeNtYU3PwbxV4m6W-UDYUceamyDL8uB8OP5ae88xAimLNpFvpFpFaO-bR_sAId1t5supGvNHij3u_VSoU4HMaHdsJtiuXfjPMwi30Wisx-VWnbGT3PiTDgr024NV34Veuy8CkX4PTdfJQUb9lI2nAUkGN9PXtfG4gNQ7DlhtranOcRN2UHlOBP8LEzKOlxNZb-WtAMvQAWaiUnhsJEVrrb8aWGObXMjY_CySXb6_szvmTSIldfny0hB3w3qdJS8M9D8Sl2Upc3bSofH5QIvvv4LLu3qw1SPjE4yRn0VeiCINRMemtqRNCnC5Iwt-9yHtmDUjbZPTNnE84C7D2bWstycMgkKg81ptD2rC8poN2eI7CnH-iJN_5vxBvkM4mA65DaYoA5G8tgboI_GvjZy5tmgX0nXRRx3knGc6qVNrKD7sDc_HosDw4BRnGLZlyvNX7Rk2wjyQEkSQz0fFxqsF5dNtE9vcpnmj_AfqIjARrp6rBZ6C0vf8hj-hnt1W5whKctUmv89K1HHJFaDfV07dUjxWRVyEs16xgxq7CBfKovhqr910QYTaAJw81aU2M1oseSuShY4K5hcknYGfH8R8cbd0bSInTxVZPfGTBWXLNEAKh33vta_jJSwLVLLVwofdr1BqouUg3rnUw=w1344-h978-no" alt="06"><br>這個網頁中有相當豐富的資訊，包括店家名稱、店家評價、商品名稱、商品價格…等等<br>我將從網頁中提取出以下項目的資訊</p><ul><li>店家名稱</li><li>店家商品數量</li><li>店家評價</li><li>店家出貨天數</li><li>店家回覆率</li><li>產品名稱</li><li>特價</li><li>原價</li><li>折數</li><li>商品評分</li><li>評價人數</li><li>收藏人數</li><li>提問人數</li><li>商品分類</li><li>商品標籤</li><li>連結</li></ul><p>找出資訊藏在哪個Element與屬性的方法同樣是透過Chrome中的「檢查」功能，以商品名稱作為範例的畫面如下：<br><img src="https://lh3.googleusercontent.com/2cJ9Wg8E48xy-KFa1OFrqOuSvjhT8xGvIl4TbBBoCYUYWbKmkXnkOVbozUG54-UDJiNDF-rtKgM4qzGRGo6WReE9vUVQEXe6aR_DpvHGD9Wr5Ft0SHpdhuogbN8HkKo0Wh1hMsC3EEoD39Xcg9SZWE_UmMPJDLYKNr182WByZNQ8a7WTHI4RzPh3Y1ceOJ-JOuajuylFQHKHOKldCIhgAtrKowOulzyewry5rTkV_tNu8E_PwgtwrxVsu96diY6i6BY6CIGbpvLOEhPJ2aqEijliWlaLDls_vsa4NqJNqBGAbxgZoWr5D5Q7Fxb0NXqreok6XF2cUdE0EMb6A4NhDeznayjpGRGwZ77UkFFgo1zCswW9IDGlyjJMcrpxS7qktsDJ9o2xG_kfDwFwkSMIj__thTzG4NO9qjmFUopysBiDYoY_IMFZM5KJglkcaS4pH8v4b7cdkZIzHkjn-e12a0bwXgCajWvWG_VqGjOA4zq2ER66fq3K84YuffFdhKC5Z8aLUcx9V9iho2SF-5PqhaSk2pdEt-6kYRfJFPdbufW13eEXp5viYS3HIfN_N3pYFajTqoOoy5BnNuNDdlRFc4GfKsjfDMXzBUvVy1Brs640NXfntJEEL_Ir1_v0N2EUq4X7IWzF5kAFunowG09wc9wHug-tfdKnsQoUtYbo9VsmjQ4VfVayP6E=w1844-h978-no" alt="07"><br>這邊是找出以上項目的程式代碼<br><img src="https://lh3.googleusercontent.com/rSOr9p5_Hvk7ucpdQWe8aNUI3f7C4MIUsLyB7kISrkDxj7Cz19obNxjNAlpOoXO8CtvSFXYbcBRQBMh0EC6QE-9GI64c2tNaBuqIJuuGYYE_J-d67cNc7FcYcJzzv1_uAy9xCtu9WsrVV-80rMgQlNQ8Pu6_na4_h1n4TFSPY4p_3qqCA5QEtZKKqg0VUq7oMO54hopLYu0WIgoSN6ASjIIjYY4Dx3mhOUgfqCzHCxagXrGX409pq8LqKHmzVu5s1jW451WzyNTZKj66GZwdD8omQnzPyfTUf7NPSPhpbUE-hBbPh9xw8quUD0cXw7ZIrH0ZKDFCJDju8TYd5ZCfx2VCGKkxdvrNmhkcKOvbNpXgdx9QVGhng2MDoGyIf63cEkZc0weC_TD6_06LRDUgLZ6INhZ74lh1T14kpoDOTsq-XNpms43xkvfHbMKk_QQAWQGzkH30ZUQbphOPLDh-fbsveyRKt_XlXVP2fG8QMsCi5x3jOPAwlQ0_Y1DCQcshDK1buGcGXSIm1co4k3hycgvEB0d11yPJaXVen8qekp2blRyZsbRBNmfkpSK1JLAf2xcC4YRw9qXrCQNhRfebQH6ZTW8CG0F3FO6XN3jH4EFdVXG74R7zoEgLpBsCWm62RfAt7q1smUMYY9hjYtKT3keDnsVYrCiBDi1mmhwfk2RoGGI08sdCWPM=w1093-h677-no" alt="08"><br><img src="https://lh3.googleusercontent.com/jiB3fz5HTgrGzMWY-g26LO0LWz_2DDNTIG3M7F_bg_pJIQtjBF01nRokBAvlOWeuUmKD32hSRkTL2xiraQXX9TwvgZwA-gl_zBLIZKOe_jiSRGdOWXVmmVlyGE4eF2752NsErFtwgpTI_gGf6_7xyeHRflyhKWXkpPvk0h9Em-OGd0USm2AlXe1TO1BGRcRe1hlOXU3r6nNuY_8wLZAqLaZAxseexwG6ugEaUP_ioGh8c5X9thqEJfHCfPUvGMyHt40joss9SxHgiFOOflveQ3tjBz_MwOIkCjNNnxNSO5r9z-TUUR95Jk0v88K03BKMCKz2qY-4RX9iOWECxmME6VnnHvFBu2RMyXaNgqFjM0degFHstky8iMKnSay2XfxwgPHyH8uVbRbjSxGG9UIuLfX7RXi7kcG_AiIiCS_bnEakZLyayY8nrIN332IvvcWm04E_723xy4_SKQQkBleXn2zNlP2GvHBT3kFs4R10SxDEs_x8wUXFdWYiOUvGj5RBF5y2e_KeDQpwWA2qVOUNh08hUiNcDeTj5IwRmfF5KPQycWJKxkEjIq0y5CRFdqnhwpraRoELhrFUg_nBvI2f6NZ6Ej3lAM6rCYWe4zDHduIeBe-5IaFH-47W9O6G1B3LGEViwez9bJTD2zgN4y12am5UjeF89UvTE_B_iIIBbPL92-S49sRgABY=w1089-h790-no" alt="09"></p><blockquote><p>其實客戶的留言也是相當重要的資訊，但礙於篇幅這裡就不多做說明，找出節點與屬性的方法是相同的，有興趣的人可以自行練習。</p></blockquote><p>透過以上的方式，我們確認用程式把商品的資料抓出來是沒問題的！接下來我們只需要把抓取「固定」目標程式語法轉換成「變動」的代號，我們就能自動抓出所有的商品資訊囉！</p><h1>轉換成抓取變動目標的語法</h1><p>簡單整理一下，我們在第一段的輸入是某個商品分類的編號，回傳的是該分類下的所有商品連結。<br>而第二段則是輸入商品的連結，自動幫我們抓出商品的各項資訊。<br>因此我們要透過迴圈，逐一地抓出(第二段語法)所有商品(第一段的產出)資訊</p><p>定義ProdList函數，輸入商品分類的編號，回傳該分類下的商品連結清單<br><img src="https://lh3.googleusercontent.com/7SlOEdsz7ndW7COAbcLJ5DNCzhKtrlHsr4zBBa3oAU2DHluoResMGY2d-27VMsGCB9vji48lLpfKuNJ7HIAs5e0I1bvCGNSp0cNnS6rL6BEU_mx_2Wib0yERNYZRex1e2780N9qmnO4uxfovc8ivcYcPK0-praLR1S4PJO1ji9VUmB2EuGg_Q4zXzWUMFXt7xc1gr_rGznmNrX9H-rj9JoNGN0I25_yfuMk2nA0fnCY1HYvY_9SdWVLlkn6ENeqoMTy-MxKliGFPnCDAqyulEa85NgArvuhIapmNd5-oYWBPGrxcq41asaOynILecU3p4nq-5r1JJMVxofmM6LquTDvzW-sCh99sTPiN6AVA1-l_mpee9XE7FM5b_DjAObZko35IhuDrcKbMj2JGOP7VIWN_8ZOp2JHctFXafADKIEO-UJ6NFKXrZC_-k7_eyJurkCkyhJ04jkg5c5iUVq6Ly6OkqDCnQKY1Zs9_pwIPLueVt7CYqgSv4MFK5n5805qSw745EdoKDM7M1wv5Se6cIKYBlA3PWSNotIWzEv4Lxsb1UarLTWdpZz_T-qZ82XK2Jun4U9wJ4QnPt1L-gRd98LMpNqXtnWPDM0jwRLMYAxafBm2tqj8EZVX0Hida0OOjkTQKG8eBlBdxrOmvpg46soh_zDu1i_eg1QIp77C94RRGgyaMyS4Ufv8=w1092-h93-no" alt="10"></p><p>定義Crawl_SongGuo函數，輸入商品的連結，回傳商品的資訊<br><img src="https://lh3.googleusercontent.com/6yrE3PjDwcWQk9hczw3vnh-A66Q1zKLgCw-l2PfmiyHVeqaM5GBjg5kn-JqKTLIfkhsO6CouisMoMlBpyWXos0dtSvakvKup85UMe3kdef5f0ZLLBJLEsUsQousaCqugA6vrRnqwyXaymRu_3mljpsiHZR0MBsOibfoz45c1JpAKuM7IsGCGlmZ6rlZhyB6yCRrkMuwwZ51r0qzEEsJHmtpjq77Fx0G31LllomGIrhdWAe6RvgBwv9bi5x573NwDb9g9Zxh8L2yPBx7hHwZEnsLqIo0CtQRjBvI0nhfToYQeHoXUkXopNCqb01Ej29CkqpkYPZq8yRQGzpvAjgmTtwwwIpBeDVi__tbCTfUIGKgZqgNPIKE82QxEKLj1SMd0TVDKYeW2z6ln-MiI-Odlc5l9_ZEqF6bDMeZd2Nw9iTVM6rS-kjRkBkWJ7IyEi8lxbJbIW7hrC9HmYTGhzr60nY5jkmEvm_GIbDyzY_E65R3HWTsS70pMwWAoD0SSEn47FYqNggH3Mj3fCcTVossvRMHaFtCP675p0P7ZL_7yc-eD3eBLohRNzLw3huQVYD-QoiUROPq4_VwjXqHffpl4ltcTmX8PoKpDG6mEMczia0Tko-wpdHin6QG6ggRA5OlQPSijjGLX_IHfdTNAVkIWO6jj2bIlae4ZN8PSUfPGPOvoefN32IzzBhk=w1091-h446-no" alt="11"></p><p>結果以上兩個函數，輸入商品分類的編號，抓取商品的各項屬性，並放在df中<br><img src="https://lh3.googleusercontent.com/CVUlflxHvvuS6kvacjRepoZ508kTfX8EBRqP7-MubmxAeHdxJV3c9a3TTF-gvr4CDxvdoSnOBDHayLUU_3u2yp_9eGTrq-Q2-iOeUQU17sKj9kd2w-9-9EIYpPdTWKYwBro0ODdGZ0M1auPSLrd93oVCH2RoJGBAhi2P2LYPrBzTaZRAlIsWMNk6Jm2qR4719Sci9M0SpwHPd6Igfz1lUofLhiTBrohyCY9I4-06o3QmAK83pwvb76itxZEUzlhUo3uHB5fi74nipssCfXDPpbpLG7jPjhiR96HkEfdMOVJLznrqw0Fg5dMNKjF-DEz3O6RZwkLidl2mOnu0deZwrorhGujysoxTxVY4Bo4nTfRvVC565R7_M9EhNu7sr5MYH1Ryag-5dn_j_GKFOIbDSVR0RRQYfLd0ZbN1sK4hiwh6JcjOhXqI4xctP-QEBt6FXWbXeykikoCADneIoI6GF3QleEZtcNYduoep5xHeJoqvUKJC1i47-gFn4Qd0PNOmnNL5EojQVU6EQBW9ehrYi3YDN43NWH_rDUFaIlN2O7JFFRHIdEX-_OnXrFQLvpI_13J_AS7QfopssE_L-gbEqj0l591az5lkAIh1SD_KYmDM5IkK3Vrx5SVjV35x9QPNVeOlisDPstISVuspRv8eVw8cEi0_I6yf4O7c0oBZ1nW61QEJeXur3Z0=w1096-h334-no" alt="12"></p><blockquote><p>這裡會輸出的商品編號的原因是，我在Crawl_SongGuo函數中刻意加入print()指令，目的是方便我們追蹤程式是否正常執行。</p></blockquote><p>系統執行的效率非常快，不到1分鐘就執行完畢了，我們來呼叫df看一下結果吧！<br><img src="https://lh3.googleusercontent.com/iyFNqo1VEO8Z5fFjGMQR-Tuy4NtiMdwYcJslILRYsKvlyIc5Vpw9uUvd3vDJatRdubrsytwLxpncZdQzDoZNFDwp5yHCN780CRY7_xF5MbOQX5CYGlBZ1MXRVjhRqnfceScKBZ5s-Umnmxanvu59h5W9N_iVsT4H5o6TK-oIfpDwHFcEIrHEtwFak6Yghftxok_41WKuIjt5dIPJm9lx5yp0tGT8dlMXWnNq1H6semxrBAhjkEosxtQS_8ooFrSK5s3RZC4f6f8aom2lsgmQ7YQSd_0Zuc8wdV1aPTtF8E6bSs2AsXZ7cl6sEkr1uPDdWWSj6i0mS4TQOCvei4tGYoEBHY1KN5MKmpNtGylM2aHxxL9Ze7-czaa28LrhDs30kmr3rFeKCaxrWtgc5aYcQZPsPdxkhfRHHrCkLneZDORN3evBfcDDSI7h7nHPDJZDhADgLhR4bTQREBrDNbPrqGkxjE7xP-0ezfvg0SdSR9Gl8J_BI11YbXzmJVg5RUXtx2lXoDYSFWCUAsGASVKu-8MNKgN3xnZt6tGD5aGlRk1YjnpOntnkLDhE-Qp_sn9EnrfF310VGBpGEHtqbluDF01TCuPcJ0gB_6H0Rzocp4iXVj_cBXtWIOfFTJYmjUj5JseT7wTuiXwvYuoVZ4pCOHTeZZSHSroGYaNKIBmnMXfxZoVgqYRbBrc=w1608-h566-no" alt="13"><br>因為畫面的限制，只顯示出第一筆資料，不過我們確定資料已經以DataFrame的格式抓下來囉！<br>接著我們嘗試把資料存成excel來檢視吧！</p><h1>保存資料</h1><p><img src="https://lh3.googleusercontent.com/MVXZ9tNyUPYJ7byE37nW8qFvupJXZTVPsTWaGF3OzQsQf6-HMZIWOIKAwUjHjIK8o12yBYOTFxqurA4uim4nkv91ajlmw-2rtj5qeT3nmbmYR1poE0JEr0eP_F9cAx_hHjK6-uP5SaatWkB5CUEEEA9PqjV4VIjBfI2P2YWERtTld--PbjXoynL33JSj6KWigfT0UqbShFD6YUKhA0CmufiUZ_Tg0X6bP7Z0PtncM_tWSU64dOFtB-QyX0mvTSbHgNtdjyTPvW-MNydLoGdezO9U96IdHTAlIpJ27Qcilx3h_CCyrU8YOnQa-NCCVfM8UBYYm7pj-EVqb5hC2NIuUraBbWDlAeXbEcULjVFw5bqD1TQZXTKJXs6z6JmejnZRbH-86BsSWxAG-YQokkrwfD_0aOU4oSNgO5Fg9kdvELlt6caM1OjwgXN4SendRsPD93X2Zne3Y6Ll7Ru-5EQK8lIvcvRXuMNU6aWPvJt74GpNQlPqAsfmihHP2MsgCd8YlwHgATUs_dasYD82IiR1xqJEgyRMwLJjq_Tbm7c1fvL_dG2jx6xw7jy_7wusaya_TMasJzlwN-5e98W08ug0j969ZCQEfjWc1tGYNIHoGu3azsRrCm7A9MWrizpqHfmoVGEpmLPr4uPj4DL6evPRyHwLjaE9sfuEbwV707nI-7TQNXqlSvQS5LI=w1611-h56-no" alt="14"><br><img src="https://lh3.googleusercontent.com/M9MkCw4o_uqvo__BsBU_flCesCmQL9saEP8VZfmxq4RqrHdD4sUC9lGYixkK77hTjHKtnBbprgBvhva15BSgnMIypSRfHGPt5tA026bpj7Dtm7hqjWhoNlNzL49Zoz8UmWpqofMjWoALa--Juk_uWUYH0wJMSdtiirGKxUctOPjFRr3rN0KsACbJ2QCETX9oi3XQHVCrrcUoApXqD6RvJPblpBuxlGLgTPHTEQ0iikdoD8rZ5d0Y82rgb0oNsnFYKsZGNTm1HcDmSSffNh5PnC6K6hEOpj8J3nnQ6bReem2kYNxtqLPzQIhy7m6kTSHS2_-Yb9i7AZd0p64oRt26ou_0y7vJ-OzlaUB1murjBNFSg_uOnD1PYpVHkIyk6e3xqQp_lIxvg3aZhLmZYKPe3AEQYm_mCRrGLUq1pVnIScU8Ny_z6Y7kLaKGZRZHy6jm4BHkqoVA4om5w1nqWchhzb0KBviHMO-Xjd3K9C9xI0miUKOG9d5L3HlIcodb3Z1CkXpdFMoBfbp3yl6YMEHdhZ7ZLO-4M22yJNwLAswyAmOvxdpeT062wcr3PZddVzPP_bRODt1fdIkAvignhSLbknXhKhGyRQQimNyoNFhYyhfi61JJxjTWrqjWtKwD8Wxua7oXipVGkNo_uokPPjwGr-UdiY_lqPqkw840hHzjVBsHBUE7gYCk3u8=w1882-h978-no" alt="15"></p><p>以上次這個爬蟲的說明，有問題可以在以下留言區提問~</p><h1>完整程式待碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入相關套件</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸入爬蟲網址與使用者資訊</span></span><br><span class="line">url = <span class="string">'https://www.pcone.com.tw/product/'</span></span><br><span class="line"><span class="comment"># 男生服飾</span></span><br><span class="line">info = <span class="string">'327'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視是否成功抓到資料</span></span><br><span class="line">resp = requests.get(url + info, headers=headers)  </span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line">a = html.find(<span class="string">'a.product-list-item'</span>)</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挑選第一筆資料作為範例</span></span><br><span class="line">resp = requests.get(<span class="string">'https://www.pcone.com.tw/'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>], headers=headers)</span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 店家名稱</span></span><br><span class="line">html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 店家商品數量</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家評價</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家出貨天數</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家回覆率</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 產品名稱</span></span><br><span class="line">html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 特價</span></span><br><span class="line">html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 原價</span></span><br><span class="line">html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 折數</span></span><br><span class="line">html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品評分</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 評價人數</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 收藏人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 提問人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 商品分類</span></span><br><span class="line">html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品標籤</span></span><br><span class="line">html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品連結</span></span><br><span class="line"><span class="string">'https://www.pcone.com.tw'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義ProdList函數，輸入商品分類編號，輸出該分類下所有商品連結</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ProdList</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(url + str(info), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    <span class="keyword">return</span>(html.find(<span class="string">'a.product-list-item'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義Crawl_SongGuo函數，輸入商品網址，輸出該商品的各項屬性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl_SongGuo</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group(), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    print(re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group())</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">            data=[&#123;</span><br><span class="line">                <span class="string">'店家名稱'</span>:html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'店家商品數量'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家評價'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家出貨天數'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家回覆率'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'產品名稱'</span>:html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'特價'</span>:html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'原價'</span>:html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'折數'</span>:html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品評分'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'評價人數'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'收藏人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'提問人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'商品分類'</span>:html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品標籤'</span>:html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'連結'</span>:<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group()&#125;],</span><br><span class="line">            columns = [<span class="string">'店家名稱'</span>, <span class="string">'店家商品數量'</span>, <span class="string">'店家評價'</span>, <span class="string">'店家出貨天數'</span>, <span class="string">'店家回覆率'</span>,  <span class="string">'產品名稱'</span>, <span class="string">'特價'</span>, <span class="string">'原價'</span>, <span class="string">'折數'</span>,</span><br><span class="line">                       <span class="string">'商品評分'</span>, <span class="string">'評價人數'</span>, <span class="string">'收藏人數'</span>,<span class="string">'提問人數'</span>, <span class="string">'商品分類'</span>, <span class="string">'商品標籤'</span>, <span class="string">'連結'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 組合以上兩個函數，輸入商品分類的編號，即自動爬出所有商品的屬性，並將資料存在df中</span></span><br><span class="line">prodlist = ProdList(<span class="number">327</span>)</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(prodlist)):</span><br><span class="line">    df = df.append(Crawl_SongGuo(prodlist[i]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視抓下來的資料</span></span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將df轉成excel並存在桌面上</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/SongGuo.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> python </tag>
            
            <tag> SongGuo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲 - 爬取Dcard文章</title>
      <link href="/2019/04/06/Crawl-Dcard/"/>
      <url>/2019/04/06/Crawl-Dcard/</url>
      <content type="html"><![CDATA[<p><a href="(https://www.dcard.tw/f)">Dcard</a>是非常適合練習爬蟲的網站，<br>除了Dcard台灣熱門的社群網站之外，Dcard也提供了非常便利的API讓我們能從網站上爬下文章。<br>在這篇文章中，我將展示如何透過python爬下Dcard上的文章！</p><a id="more"></a><p>這邊文章的架構如下</p><ul><li>抓取一篇Dcard的文章<br>具體項目如下：編號 / 標題 / 引言 / 內容 / 發布時間 / 更新時間…等等</li><li>一次爬100篇Dcard文章<br>透過系統提供的api，一次抓取100篇熱門文章</li><li>爬超過100篇Dcard文章<br>因為API限制一次最多100篇，在這裡我們透過簡單的迴圈一次爬1000篇文章。</li></ul><p>在這裡我們先練習爬文章內容的方法，若想進一步爬文章底下留言的人，可以參考補充資料中的範例，以下我們就開始練習吧！</p><p>補充資料：<a href="https://medium.com/pyladies-taiwan/%E7%88%AC%E8%9F%B2-%E5%BE%9Edcard%E7%B6%B2%E7%AB%99%E7%9C%8B%E7%88%AC%E8%9F%B2%E5%85%A5%E9%96%80-iii-ded52759d922" target="_blank" rel="noopener">爬蟲-從dcard網站看爬蟲入門-iii</a></p><h1>抓取一篇Dcard的文章</h1><p>我們先隨機挑選一篇Dcard上的文章作為練習，我挑選到的是這篇文章<a href="https://www.dcard.tw/f/funny/p/231030181" target="_blank" rel="noopener">警察閃光get</a>。</p><p>文章在Chrome上的畫面如下<br><img src="/2019/04/06/Crawl-Dcard/01.JPG" alt="01"><br>從網址列中可以看到這篇文章的編號是231030181，因此我們稍後會透過這個編號來爬這篇文章</p><p>首先我們先載入需要使用到的套件<br><img src="/2019/04/06/Crawl-Dcard/02.JPG" alt="02"><br>將這篇文章的編號透過quest套件讀取，並檢視抓下來資料的結構</p><p><img src="/2019/04/06/Crawl-Dcard/03.JPG" alt="03"><br>透過比對網站顯示的內容與上面輸出的資料結構後，我們可以從中發現id即為文章的編號, title是標題, conten則是內容，其他欄位的說明如下表：</p><table><thead><tr><th style="text-align:left">欄位</th><th style="text-align:center">說明</th><th style="text-align:left">備註</th></tr></thead><tbody><tr><td style="text-align:left">ID</td><td style="text-align:center">編號</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">title</td><td style="text-align:center">標題</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">content</td><td style="text-align:center">內容</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">excerpt</td><td style="text-align:center">摘要</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">createdAt</td><td style="text-align:center">發布時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">updatedAt</td><td style="text-align:center">更新時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">commentCount</td><td style="text-align:center">留言數</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">forumName</td><td style="text-align:center">分類</td><td style="text-align:left">中文</td></tr><tr><td style="text-align:left">forumAlias</td><td style="text-align:center">分類</td><td style="text-align:left">英文</td></tr><tr><td style="text-align:left">gender</td><td style="text-align:center">性別</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">likeCount</td><td style="text-align:center">心情數量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">reactions</td><td style="text-align:center">心情細項</td><td style="text-align:left">把以上心情細分為「愛心」、「哈哈」、「跪」、「森77」、「驚訝」等類型</td></tr><tr><td style="text-align:left">topics</td><td style="text-align:center">標籤</td><td style="text-align:left"></td></tr></tbody></table><blockquote><p>在上表中的心情數量是各種心情數量的加總，若想進一步分析各種心情，可以再從reactions欄位提取。</p></blockquote><p>我們來嘗試把資料轉換為DataFrame吧！</p><p><img src="/2019/04/06/Crawl-Dcard/04.JPG" alt="04"></p><p>確認可以透過程式把文章爬下來之後，我們就來寫個簡單的Crawl函數，期望只需要輸入文章的ID後，就回傳爬下來的文章內容！</p><p><img src="/2019/04/06/Crawl-Dcard/05.JPG" alt="05"></p><p>接著我們就透過Crawl來爬文章吧！<br><img src="/2019/04/06/Crawl-Dcard/06.JPG" alt="06"><br>Good!</p><p>確認函數能正常執行!</p><h1>一次爬100篇Dcard文章</h1><p>在這邊我使用dcard提供便利的API，讓我們可以直接快速爬取資料<br><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">dcard API</a><br>以下簡單說明這個網址</p><ul><li>popular參數：若設定為true，表示按照熱門程度排序，若設定為false，則按照發布時間排序</li><li>limit參數：限定在0-100的數值，表示要抓多少文章</li></ul><p><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">https://www.dcard.tw/_api/posts?popular=true&amp;limit=100</a></p><p><img src="/2019/04/06/Crawl-Dcard/07.JPG" alt="07"></p><h1>爬超過100篇Dcard文章</h1><p>由於API限制最多載入100篇文章，如果我們想要爬更多資料，可以透過before參數與迴圈進行!<br><br>before參數後面是接文章的ID，讓我們可以抓取某篇文章之前的資料<br><br>而透過迴圈，我們只需要把之前抓到最後一篇文章的ID放入before參數中，我們就可以抓到這篇文章的前100篇文章。<br><img src="/2019/04/06/Crawl-Dcard/08.JPG" alt="08"></p><h1>保存資料</h1><p>將資料轉換為excel保存到桌面<br><img src="/2019/04/06/Crawl-Dcard/09.JPG" alt="09"><br>用excel檢視抓下來的資料<br><img src="/2019/04/06/Crawl-Dcard/10.JPG" alt="10"></p><h1>完整的程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入使用的套件</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視資料結構</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line"><span class="comment"># 透過request套件抓下這個網址的資料</span></span><br><span class="line">requ = requests.get(url)</span><br><span class="line"><span class="comment"># 初步檢視抓到的資料結構</span></span><br><span class="line">requ.json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將抓下來的資料轉為DataFrame</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line">requ = requests.get(url)</span><br><span class="line">rejs = requ.json()</span><br><span class="line">pd.DataFrame(</span><br><span class="line">    data=</span><br><span class="line">    [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">      <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">      <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">      <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">      <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">      <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">      <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">      <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">      <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">      <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">      <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">      <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">      <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">    columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撰寫簡單的函數，透過輸入文章ID，就輸出文章的資料</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl</span><span class="params">(ID)</span>:</span></span><br><span class="line">    link = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + str(ID)</span><br><span class="line">    requ = requests.get(link)</span><br><span class="line">    rejs = requ.json()</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">        data=</span><br><span class="line">        [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">          <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">          <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">          <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">          <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">          <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">          <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">          <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">          <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">          <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">          <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">          <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">          <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">        columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嘗試使用撰寫出的函數，抓取編號231030181的文章</span></span><br><span class="line">Crawl(<span class="number">231030181</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次讀取100篇最熱門的文章</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100'</span></span><br><span class="line">resq = requests.get(url)</span><br><span class="line">rejs = resq.json()</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">    df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]),ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過迴圈讀取10*100篇文章，若需讀取更多資料，可以將range(10)中的數值提升</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    last = str(int(df.tail(<span class="number">1</span>).ID)) <span class="comment"># 找出爬出資料的最後一筆ID</span></span><br><span class="line">    url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100&amp;before='</span> + last</span><br><span class="line">    resq = requests.get(url)</span><br><span class="line">    rejs = resq.json()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">        df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將資料存到桌面</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/Dcard.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> Dcard </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
