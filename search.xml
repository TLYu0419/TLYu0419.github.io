<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>網路爬蟲_Facebook粉絲團貼文與留言</title>
      <link href="/2019/05/01/Crawl-Facebook/"/>
      <url>/2019/05/01/Crawl-Facebook/</url>
      <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>上個月參加某公司的面試時，面試的長官詢問「有沒有辦法將臉書上的貼文時間、內容、按讚數、留言數與分享數、甚至是粉絲的留言資訊都抓下來？」</p><p>當時我還不太熟悉爬蟲，只能簡單回應說透過python與Selenium應該是可以，但卻沒辦法說出更詳細的方法愈步驟。後來經過一番研究，總算知道怎麼把這些資料爬來下來！</p><p>廢話不多說，我們就開始吧！</p><a id="more"></a><h1>輸入與輸出結果</h1><p>這是我們要爬的臉書頁面，從畫面上我們可以找到許多住要的資訊，如貼文的人名、ID、貼文發佈時間、貼文內容、多少個心情數量（按讚、生氣或哈哈）、以及留言數與分享數等等資訊。如果貼文底下有留言，我們也希望一併把這些留言都抓下來。</p><p><img src="/2019/05/01/Crawl-Facebook/01.jpg" alt="01.jpg"><br>而這是我們抓下來的結果，結果分成兩張表格：</p><p>第一張表格紀錄貼文資訊與互動摘要</p><ul><li>貼文資訊<ul><li>Name：貼文者名稱</li><li>ID：Facebook的客戶編號</li><li>Time：貼文發佈時間</li><li>Content：貼文內容</li></ul></li><li>互動摘要<ul><li>Like：按讚數</li><li>ANGER：生氣數</li><li>HAHA：哈哈數</li><li>commentcount：留言數</li><li>share：分享數</li></ul></li></ul><p>第二張表格則是記錄粉絲留言的資訊。</p><ul><li>留言資訊<ul><li>CommentID：留言人ID</li><li>CommentName：留言人姓名</li><li>CommentTime：留言時間</li><li>CommentContent：留言內容</li><li>Link：貼文連結</li></ul></li></ul><p><img src="/2019/05/01/Crawl-Facebook/02.jpg" alt="02.jpg"></p><h1>過程中曾遇過的坑</h1><p>在這裡先分享一些我在過程中遇到的坑與解決方式</p><ul><li>瀑布式網頁：網頁不會一次把所有貼文都加載給你，當我們把頁面滾動到最下端時才會加載新貼文。<ul><li>解決方案：透過java指令讓視窗滾動到底部</li></ul></li><li>系統彈窗：當我們把頁面往下滾動後，會彈出一個請我們登入或註冊的視窗，阻礙我們爬資料的流程。<ul><li>解決方案：偵測「Not Now」的element位置，並透過程式點擊這個element<br><img src="/2019/05/01/Crawl-Facebook/03.jpg" alt="03.jpg"></li></ul></li><li>心情互動：心情互動分成「讚」、「生氣」與「哈哈」等心情，這邊的坑在於當心情數量大於1才會顯示，若我們單純的設定要抓生氣的心情數量，而該篇文章沒有這個心情，就會顯示錯誤。<ul><li>解決方式：抓資料要運用try-except的方式嘗試抓該項資料，若無法抓這個資料則帶入0</li></ul></li><li>檢視留言：留言不會自動載入我們需要點擊「Comments」後才會顯示留言。<ul><li>解決方式：偵測「Comments」的element位置，並透過程式點擊該element<br><img src="/2019/05/01/Crawl-Facebook/04.jpg" alt="04.jpg"></li></ul></li><li>看更多留言：即使點擊「Comments」後，也只會顯示部分留言，需要反覆點擊「More」後才能不斷加載資料，但問題在於我們不知道到底要點幾次。<ul><li>解決方式：透過while迴圈，偵測頁面上是否還有「More comments」的選項能點選，停止的條件沒有「More comments」後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/05.jpg" alt="05.jpg"></li></ul></li><li>看更多內容：留言中若內容太長，系統只會顯示部分留言，需要點擊「See more」的選項後才會顯示完整訊息。<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/06.jpg" alt="06.jpg"></li></ul></li><li>看更多回覆：除了回應給貼文的留言之外，還有另一種留言是在回應別人的留言。我們也需要將這些留言抓下來<ul><li>解決方式：同上，透過while迴圈偵測，直到沒有這類選項後才停止迴圈。<br><img src="/2019/05/01/Crawl-Facebook/07.jpg" alt="07.jpg"></li></ul></li><li>相同element名稱：透過Chrome的檢查功能，我們可以看到我們想要的資訊放在span的timestampContent位置。但是我們如果只輸入這個條件並沒有法辦找出正確的資訊。因為在這個頁面中相同條件的element有許多筆…<ul><li>解決方式：抓資料應用逐層搜索的方式擷取資料，在一開始就設定清楚要抓哪個大區塊中的這個element。<br><img src="/2019/05/01/Crawl-Facebook/08.jpg" alt="08.jpg"><br><img src="/2019/05/01/Crawl-Facebook/09.jpg" alt="09.jpg"></li></ul></li></ul><h1>程式代碼</h1><p>完整的程式代碼會在文末附上，在這裡大家可以先把焦點放在程式碼的理解<br>載入使用套件</p><h2 id="載入套件"><a class="header-anchor" href="#載入套件">¶</a>載入套件</h2><p><img src="/2019/05/01/Crawl-Facebook/10.jpg" alt="10.jpg"></p><h2 id="搜尋貼文連結"><a class="header-anchor" href="#搜尋貼文連結">¶</a>搜尋貼文連結</h2><p>在這裡我們先定義一個函數，希望把網頁中各篇貼文的連結都找出來!<br>ulr放我們要爬的Facebook網址，n是稍後要送出幾次滾動網頁到底部的命令，藉以加載更多資料。<br><img src="/2019/05/01/Crawl-Facebook/11.jpg" alt="11.jpg"></p><h2 id="展開所有留言"><a class="header-anchor" href="#展開所有留言">¶</a>展開所有留言</h2><p>定義一個展開所有留言的函數，透過while迴圈反覆搜尋與點擊「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕。<br>在過程中會出現請我們登入或註冊的彈跳視窗，但我們不確定到底什麼時候會跳出，因此需要在過程中反覆偵測是有出現這個彈窗，若有就點擊「Not Now」<br><img src="/2019/05/01/Crawl-Facebook/12.jpg" alt="12.jpg"></p><h2 id="擷取貼文資訊與互動摘要"><a class="header-anchor" href="#擷取貼文資訊與互動摘要">¶</a>擷取貼文資訊與互動摘要</h2><p>透過逐層搜索的方式，逐步定位我們要找的資訊<br>在這個環節需要反覆透過Chrome的功能比對資料，需要花一些心力進行比對<br>另外在這部分也使用了大量個try-except，原因是許多資料是有內容才會出現。例如並非每天貼人都會收到「哈哈」、「生氣」的心情。<br><img src="/2019/05/01/Crawl-Facebook/13.jpg" alt="13.jpg"></p><h2 id="擷取粉絲留言資訊"><a class="header-anchor" href="#擷取粉絲留言資訊">¶</a>擷取粉絲留言資訊</h2><p>這邊要留意雖然都是粉絲留言，但實際上分成「回應貼文的留言」與「回應留言的留言」。<br>函數中的第一個迴圈是用來抓「回應貼文的留言」，第二個則是抓「回應留言的留言」。讀者可以自行比較一下兩個迴圈中不同的地方。<br><img src="/2019/05/01/Crawl-Facebook/14.jpg" alt="14.jpg"></p><h1>實作</h1><p>今天要爬的是<a href="https://zh-tw.facebook.com/taiwanmobile/" target="_blank" rel="noopener">台灣大哥大</a>的粉絲團頁面。暫時先設定加載20次資料，若想抓更多/更少資料的話可以自行調整n的數值。<br>注意這裡會透過Selenium開啟一個Chrome瀏覽器，若沒有下載的人可以參考<a href="https://medium.com/@NorthBei/%E5%9C%A8windows%E4%B8%8A%E5%AE%89%E8%A3%9Dpython-selenium-%E7%B0%A1%E6%98%93%E6%95%99%E5%AD%B8-eade1cd2d12d" target="_blank" rel="noopener">在Windows上安裝Python &amp; Selenium + 簡易教學</a>。<br><img src="/2019/05/01/Crawl-Facebook/15.jpg" alt="15.jpg"><br>接著先創造兩個DataFrame，一個放文章內容，另一個放留言內容。<br>我讓程式自動輸出目前的處理的網址，若有無法抓出的頁面也會送出一個訊息提醒。方便我們後續追蹤哪裡出現錯誤。<br><img src="/2019/05/01/Crawl-Facebook/16.jpg" alt="16.jpg"><br>跑完之後我們就可以看到抓下來的結果囉！<br><img src="/2019/05/01/Crawl-Facebook/17.jpg" alt="17.jpg"><br>將資料保存到桌面，打開檔案的結果在文章的開頭，這裡就不再重複放囉!<br><img src="/2019/05/01/Crawl-Facebook/18.jpg" alt="18.jpg"></p><h1>完整程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re, time, requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FindLinks</span><span class="params">(url, n)</span>:</span></span><br><span class="line">    Links = []</span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 這裡會跳出要我們登入的大畫面，找到「稍後再說」的按鈕並點擊</span></span><br><span class="line">    driver.find_element_by_xpath(<span class="string">'//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">    soup = BeautifulSoup(driver.page_source)</span><br><span class="line">    posts = soup.findAll(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'clearfix y_c3pyo2ta3'</span>&#125;)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> posts:</span><br><span class="line">        Links.append(<span class="string">'https://www.facebook.com'</span> + i.find(<span class="string">'a'</span>,&#123;<span class="string">'class'</span>:<span class="string">'_5pcq'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'?'</span>,<span class="number">2</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> Links</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand</span><span class="params">(url)</span>:</span></span><br><span class="line">    driver.get(url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//a[@lang="en_US"]'</span>).click()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Now is in EN_US"</span>)</span><br><span class="line">    driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">    <span class="comment"># 點擊「comments」，藉以展開留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        driver.find_element_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//a[@data-testid="UFI2CommentsCount/root"]'</span>).click()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        driver.find_element_by_id(<span class="string">'expanding_cta_close_button'</span>).click() </span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'There is no comment!'</span>)</span><br><span class="line">    k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> k != <span class="number">0</span>:</span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> driver.find_elements_by_xpath(<span class="string">'//div[@class="_5pcr userContentWrapper"]//div[@data-testid="UFI2CommentsList/root_depth_0"]//a[@role="button"]'</span>): </span><br><span class="line">            <span class="comment"># 反覆偵測是否有「看更多留言」、「看更多回覆」與「看完整貼文內容」等按鈕，若有擇點擊</span></span><br><span class="line">            <span class="keyword">if</span> bool(re.search(<span class="string">'comment|More|Repl'</span>,i.text)) == <span class="keyword">True</span> :</span><br><span class="line">                driver.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight);'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    driver.find_element_by_xpath(<span class="string">'//div[@style="display: block;"]//a[@id="expanding_cta_close_button"]'</span>).click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'No pupup!'</span>)</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    i.click()</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    print(<span class="string">'Nothing'</span>)</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文章內容與互動摘要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PostContent</span><span class="params">(soup)</span>:</span></span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># po文人資訊區塊</span></span><br><span class="line">    PosterInfo = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'l_c3pyo2v0u i_c3pynyi2f clearfix'</span>&#125;)</span><br><span class="line">    <span class="comment"># 互動摘要區(讚、留言與分享)</span></span><br><span class="line">    feedback = soup.find(<span class="string">'form'</span>, &#123;<span class="string">'class'</span>:<span class="string">'commentable_item collapsed_comments'</span>&#125;)</span><br><span class="line">    <span class="comment"># 名稱</span></span><br><span class="line">    Name = PosterInfo.find(<span class="string">'img'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="comment"># ID</span></span><br><span class="line">    ID = PosterInfo.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pb8 o_c3pynyi2g _8o _8s lfloat _ohe'</span>&#125;).attrs[<span class="string">'href'</span>].split(<span class="string">'/?'</span>,<span class="number">2</span>)[<span class="number">0</span>].split(<span class="string">'/'</span>,<span class="number">-1</span>)[<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># 網址</span></span><br><span class="line">    Link = driver.current_url</span><br><span class="line">    <span class="comment"># 發文時間</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'abbr'</span>).attrs[<span class="string">'title'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Time = PosterInfo.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_1atc fsm fwn fcg'</span>&#125;).text</span><br><span class="line">    <span class="comment"># 文章內容</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Content = userContent.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pbx userContent _3576'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Content = <span class="string">""</span></span><br><span class="line">    <span class="comment"># Like</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        Like = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_LIKE'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        Like = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># Angry</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ANGER = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_ANGER'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        ANGER = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># HAHA</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        HAHA = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2TopReactions/tooltip_HAHA'</span>&#125;).find(<span class="string">'a'</span>).attrs[<span class="string">'aria-label'</span>]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        HAHA = <span class="string">'0'</span></span><br><span class="line">    <span class="comment"># 留言</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        commentcount = feedback.find(<span class="string">'a'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2CommentsCount/root'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        commentcount = <span class="string">'0'</span> </span><br><span class="line">    <span class="comment"># 分享</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        share = feedback.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_355t _4vn2'</span>&#125;).text</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        share = <span class="string">'0'</span> </span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        data = [&#123;<span class="string">'Name'</span>:Name,</span><br><span class="line">                 <span class="string">'ID'</span>:ID,</span><br><span class="line">                 <span class="string">'Link'</span>:Link,</span><br><span class="line">                 <span class="string">'Time'</span>:Time,</span><br><span class="line">                 <span class="string">'Content'</span>:Content,</span><br><span class="line">                 <span class="string">'Like'</span>:Like,</span><br><span class="line">                 <span class="string">'ANGER'</span>:ANGER,</span><br><span class="line">                 <span class="string">"HAHA"</span>:HAHA,</span><br><span class="line">                 <span class="string">'commentcount'</span>:commentcount,</span><br><span class="line">                 <span class="string">'share'</span>:share&#125;],</span><br><span class="line">        columns = [<span class="string">'Name'</span>, <span class="string">'ID'</span>, <span class="string">'Time'</span>, <span class="string">'Content'</span>, <span class="string">'Like'</span>, <span class="string">'ANGER'</span>, <span class="string">'HAHA'</span>, <span class="string">'commentcount'</span>, <span class="string">'share'</span>, <span class="string">'Link'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留言</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CrawlComment</span><span class="params">(soup)</span>:</span></span><br><span class="line">    Comments = pd.DataFrame()</span><br><span class="line">    <span class="comment"># po文區塊</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 用戶留言區</span></span><br><span class="line">    userContent = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'_5pcr userContentWrapper'</span>&#125;)</span><br><span class="line">    <span class="comment"># 回應貼文的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_0'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg0'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 回應留言的留言</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> userContent.findAll(<span class="string">'div'</span>, &#123;<span class="string">'data-testid'</span>:<span class="string">'UFI2Comment/root_depth_1'</span>&#125;):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            CommentContent = i.find(<span class="string">'span'</span>, &#123;<span class="string">'dir'</span>:<span class="string">'ltr'</span>&#125;).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            CommentContent = <span class="string">'Sticker'</span></span><br><span class="line">        Comment = pd.DataFrame(data = [&#123;<span class="string">'CommentID'</span>:i.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">' _3mf5 _3mg1'</span>&#125;).attrs[<span class="string">'data-hovercard'</span>].split(<span class="string">'id='</span>,<span class="number">2</span>)[<span class="number">1</span>],</span><br><span class="line">                                 <span class="string">'CommentName'</span>:i.find(<span class="string">'img'</span>).attrs[<span class="string">'alt'</span>],</span><br><span class="line">                                 <span class="string">'CommentTime'</span>:i.find(<span class="string">'abbr'</span>,&#123;<span class="string">'class'</span>:<span class="string">'livetimestamp'</span>&#125;).attrs[<span class="string">'data-tooltip-content'</span>],</span><br><span class="line">                                 <span class="string">'CommentContent'</span>:CommentContent,</span><br><span class="line">                                 <span class="string">'Link'</span>:driver.current_url&#125;],</span><br><span class="line">                        columns = [<span class="string">'CommentID'</span>, <span class="string">'CommentName'</span>, <span class="string">'CommentTime'</span>, <span class="string">'CommentContent'</span>, <span class="string">'Link'</span>])</span><br><span class="line">        Comments = pd.concat([Comments, Comment], ignore_index=<span class="keyword">True</span>)        </span><br><span class="line">    <span class="keyword">return</span> Comments</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">Links = FindLinks(url = <span class="string">'https://facebook.com/taiwanmobile/'</span>,</span><br><span class="line">                  n = <span class="number">20</span>)</span><br><span class="line">Links</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抓下來所有留言</span></span><br><span class="line">PostsInformation = pd.DataFrame()</span><br><span class="line">PostsComments = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Links:</span><br><span class="line">    print(<span class="string">'Dealing with: '</span> + i)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        expand(i)</span><br><span class="line">        soup = BeautifulSoup(driver.page_source)</span><br><span class="line">        PostsInformation = pd.concat([PostsInformation, PostContent(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">        PostsComments = pd.concat([PostsComments, CrawlComment(soup)],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'Load Failed: '</span> + i)</span><br><span class="line"></span><br><span class="line">PostsInformation</span><br><span class="line">PostsComments</span><br><span class="line"></span><br><span class="line">PostsInformation.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsInformation.xlsx'</span>)</span><br><span class="line">PostsComments.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/PostsComments.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> python </tag>
            
            <tag> Selenium </tag>
            
            <tag> BeautifulSoup </tag>
            
            <tag> Facebook粉絲團 </tag>
            
            <tag> 免登入 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲_104人力銀行工作清單</title>
      <link href="/2019/04/18/Crawl-JobList104/"/>
      <url>/2019/04/18/Crawl-JobList104/</url>
      <content type="html"><![CDATA[<p><em>本文僅限於學習使用，請勿用於商業目的</em></p><p>找工作是一件很辛苦的事情，當我們在<a href="https://www.104.com.tw/jobs/main/" target="_blank" rel="noopener">104人力銀行</a>的網站上輸入關鍵字查詢職缺時，經常會直接跳出上百、千份的職缺。雖然有很多職缺對於求職者是好事，但這也造成了求職者的大困擾！</p><p>因此我希望能透過Python的爬蟲，一次把我想要查詢的結果(工作內容、地點、薪資、要求技能、工作地點…等等資訊)抓下來，在我自己的電腦上按照我的需求進行篩選，讓我能更有效率的挑選工作並投遞履歷。</p><p>另一方面，如果想了解各個產業會需要哪些工具、技能，也可以從這份資料中進一步的分析。那麼我們就開始吧！</p><a id="more"></a><p>我們直接先看輸入與最終的輸出結果吧!<br>在左邊的圖是104的查詢系統，我在這邊搜尋的關鍵字幾項條件分別是「資料科學」、「台北市」、「最近一個月有更新」等項目，經過搜尋之後，系統幫我查詢到25頁，共730筆職缺。並且也有跟我說各職缺的公司名稱、學歷要求、工作經歷等初步資訊。</p><p>但只有這些資訊是不夠的，當我們看到有興趣的職缺時，還需要進一步的點開超連結，檢視詳細的職務說明(右邊的圖)。在這個分頁中我們就能夠看到詳細的工作內容、條件要求、公司福利與聯繫方式等資訊囉！我最終的目的是希望透過程式自動幫我們這些資訊都整理成一份excel表格！<br><img src="/2019/04/18/Crawl-JobList104/104HomePage.JPG" alt="HomePage"><br>而我們最終的目標就是將所有職缺以及各職缺的內容都整理成這份excel表格，讓我能按照自己的方式篩選資料，提升找工作的效率！<br><img src="/2019/04/18/Crawl-JobList104/JobList.JPG" alt="JobList"></p><h1>載入使用套件</h1><p><img src="/2019/04/18/Crawl-JobList104/01.JPG" alt="01.JPG"></p><h1>設定查詢條件</h1><p>這些查詢條件可以在<a href="https://www.104.com.tw/jobs/search/" target="_blank" rel="noopener">104的搜尋網頁</a>上搜索，在這裡不多做說明<br><img src="/2019/04/18/Crawl-JobList104/02.JPG" alt="02.JPG"></p><h1>展開所有工作清單，後續將依序開始爬蟲</h1><p>這裡會透過Selenium打開一個瀏覽器並開始跑程式~<br><img src="/2019/04/18/Crawl-JobList104/03.JPG" alt="03.JPG"></p><h1>解析爬蟲資料並整理成DataFrame</h1><p>在正式開始爬蟲之前，我預先定義一個函數，專於用來處理「職務類別」這個複選題，稍後將用這個函數將其串接在一起<br><img src="/2019/04/18/Crawl-JobList104/04.JPG" alt="04.JPG"></p><p>開始逐筆爬資料囉!<br><img src="/2019/04/18/Crawl-JobList104/05.JPG" alt="05.JPG"></p><h1>結果</h1><p>這裡爬得很快，大約10分鐘就抓完這700筆資料囉!<br><img src="/2019/04/18/Crawl-JobList104/06.JPG" alt="06.JPG"></p><p><img src="/2019/04/18/Crawl-JobList104/07.JPG" alt="07.JPG"></p><p>因為內容並沒有太難，因此我就不做太多說明了，不過有問題的人也歡迎在底下的留言提出！</p><h1>完整程式代碼</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import re, time, requests</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span><br><span class="line">headers = &#123;&apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&apos;&#125;</span><br><span class="line"></span><br><span class="line"># 查詢的關鍵字</span><br><span class="line">my_params = &#123;&apos;ro&apos;:&apos;1&apos;, # 限定全職的工作，如果不限定則輸入0</span><br><span class="line">             &apos;keyword&apos;:&apos;資料科學&apos;, # 想要查詢的關鍵字</span><br><span class="line">             &apos;area&apos;:&apos;6001001000&apos;, # 限定在台北的工作</span><br><span class="line">             &apos;isnew&apos;:&apos;30&apos;, # 只要最近一個月有更新的過的職缺</span><br><span class="line">             &apos;mode&apos;:&apos;l&apos;&#125; # 清單的瀏覽模式</span><br><span class="line"></span><br><span class="line">url = requests.get(&apos;https://www.104.com.tw/jobs/search/?&apos; , my_params, headers = headers).url</span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">driver.get(url)</span><br><span class="line"></span><br><span class="line"># 網頁的設計方式是滑動到下方時，會自動加載新資料，在這裡透過程式送出Java語法幫我們執行「滑到下方」的動作</span><br><span class="line">for i in range(20): </span><br><span class="line">    driver.execute_script(&apos;window.scrollTo(0, document.body.scrollHeight);&apos;)</span><br><span class="line">    time.sleep(0.6)</span><br><span class="line">    </span><br><span class="line"># 自動加載只會加載15次，超過之後必須要點選「手動載入」的按鈕才會繼續載入新資料（可能是防止爬蟲）</span><br><span class="line">k = 1</span><br><span class="line">while k != 0:</span><br><span class="line">    try:</span><br><span class="line">        # 手動載入新資料之後會出現新的more page，舊的就無法再使用，所以要使用最後一個物件</span><br><span class="line">        driver.find_elements_by_class_name(&quot;js-more-page&quot;,)[-1].click() </span><br><span class="line">        # 如果真的找不到，也可以直接找中文!</span><br><span class="line">        # driver.find_element_by_xpath(&quot;//*[contains(text(),&apos;手動載入&apos;)]&quot;).click()</span><br><span class="line">        print(&apos;Click 手動載入，&apos; + &apos;載入第&apos; + str(15 + k) + &apos;頁&apos;)</span><br><span class="line">        k = k+1</span><br><span class="line">        time.sleep(1) # 時間設定太短的話，來不及載入新資料就會跳錯誤</span><br><span class="line">    except:</span><br><span class="line">        k = 0</span><br><span class="line">        print(&apos;No more Job&apos;)</span><br><span class="line"></span><br><span class="line"># 透過BeautifulSoup解析資料</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &apos;html.parser&apos;)</span><br><span class="line">List = soup.findAll(&apos;a&apos;,&#123;&apos;class&apos;:&apos;js-job-link&apos;&#125;)</span><br><span class="line">print(&apos;共有 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line"></span><br><span class="line">def bind(cate):</span><br><span class="line">    k = []</span><br><span class="line">    for i in cate:</span><br><span class="line">        if len(i.text) &gt; 0:</span><br><span class="line">            k.append(i.text)</span><br><span class="line">    return str(k)</span><br><span class="line"></span><br><span class="line">JobList = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">while i &lt; len(List):</span><br><span class="line">    # print(&apos;正在處理第&apos; + str(i) + &apos;筆，共 &apos; + str(len(List)) + &apos; 筆資料&apos;)</span><br><span class="line">    content = List[i]</span><br><span class="line">    # 這裡用Try的原因是，有時候爬太快會遭到系統阻擋導致失敗。因此透過這個方式，當我們遇到錯誤時，會重新再爬一次資料！</span><br><span class="line">    try:</span><br><span class="line">        resp = requests.get(&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;))</span><br><span class="line">        soup2 = BeautifulSoup(resp.text,&apos;html.parser&apos;)</span><br><span class="line">        df = pd.DataFrame(</span><br><span class="line">            data = [&#123;</span><br><span class="line">                &apos;公司名稱&apos;:soup2.find(&apos;a&apos;, &#123;&apos;class&apos;:&apos;cn&apos;&#125;).text,</span><br><span class="line">                &apos;工作職稱&apos;:content.attrs[&apos;title&apos;],</span><br><span class="line">                &apos;工作內容&apos;:soup2.find(&apos;p&apos;).text,</span><br><span class="line">                &apos;職務類別&apos;:bind(soup2.findAll(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;cate&apos;&#125;)[0].findAll(&apos;span&apos;)),</span><br><span class="line">                &apos;工作待遇&apos;:soup2.find(&apos;dd&apos;, &#123;&apos;class&apos;:&apos;salary&apos;&#125;).text.split(&apos;\n\n&apos;,2)[0].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;工作性質&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[2].text,</span><br><span class="line">                &apos;上班地點&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[3].text.split(&apos;\n\n&apos;,2)[0].split(&apos;\n&apos;,2)[1].replace(&apos; &apos;,&apos;&apos;),</span><br><span class="line">                &apos;管理責任&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[4].text,</span><br><span class="line">                &apos;出差外派&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[5].text,</span><br><span class="line">                &apos;上班時段&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[6].text,</span><br><span class="line">                &apos;休假制度&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[7].text,</span><br><span class="line">                &apos;可上班日&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[8].text,</span><br><span class="line">                &apos;需求人數&apos;:soup2.select(&apos;div &gt; dl &gt; dd&apos;)[9].text,</span><br><span class="line">                &apos;接受身份&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[10].text,</span><br><span class="line">                &apos;學歷要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[12].text,</span><br><span class="line">                &apos;工作經歷&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[11].text,</span><br><span class="line">                &apos;語文條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[14].text,</span><br><span class="line">                &apos;擅長工具&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[15].text,</span><br><span class="line">                &apos;工作技能&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[16].text,</span><br><span class="line">                &apos;其他條件&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[17].text,</span><br><span class="line">                &apos;公司福利&apos;:soup2.select(&apos;div.content &gt; p&apos;)[1].text,</span><br><span class="line">                &apos;科系要求&apos;:soup2.select(&apos;div.content &gt; dl &gt; dd&apos;)[13].text,</span><br><span class="line">                &apos;聯絡方式&apos;:soup2.select(&apos;div.content&apos;)[3].text.replace(&apos;\n&apos;,&apos;&apos;),</span><br><span class="line">                &apos;連結路徑&apos;:&apos;https://&apos; + content.attrs[&apos;href&apos;].strip(&apos;//&apos;)&#125;],</span><br><span class="line">            columns = [&apos;公司名稱&apos;,&apos;工作職稱&apos;,&apos;工作內容&apos;,&apos;職務類別&apos;,&apos;工作待遇&apos;,&apos;工作性質&apos;,&apos;上班地點&apos;,&apos;管理責任&apos;,&apos;出差外派&apos;,</span><br><span class="line">                       &apos;上班時段&apos;,&apos;休假制度&apos;,&apos;可上班日&apos;,&apos;需求人數&apos;,&apos;接受身份&apos;,&apos;學歷要求&apos;,&apos;工作經歷&apos;,&apos;語文條件&apos;,&apos;擅長工具&apos;,</span><br><span class="line">                       &apos;工作技能&apos;,&apos;其他條件&apos;,&apos;公司福利&apos;,&apos;科系要求&apos;,&apos;聯絡方式&apos;,&apos;連結路徑&apos;])</span><br><span class="line">        JobList = JobList.append(df, ignore_index=True)</span><br><span class="line">        i += 1</span><br><span class="line">        print(&quot;Success and Crawl Next 目前正在爬第&quot; + str(i) + &quot;個職缺資訊&quot;)</span><br><span class="line">        time.sleep(0.5) # 執行完休息0.5秒，避免造成對方主機負擔</span><br><span class="line">    except:</span><br><span class="line">        print(&quot;Fail and Try Again!&quot;)</span><br><span class="line"></span><br><span class="line">JobList</span><br><span class="line"></span><br><span class="line">JobList.to_excel(&apos;C:/Users/TLYu0419/Desktop/JobList2.xlsx&apos;, encoding=&apos;cp950&apos;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> python </tag>
            
            <tag> 104人力銀行 </tag>
            
            <tag> Selenium </tag>
            
            <tag> requests </tag>
            
            <tag> BeautifulSoup </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲 - 爬取SongGuo購物商品資訊</title>
      <link href="/2019/04/07/Crawl-SongGuo/"/>
      <url>/2019/04/07/Crawl-SongGuo/</url>
      <content type="html"><![CDATA[<p><em>本文章僅限於學習目的使用，請勿用於商業目的</em></p><p><a href="https://www.pcone.com.tw/" target="_blank" rel="noopener">SongGuo</a>購物是台灣新創的電商，截至2019年1月為止，平台上有超過3,600間廠商進駐，商品數量也超過15萬件。僅用了2年的時間就開始<br>我很喜歡創辦人分享以下兩篇創業歷程的文章。有興趣的人從以下文章了解更多松果購物的故事。</p><ul><li><a href="http://tesa.today/article/1629" target="_blank" rel="noopener">松果的創業故事-第1年</a></li><li><a href="https://tesa.today/article/1909" target="_blank" rel="noopener">松果的創業故事-第2年</a></li></ul><p>在這篇文章中，我們要透過python把SongGuo上的商品資料爬下來！</p><a id="more"></a><p><em>再強調一次，請勿用於商業目的。</em><br>這邊文章的架構如下</p><ul><li>找出「男生服飾」下所有的商品連結<br>選擇「男生服飾」的原因是這類型的商品數量較少，我們在抓資料比較不會造成系統負擔。若想抓其他類型商品的人可以自行調整類型的代號</li><li>從商品連結抓出店家/商品的資訊<br>包括店家名稱 / 店家評價 / 商品名稱 / 商品銷售量…等等。具體項目會在後面提及。</li><li>轉換成抓取變動目標的語法<br>將以上抓資料的過程撰寫成簡單的函數，讓我們可以輸入類型的代號，就抓出該類型下的所有商品資訊</li></ul><p>那我們就開始吧！</p><h1>找出「男生服飾」下所有的商品連結</h1><p>我們要練習的是SongGuo上男生服飾類型，這是網頁畫面。若想要練習其他類型的商品，可以自行修改網址中最後的3碼數字。<br><img src="/2019/04/07/Crawl-SongGuo/01.JPG" alt="01"></p><p>首先我們需要載入使用到的相關套件<br><img src="/2019/04/07/Crawl-SongGuo/02.JPG" alt="02"></p><p>設定待爬網頁的網址，與使用者資訊<br><img src="/2019/04/07/Crawl-SongGuo/03.JPG" alt="03"></p><blockquote><p>需要注意的是，若沒有輸入使用者資訊，很容易被系統偵測為爬蟲系統，進而阻止後續的爬蟲作業。因此這裡需要加入這些資訊，藉以模擬真實的瀏覽網頁情境。</p></blockquote><p>接著我們可以透過這段語法找出網頁上所有商品的連結<br><img src="/2019/04/07/Crawl-SongGuo/04.JPG" alt="04"></p><blockquote><p>需要留意的是，透過輸入’a.product-list-item’的方式找出商品連結只限於用載SongGuo的網站。如果想要在其他網站爬資料，則需要檢視個別網站的網站結構。<br>具體的方法是</p><ol><li>透過Chrome網頁開啟目標網站，並在網頁中點選「滑鼠右鍵」的「檢查」功能。</li><li>Ctrl + Shift + I<br>畫面如下<br><img src="/2019/04/07/Crawl-SongGuo/05.JPG" alt="05"></li></ol></blockquote><p>從以上的畫面我們就可以看到，商品的連結就藏在各個Element的href屬性中囉！</p><h1>從商品連結抓出店家/商品的資訊</h1><p>我從中挑選第一個商品連結(/product/info/190117048588)作為範例，商品項目是<a href="https://www.pcone.com.tw/product/info/190117048588" target="_blank" rel="noopener">【瑞典】旅行折疊電熱水壺</a>，網頁畫面如下<br><img src="/2019/04/07/Crawl-SongGuo/06.JPG" alt="06"><br>這個網頁中有相當豐富的資訊，包括店家名稱、店家評價、商品名稱、商品價格…等等<br>我將從網頁中提取出以下項目的資訊</p><ul><li>店家名稱</li><li>店家商品數量</li><li>店家評價</li><li>店家出貨天數</li><li>店家回覆率</li><li>產品名稱</li><li>特價</li><li>原價</li><li>折數</li><li>商品評分</li><li>評價人數</li><li>收藏人數</li><li>提問人數</li><li>商品分類</li><li>商品標籤</li><li>連結</li></ul><p>找出資訊藏在哪個Element與屬性的方法同樣是透過Chrome中的「檢查」功能，以商品名稱作為範例的畫面如下：<br><img src="/2019/04/07/Crawl-SongGuo/07.JPG" alt="07"><br>這邊是找出以上項目的程式代碼<br><img src="/2019/04/07/Crawl-SongGuo/08.JPG" alt="08"><br><img src="/2019/04/07/Crawl-SongGuo/09.JPG" alt="09"></p><blockquote><p>其實客戶的留言也是相當重要的資訊，但礙於篇幅這裡就不多做說明，找出節點與屬性的方法是相同的，有興趣的人可以自行練習。</p></blockquote><p>透過以上的方式，我們確認用程式把商品的資料抓出來是沒問題的！接下來我們只需要把抓取「固定」目標程式語法轉換成「變動」的代號，我們就能自動抓出所有的商品資訊囉！</p><h1>轉換成抓取變動目標的語法</h1><p>簡單整理一下，我們在第一段的輸入是某個商品分類的編號，回傳的是該分類下的所有商品連結。<br>而第二段則是輸入商品的連結，自動幫我們抓出商品的各項資訊。<br>因此我們要透過迴圈，逐一地抓出(第二段語法)所有商品(第一段的產出)資訊</p><p>定義ProdList函數，輸入商品分類的編號，回傳該分類下的商品連結清單<br><img src="/2019/04/07/Crawl-SongGuo/10.JPG" alt="10"></p><p>定義Crawl_SongGuo函數，輸入商品的連結，回傳商品的資訊<br><img src="/2019/04/07/Crawl-SongGuo/11.JPG" alt="11"></p><p>結果以上兩個函數，輸入商品分類的編號，抓取商品的各項屬性，並放在df中<br><img src="/2019/04/07/Crawl-SongGuo/12.JPG" alt="12"></p><blockquote><p>這裡會輸出的商品編號的原因是，我在Crawl_SongGuo函數中刻意加入print()指令，目的是方便我們追蹤程式是否正常執行。</p></blockquote><p>系統執行的效率非常快，不到1分鐘就執行完畢了，我們來呼叫df看一下結果吧！<br><img src="/2019/04/07/Crawl-SongGuo/13.JPG" alt="13"><br>因為畫面的限制，只顯示出第一筆資料，不過我們確定資料已經以DataFrame的格式抓下來囉！<br>接著我們嘗試把資料存成excel來檢視吧！</p><h1>保存資料</h1><p><img src="/2019/04/07/Crawl-SongGuo/14.JPG" alt="14"><br><img src="/2019/04/07/Crawl-SongGuo/15.JPG" alt="15"></p><p>以上次這個爬蟲的說明，有問題可以在以下留言區提問~</p><h1>完整程式待碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入相關套件</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸入爬蟲網址與使用者資訊</span></span><br><span class="line">url = <span class="string">'https://www.pcone.com.tw/product/'</span></span><br><span class="line"><span class="comment"># 男生服飾</span></span><br><span class="line">info = <span class="string">'327'</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入使用者資訊(如使用什麼瀏覽器、作業系統...等資訊)模擬真實瀏覽網頁的情況</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視是否成功抓到資料</span></span><br><span class="line">resp = requests.get(url + info, headers=headers)  </span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line">a = html.find(<span class="string">'a.product-list-item'</span>)</span><br><span class="line">a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挑選第一筆資料作為範例</span></span><br><span class="line">resp = requests.get(<span class="string">'https://www.pcone.com.tw/'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>], headers=headers)</span><br><span class="line">html = HTML(html=resp.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 店家名稱</span></span><br><span class="line">html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 店家商品數量</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家評價</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家出貨天數</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 店家回覆率</span></span><br><span class="line">html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>]</span><br><span class="line"><span class="comment"># 產品名稱</span></span><br><span class="line">html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 特價</span></span><br><span class="line">html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 原價</span></span><br><span class="line">html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 折數</span></span><br><span class="line">html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品評分</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 評價人數</span></span><br><span class="line">html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 收藏人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 提問人數</span></span><br><span class="line">html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text</span><br><span class="line"><span class="comment"># 商品分類</span></span><br><span class="line">html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品標籤</span></span><br><span class="line">html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text</span><br><span class="line"><span class="comment"># 商品連結</span></span><br><span class="line"><span class="string">'https://www.pcone.com.tw'</span> + a[<span class="number">0</span>].attrs[<span class="string">'href'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義ProdList函數，輸入商品分類編號，輸出該分類下所有商品連結</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ProdList</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(url + str(info), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    <span class="keyword">return</span>(html.find(<span class="string">'a.product-list-item'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義Crawl_SongGuo函數，輸入商品網址，輸出該商品的各項屬性</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl_SongGuo</span><span class="params">(info)</span>:</span></span><br><span class="line">    resp = requests.get(<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group(), headers=headers)</span><br><span class="line">    html = HTML(html=resp.text)</span><br><span class="line">    print(re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group())</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">            data=[&#123;</span><br><span class="line">                <span class="string">'店家名稱'</span>:html.find(<span class="string">'a.store-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'店家商品數量'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家評價'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家出貨天數'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">2</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'店家回覆率'</span>:html.find(<span class="string">'div.store-val'</span>,first = <span class="keyword">False</span>)[<span class="number">3</span>].attrs[<span class="string">'data-val'</span>],</span><br><span class="line">                <span class="string">'產品名稱'</span>:html.find(<span class="string">'h1.product-name'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'特價'</span>:html.find(<span class="string">'span.bind-lowest-price.discount'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'原價'</span>:html.find(<span class="string">'span.original'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'折數'</span>:html.find(<span class="string">'span.bind-discount-number.discount-number'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品評分'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'評價人數'</span>:html.find(<span class="string">'span.count &gt; span'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'收藏人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">0</span>].text,</span><br><span class="line">                <span class="string">'提問人數'</span>:html.find(<span class="string">'div.count'</span>,first = <span class="keyword">False</span>)[<span class="number">1</span>].text,</span><br><span class="line">                <span class="string">'商品分類'</span>:html.find(<span class="string">'div.breadcrumbs-set'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'商品標籤'</span>:html.find(<span class="string">'div.tags'</span>,first = <span class="keyword">True</span>).text,</span><br><span class="line">                <span class="string">'連結'</span>:<span class="string">'https://www.pcone.com.tw/product/info/'</span> + re.search(<span class="string">r'\d&#123;12&#125;'</span>,str(info)).group()&#125;],</span><br><span class="line">            columns = [<span class="string">'店家名稱'</span>, <span class="string">'店家商品數量'</span>, <span class="string">'店家評價'</span>, <span class="string">'店家出貨天數'</span>, <span class="string">'店家回覆率'</span>,  <span class="string">'產品名稱'</span>, <span class="string">'特價'</span>, <span class="string">'原價'</span>, <span class="string">'折數'</span>,</span><br><span class="line">                       <span class="string">'商品評分'</span>, <span class="string">'評價人數'</span>, <span class="string">'收藏人數'</span>,<span class="string">'提問人數'</span>, <span class="string">'商品分類'</span>, <span class="string">'商品標籤'</span>, <span class="string">'連結'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 組合以上兩個函數，輸入商品分類的編號，即自動爬出所有商品的屬性，並將資料存在df中</span></span><br><span class="line">prodlist = ProdList(<span class="number">327</span>)</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(prodlist)):</span><br><span class="line">    df = df.append(Crawl_SongGuo(prodlist[i]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視抓下來的資料</span></span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將df轉成excel並存在桌面上</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/SongGuo.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> python </tag>
            
            <tag> SongGuo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>爬蟲 - 爬取Dcard文章</title>
      <link href="/2019/04/06/Crawl-Dcard/"/>
      <url>/2019/04/06/Crawl-Dcard/</url>
      <content type="html"><![CDATA[<p><a href="(https://www.dcard.tw/f)">Dcard</a>是非常適合練習爬蟲的網站，<br>除了Dcard台灣熱門的社群網站之外，Dcard也提供了非常便利的API讓我們能從網站上爬下文章。<br>在這篇文章中，我將展示如何透過python爬下Dcard上的文章！</p><a id="more"></a><p>這邊文章的架構如下</p><ul><li>抓取一篇Dcard的文章<br>具體項目如下：編號 / 標題 / 引言 / 內容 / 發布時間 / 更新時間…等等</li><li>一次爬100篇Dcard文章<br>透過系統提供的api，一次抓取100篇熱門文章</li><li>爬超過100篇Dcard文章<br>因為API限制一次最多100篇，在這裡我們透過簡單的迴圈一次爬1000篇文章。</li></ul><p>在這裡我們先練習爬文章內容的方法，若想進一步爬文章底下留言的人，可以參考補充資料中的範例，以下我們就開始練習吧！</p><p>補充資料：<a href="https://medium.com/pyladies-taiwan/%E7%88%AC%E8%9F%B2-%E5%BE%9Edcard%E7%B6%B2%E7%AB%99%E7%9C%8B%E7%88%AC%E8%9F%B2%E5%85%A5%E9%96%80-iii-ded52759d922" target="_blank" rel="noopener">爬蟲-從dcard網站看爬蟲入門-iii</a></p><h1>抓取一篇Dcard的文章</h1><p>我們先隨機挑選一篇Dcard上的文章作為練習，我挑選到的是這篇文章<a href="https://www.dcard.tw/f/funny/p/231030181" target="_blank" rel="noopener">警察閃光get</a>。</p><p>文章在Chrome上的畫面如下<br><img src="/2019/04/06/Crawl-Dcard/01.JPG" alt="01"><br>從網址列中可以看到這篇文章的編號是231030181，因此我們稍後會透過這個編號來爬這篇文章</p><p>首先我們先載入需要使用到的套件<br><img src="/2019/04/06/Crawl-Dcard/02.JPG" alt="02"><br>將這篇文章的編號透過quest套件讀取，並檢視抓下來資料的結構</p><p><img src="/2019/04/06/Crawl-Dcard/03.JPG" alt="03"><br>透過比對網站顯示的內容與上面輸出的資料結構後，我們可以從中發現id即為文章的編號, title是標題, conten則是內容，其他欄位的說明如下表：</p><table><thead><tr><th style="text-align:left">欄位</th><th style="text-align:center">說明</th><th style="text-align:left">備註</th></tr></thead><tbody><tr><td style="text-align:left">ID</td><td style="text-align:center">編號</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">title</td><td style="text-align:center">標題</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">content</td><td style="text-align:center">內容</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">excerpt</td><td style="text-align:center">摘要</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">createdAt</td><td style="text-align:center">發布時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">updatedAt</td><td style="text-align:center">更新時間</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">commentCount</td><td style="text-align:center">留言數</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">forumName</td><td style="text-align:center">分類</td><td style="text-align:left">中文</td></tr><tr><td style="text-align:left">forumAlias</td><td style="text-align:center">分類</td><td style="text-align:left">英文</td></tr><tr><td style="text-align:left">gender</td><td style="text-align:center">性別</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">likeCount</td><td style="text-align:center">心情數量</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">reactions</td><td style="text-align:center">心情細項</td><td style="text-align:left">把以上心情細分為「愛心」、「哈哈」、「跪」、「森77」、「驚訝」等類型</td></tr><tr><td style="text-align:left">topics</td><td style="text-align:center">標籤</td><td style="text-align:left"></td></tr></tbody></table><blockquote><p>在上表中的心情數量是各種心情數量的加總，若想進一步分析各種心情，可以再從reactions欄位提取。</p></blockquote><p>我們來嘗試把資料轉換為DataFrame吧！</p><p><img src="/2019/04/06/Crawl-Dcard/04.JPG" alt="04"></p><p>確認可以透過程式把文章爬下來之後，我們就來寫個簡單的Crawl函數，期望只需要輸入文章的ID後，就回傳爬下來的文章內容！</p><p><img src="/2019/04/06/Crawl-Dcard/05.JPG" alt="05"></p><p>接著我們就透過Crawl來爬文章吧！<br><img src="/2019/04/06/Crawl-Dcard/06.JPG" alt="06"><br>Good!</p><p>確認函數能正常執行!</p><h1>一次爬100篇Dcard文章</h1><p>在這邊我使用dcard提供便利的API，讓我們可以直接快速爬取資料<br><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">dcard API</a><br>以下簡單說明這個網址</p><ul><li>popular參數：若設定為true，表示按照熱門程度排序，若設定為false，則按照發布時間排序</li><li>limit參數：限定在0-100的數值，表示要抓多少文章</li></ul><p><a href="https://www.dcard.tw/_api/posts?popular=true&amp;limit=100" target="_blank" rel="noopener">https://www.dcard.tw/_api/posts?popular=true&amp;limit=100</a></p><p><img src="/2019/04/06/Crawl-Dcard/07.JPG" alt="07"></p><h1>爬超過100篇Dcard文章</h1><p>由於API限制最多載入100篇文章，如果我們想要爬更多資料，可以透過before參數與迴圈進行!<br><br>before參數後面是接文章的ID，讓我們可以抓取某篇文章之前的資料<br><br>而透過迴圈，我們只需要把之前抓到最後一篇文章的ID放入before參數中，我們就可以抓到這篇文章的前100篇文章。<br><img src="/2019/04/06/Crawl-Dcard/08.JPG" alt="08"></p><h1>保存資料</h1><p>將資料轉換為excel保存到桌面<br><img src="/2019/04/06/Crawl-Dcard/09.JPG" alt="09"><br>用excel檢視抓下來的資料<br><img src="/2019/04/06/Crawl-Dcard/10.JPG" alt="10"></p><h1>完整的程式代碼</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 載入使用的套件</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_html <span class="keyword">import</span> HTML</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視資料結構</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line"><span class="comment"># 透過request套件抓下這個網址的資料</span></span><br><span class="line">requ = requests.get(url)</span><br><span class="line"><span class="comment"># 初步檢視抓到的資料結構</span></span><br><span class="line">requ.json()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將抓下來的資料轉為DataFrame</span></span><br><span class="line">ID = <span class="string">'231030181'</span></span><br><span class="line">url = url = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + ID</span><br><span class="line">requ = requests.get(url)</span><br><span class="line">rejs = requ.json()</span><br><span class="line">pd.DataFrame(</span><br><span class="line">    data=</span><br><span class="line">    [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">      <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">      <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">      <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">      <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">      <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">      <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">      <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">      <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">      <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">      <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">      <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">      <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">    columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 撰寫簡單的函數，透過輸入文章ID，就輸出文章的資料</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Crawl</span><span class="params">(ID)</span>:</span></span><br><span class="line">    link = <span class="string">'https://www.dcard.tw/_api/posts/'</span> + str(ID)</span><br><span class="line">    requ = requests.get(link)</span><br><span class="line">    rejs = requ.json()</span><br><span class="line">    <span class="keyword">return</span>(pd.DataFrame(</span><br><span class="line">        data=</span><br><span class="line">        [&#123;<span class="string">'ID'</span>:rejs[<span class="string">'id'</span>],</span><br><span class="line">          <span class="string">'title'</span>:rejs[<span class="string">'title'</span>],</span><br><span class="line">          <span class="string">'content'</span>:rejs[<span class="string">'content'</span>],</span><br><span class="line">          <span class="string">'excerpt'</span>:rejs[<span class="string">'excerpt'</span>],</span><br><span class="line">          <span class="string">'createdAt'</span>:rejs[<span class="string">'createdAt'</span>],</span><br><span class="line">          <span class="string">'updatedAt'</span>:rejs[<span class="string">'updatedAt'</span>],</span><br><span class="line">          <span class="string">'commentCount'</span>:rejs[<span class="string">'commentCount'</span>],</span><br><span class="line">          <span class="string">'forumName'</span>:rejs[<span class="string">'forumName'</span>],</span><br><span class="line">          <span class="string">'forumAlias'</span>:rejs[<span class="string">'forumAlias'</span>],</span><br><span class="line">          <span class="string">'gender'</span>:rejs[<span class="string">'gender'</span>],</span><br><span class="line">          <span class="string">'likeCount'</span>:rejs[<span class="string">'likeCount'</span>],</span><br><span class="line">          <span class="string">'reactions'</span>:rejs[<span class="string">'reactions'</span>],</span><br><span class="line">          <span class="string">'topics'</span>:rejs[<span class="string">'topics'</span>]&#125;],</span><br><span class="line">        columns=[<span class="string">'ID'</span>,<span class="string">'title'</span>,<span class="string">'content'</span>,<span class="string">'excerpt'</span>,<span class="string">'createdAt'</span>,<span class="string">'updatedAt'</span>,<span class="string">'commentCount'</span>,<span class="string">'forumName'</span>,<span class="string">'forumAlias'</span>,<span class="string">'gender'</span>,<span class="string">'likeCount'</span>,<span class="string">'reactions'</span>,<span class="string">'topics'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嘗試使用撰寫出的函數，抓取編號231030181的文章</span></span><br><span class="line">Crawl(<span class="number">231030181</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次讀取100篇最熱門的文章</span></span><br><span class="line">url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100'</span></span><br><span class="line">resq = requests.get(url)</span><br><span class="line">rejs = resq.json()</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">    df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]),ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過迴圈讀取10*100篇文章，若需讀取更多資料，可以將range(10)中的數值提升</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    last = str(int(df.tail(<span class="number">1</span>).ID)) <span class="comment"># 找出爬出資料的最後一筆ID</span></span><br><span class="line">    url = <span class="string">'https://www.dcard.tw/_api/posts?popular=true&amp;limit=100&amp;before='</span> + last</span><br><span class="line">    resq = requests.get(url)</span><br><span class="line">    rejs = resq.json()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(rejs)):</span><br><span class="line">        df = df.append(Crawl(rejs[i][<span class="string">'id'</span>]), ignore_index=<span class="keyword">True</span>)</span><br><span class="line">print(df.shape)</span><br><span class="line">df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將資料存到桌面</span></span><br><span class="line">df.to_excel(<span class="string">'C:/Users/TLYu0419/Desktop/Dcard.xlsx'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> 爬蟲(Crawl) </tag>
            
            <tag> Dcard </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>猜數字遊戲</title>
      <link href="/2018/12/30/GuessNumber/"/>
      <url>/2018/12/30/GuessNumber/</url>
      <content type="html"><![CDATA[<p>這是運用while迴圈和if else的判斷式寫的一個簡單的猜數字遊戲。<br>透過提示使用者猜大一點、猜小一點來引導使用者找到正確的答案！</p><a id="more"></a><h1>流程說明</h1><p>首先會從1到100的數值中隨機生成一個數值作為答案。<br>接著請使用者猜一個數字<br>- 如果大於答案，就會提示說：「太高囉，猜低一點！」<br>- 如果小於答案，則會提示說「太低囉，猜高一點！」<br>- 一直到猜到正確的答案後，跳出「恭喜答對囉！」的宣告語並結束這個迴圈。</p><h1>Python 的程式代碼如下</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">random = np.random.randint(1,100)</span><br><span class="line">condiction = False</span><br><span class="line">while condiction == False: # 符合條件就執行迴圈</span><br><span class="line">    guess = int(input(&quot;猜個數字吧 &gt;&gt;&quot;))</span><br><span class="line">    if guess &gt; random:</span><br><span class="line">        print(&quot;太高囉，猜低一點！&quot;)</span><br><span class="line">    elif guess &lt; random:</span><br><span class="line">        print(&quot;太低囉，猜高一點！)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;恭喜答對囉!&quot;)</span><br><span class="line">        condiction = True # 答對後，將繼續執行的條件改為False</span><br></pre></td></tr></table></figure><h1>使用畫面</h1><p><img src="/2018/12/30/GuessNumber/guess.JPG" alt="guess"></p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 迴圈 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>台灣的文化品味如何？階級與音樂品味的多變量分析</title>
      <link href="/2018/09/29/Taste-Cluster/"/>
      <url>/2018/09/29/Taste-Cluster/</url>
      <content type="html"><![CDATA[<p>這邊文章是從我的碩士論文「<a href="https://hdl.handle.net/11296/tu5y5c" target="_blank" rel="noopener">重新檢視文化資本對台灣社會階層區辨的有效性</a>」的資料修改而來，並與東華大學的莊致嘉老師以「<a href="http://www.edubook.com.tw/OAtw/File/PDf/406283.pdf" target="_blank" rel="noopener">反省文化資本理論在臺灣的有效性：文化品味、教育和階級的關聯性及其變遷</a>」共同發表在台灣教育社會學研究期刊上。有興趣的人可以透過以上連結檢視文化品為與社會階層之間的關聯性與其理論意義。在此僅分享研究流程的代碼留作紀錄。</p><a id="more"></a><ul><li><a href="#%E8%B3%87%E6%96%99%E4%BE%86%E6%BA%90">資料來源</a></li><li><a href="#%E5%88%86%E6%9E%90%E6%B5%81%E7%A8%8B">分析流程</a></li><li><a href="#%E8%B3%87%E6%96%99%E6%B8%85%E7%90%86">資料清理</a></li><li><a href="#%E7%B5%B1%E8%A8%88%E5%88%86%E7%BE%A4">統計分群</a></li><li><a href="#%E5%A4%9A%E9%A0%85%E9%82%8F%E8%BC%AF%E8%BF%B4%E6%AD%B8">多項邏輯迴歸</a></li><li><a href="#%E7%B8%BD%E7%B5%90">總結</a></li></ul><h1>資料來源</h1><p>這次使用的資料是來自中央研究院的<a href="http://www.ios.sinica.edu.tw/sc/cht/home.php" target="_blank" rel="noopener">台灣社會變遷調查</a>資料庫中2007年的<a href="http://www.ios.sinica.edu.tw/sc/cht/download.php?fn=tscs071.pdf" target="_blank" rel="noopener">社會階層組的調查問卷</a>的資料進行分析。這是份公開的資料，對於想要再製或再進一步深入研究的人來說是非常棒的資料源。</p><h1>分析流程</h1><p>這次的分析會從原始資料中提取部分變來探討階級與品味間的關聯性，如下表的說明，a1到f1是我使用的解釋變數，並且我將透過g3a到g3i題組（受訪者消費各種音樂頻率）來提取文化品味的分群結果。讓每位受訪者都有自己的一種文化品味偏好的標籤。最後我會運用解釋變數來解釋文化品味的差異，找出階級與品味之間的關聯性。</p><style> table th:nth-of-type(1) { width: 10px; } </style><style> table th:nth-of-type(2) { width: 90px; } </style><table><thead><tr><th style="text-align:left">代碼</th><th style="text-align:left">變項名稱</th><th style="text-align:left">說明</th></tr></thead><tbody><tr><td style="text-align:left">a1</td><td style="text-align:left">性別</td><td style="text-align:left">分為男性和女性，以女性為對照組</td></tr><tr><td style="text-align:left">age</td><td style="text-align:left">年齡</td><td style="text-align:left">調查時受訪者的年齡</td></tr><tr><td style="text-align:left">c1</td><td style="text-align:left">教育程度</td><td style="text-align:left">分為國小、國中…碩士、博士等22個類別，在此轉換為對應的教育年數</td></tr><tr><td style="text-align:left">c8</td><td style="text-align:left">父親教育</td><td style="text-align:left">同c1題項的處理方式</td></tr><tr><td style="text-align:left">d2b3r</td><td style="text-align:left">父親職位</td><td style="text-align:left">受訪者15歲時父親的職業地位分數，轉換的標準是按照黃毅志(2008)的研究進行轉換</td></tr><tr><td style="text-align:left">f1</td><td style="text-align:left">主觀地位</td><td style="text-align:left">請受訪者從1到10分中挑選符合自己社會地位的分數</td></tr><tr><td style="text-align:left">g3a</td><td style="text-align:left">台語流行歌</td><td style="text-align:left">詢問受訪者常不常聽台語流行歌，分為從不、很少、有時、經常等四類</td></tr><tr><td style="text-align:left">g3b</td><td style="text-align:left">國語流行歌</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3c</td><td style="text-align:left">日本流行歌</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3d</td><td style="text-align:left">西洋流行歌</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3e</td><td style="text-align:left">古典音樂</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3f</td><td style="text-align:left">國樂</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3g</td><td style="text-align:left">平劇</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3h</td><td style="text-align:left">歌仔戲</td><td style="text-align:left">同g3a題項的分類方式</td></tr><tr><td style="text-align:left">g3i</td><td style="text-align:left">布袋戲</td><td style="text-align:left">同g3a題項的分類方式</td></tr></tbody></table><h1>資料清理</h1><p>載入使用套件，並按照上表清理資料。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">library(tidyverse)</span><br><span class="line">library(magrittr)</span><br><span class="line">library(foreign)</span><br><span class="line">library(PCAmixdata)</span><br><span class="line">library(nnet)</span><br><span class="line">library(DescTools)</span><br><span class="line">library(stargazer)</span><br><span class="line">library(fpc)</span><br></pre></td></tr></table></figure><p>按照上表的說明整理資料。由於資料處理的過程比較繁雜，在此僅列出程式碼提供參考，有興趣的讀者請自行檢視。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">tscs2007 &lt;- </span><br><span class="line">  # 從中研院社會變遷調查資料庫下載資料</span><br><span class="line">  read.spss(file = &quot;https://www.ios.sinica.edu.tw/sc/cht/datafile/tscs071.sav&quot;, </span><br><span class="line">                      use.value.labels = F, </span><br><span class="line">                      to.data.frame = T) %&gt;%</span><br><span class="line">  # 按照上表挑選分析所需之變項</span><br><span class="line">  select(a1, age, c1, c8, d2b3_r, f1, g3a:g3i) %&gt;% </span><br><span class="line">  mutate(</span><br><span class="line">    # 將性別轉換為虛擬變項，女性為對照組</span><br><span class="line">    a1 = case_when( </span><br><span class="line">      grepl(&quot;1&quot;, a1) ~ 1,</span><br><span class="line">      grepl(&quot;2&quot;, a1) ~ 0),</span><br><span class="line">    # 將父親的職業類別按黃毅志(2008)的研究轉換為對應的社經地位分數</span><br><span class="line">    d2b3_r = case_when( </span><br><span class="line">      grepl(&quot;110&quot;, d2b3_r) ~ 83.3,</span><br><span class="line">      grepl(&quot;120|130|140&quot;, d2b3_r) ~ 81.4,</span><br><span class="line">      grepl(&quot;201&quot;, d2b3_r) ~ 87.9,</span><br><span class="line">      grepl(&quot;202&quot;, d2b3_r) ~ 81.1,</span><br><span class="line">      grepl(&quot;211|221&quot;, d2b3_r) ~ 86.0,</span><br><span class="line">      grepl(&quot;212|213|214&quot;, d2b3_r) ~ 80.0,</span><br><span class="line">      grepl(&quot;222|223&quot;, d2b3_r) ~ 79.1,</span><br><span class="line">      grepl(&quot;230&quot;, d2b3_r) ~ 85.1,</span><br><span class="line">      grepl(&quot;250&quot;, d2b3_r) ~ 83.2,</span><br><span class="line">      grepl(&quot;301|302|303&quot;, d2b3_r) ~ 78.4,</span><br><span class="line">      grepl(&quot;311&quot;, d2b3_r) ~ 80.1,</span><br><span class="line">      grepl(&quot;312|314&quot;, d2b3_r) ~ 74.5,</span><br><span class="line">      grepl(&quot;313&quot;, d2b3_r) ~ 78.1,</span><br><span class="line">      grepl(&quot;321|322|340&quot;, d2b3_r) ~ 77.5,</span><br><span class="line">      grepl(&quot;331&quot;, d2b3_r) ~ 78.8,</span><br><span class="line">      grepl(&quot;332&quot;, d2b3_r) ~ 77.2,</span><br><span class="line">      grepl(&quot;350|360&quot;, d2b3_r) ~ 80.1,</span><br><span class="line">      grepl(&quot;370&quot;, d2b3_r) ~ 81.9,</span><br><span class="line">      grepl(&quot;410&quot;, d2b3_r) ~ 76.5,</span><br><span class="line">      grepl(&quot;420|511&quot;, d2b3_r) ~ 74.3,</span><br><span class="line">      grepl(&quot;431&quot;, d2b3_r) ~ 76.0,</span><br><span class="line">      grepl(&quot;432&quot;, d2b3_r) ~ 76.7,</span><br><span class="line">      grepl(&quot;512|514&quot;, d2b3_r) ~ 66.8,</span><br><span class="line">      grepl(&quot;513&quot;, d2b3_r) ~ 68.9,</span><br><span class="line">      grepl(&quot;515|516&quot;, d2b3_r) ~ 73.1,</span><br><span class="line">      grepl(&quot;520&quot;, d2b3_r) ~ 76.9,</span><br><span class="line">      grepl(&quot;531&quot;, d2b3_r) ~ 71.8,</span><br><span class="line">      grepl(&quot;532&quot;, d2b3_r) ~ 67.3,</span><br><span class="line">      grepl(&quot;610&quot;, d2b3_r) ~ 66.0,</span><br><span class="line">      grepl(&quot;620&quot;, d2b3_r) ~ 65.9,</span><br><span class="line">      grepl(&quot;710&quot;, d2b3_r) ~ 72.0,</span><br><span class="line">      grepl(&quot;720&quot;, d2b3_r) ~ 74.2,</span><br><span class="line">      grepl(&quot;790&quot;, d2b3_r) ~ 71.1,</span><br><span class="line">      grepl(&quot;810|840&quot;, d2b3_r) ~ 70.7,</span><br><span class="line">      grepl(&quot;820&quot;, d2b3_r) ~ 70.8,</span><br><span class="line">      grepl(&quot;830&quot;, d2b3_r) ~ 69.4,</span><br><span class="line">      grepl(&quot;910&quot;, d2b3_r) ~ 66.1,</span><br><span class="line">      grepl(&quot;920&quot;, d2b3_r) ~ 71.0,</span><br><span class="line">      grepl(&quot;930&quot;, d2b3_r) ~ 65.7,</span><br><span class="line">      grepl(&quot;940&quot;, d2b3_r) ~ 64.5,</span><br><span class="line">      grepl(&quot;950&quot;, d2b3_r) ~ 64.6,</span><br><span class="line">      grepl(&quot;960&quot;, d2b3_r) ~ 69.6),</span><br><span class="line">    # 主觀地位分數</span><br><span class="line">    f1 = case_when(between(f1, 1, 10) ~ f1)) %&gt;% </span><br><span class="line">  mutate_at(</span><br><span class="line">     # 教育程度轉換為相對應的教育年數</span><br><span class="line">    vars(c1,c8),</span><br><span class="line">    funs(case_when(</span><br><span class="line">      . == 1 ~ 0, </span><br><span class="line">      . == 2 ~ 3,</span><br><span class="line">      . == 3 ~ 6,</span><br><span class="line">      between(., 4, 5) ~ 9 ,</span><br><span class="line">      between(., 6, 9) ~ 12 ,</span><br><span class="line">      between(., 10, 15) ~ 14 ,</span><br><span class="line">      between(., 16, 19) ~ 16 ,</span><br><span class="line">      . == 20 ~ 18,</span><br><span class="line">      . == 21 ~ 22)))%&gt;%</span><br><span class="line">  mutate_at(</span><br><span class="line">    # 將文化品味的題組，從「經常」到「從不」分別給予4到1分</span><br><span class="line">    vars(g3a,g3b,g3c,g3d,g3e,g3f,g3g,g3h,g3i), </span><br><span class="line">    funs(case_when(</span><br><span class="line">      . == 1 ~ 4,</span><br><span class="line">      . == 2 ~ 3,</span><br><span class="line">      . == 3 ~ 2,</span><br><span class="line">      . == 4 ~ 1)))%&gt;%</span><br><span class="line">  # 排除遺漏值</span><br><span class="line">  na.omit(.)</span><br></pre></td></tr></table></figure><h1>統計分群</h1><p>由於文化品味之間或多或少都會存在一些關聯性，並不適合直接進行k-medoids分群，在此我們會先透過主成分分析與因子轉軸<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>來排除變項間存在相關性的問題，接著再將提取出特徵值大於1的維度透過k-medoids進行統計分群。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">culture_PCA &lt;- </span><br><span class="line">  # 運用g3a到g3i等9項題目進行主成分分析</span><br><span class="line">  PCAmix(X.quanti = select(tscs2007, g3a:g3i), graph = F) %&gt;%</span><br><span class="line">  # 提取特徵值大於1的維度並進行轉軸</span><br><span class="line">  PCArot(dim=sum(.$eig[,1]&gt;=1), graph = F)</span><br><span class="line"></span><br><span class="line"># 檢視轉軸過後各維度的意義</span><br><span class="line">culture_PCA$sqload%&gt;%round(2)</span><br></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">Var</th><th style="text-align:center">dim1.rot</th><th style="text-align:center">dim2.rot</th><th style="text-align:center">dim3.rot</th></tr></thead><tbody><tr><td style="text-align:left">g3a</td><td style="text-align:center">0.06</td><td style="text-align:center"><strong>0.42</strong></td><td style="text-align:center">0.20</td></tr><tr><td style="text-align:left">g3b</td><td style="text-align:center">0.00</td><td style="text-align:center">0.02</td><td style="text-align:center"><strong>0.71</strong></td></tr><tr><td style="text-align:left">g3c</td><td style="text-align:center">0.07</td><td style="text-align:center">0.00</td><td style="text-align:center"><strong>0.41</strong></td></tr><tr><td style="text-align:left">g3d</td><td style="text-align:center">0.12</td><td style="text-align:center">0.04</td><td style="text-align:center"><strong>0.55</strong></td></tr><tr><td style="text-align:left">g3e</td><td style="text-align:center"><strong>0.44</strong></td><td style="text-align:center">0.01</td><td style="text-align:center">0.19</td></tr><tr><td style="text-align:left">g3f</td><td style="text-align:center"><strong>0.62</strong></td><td style="text-align:center">0.04</td><td style="text-align:center">0.02</td></tr><tr><td style="text-align:left">g3g</td><td style="text-align:center"><strong>0.51</strong></td><td style="text-align:center">0.06</td><td style="text-align:center">0.00</td></tr><tr><td style="text-align:left">g3h</td><td style="text-align:center">0.09</td><td style="text-align:center"><strong>0.57</strong></td><td style="text-align:center">0.03</td></tr><tr><td style="text-align:left">g3i</td><td style="text-align:center">0.04</td><td style="text-align:center"><strong>0.54</strong></td><td style="text-align:center">0.00</td></tr></tbody></table><p>在這裡我將因子載荷係數大於0.3的項目用粗體標示，如果文化品味在該維度上的係數值越高，表示該維度反映越多的該文化品味的意義。而上表的結果顯示，維度1主要反映的是古典音樂(g3e)、國樂(g3f)、平劇(g3g)等項目，我將其命名為消費高雅文化的程度。維度2主要反映的是台語流行歌(g3a)、歌仔戲(g3h)、布袋戲(g3i)等題目，我將其命名為消費傳統文化的程度。維度3主要反映的是國語流行歌(g3b)、日本流行歌(g3c)、西洋流行歌(g3d)等題目，我將其命名為消費流行文化的程度。</p><p>在排除了變數之間相關性的問題之後，我們接下來就運用k-medoids演算法對每位受訪者的文化品味進行分群。在此我使用的函數是fpc套件中的pamk函數，這個函數的優點是當我們在進行分群時，我們只需要輸入資料集與分群組數的範圍，系統就會幫我們把這個範圍內的分群組數都計算出來，並根據分成各群時的silhouette係數從中挑選最佳的分群結果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 將提取後的結果併回資料中</span><br><span class="line">tscs2007 %&lt;&gt;% </span><br><span class="line">  bind_cols(culture_PCA$ind$coord %&gt;% </span><br><span class="line">              as_tibble(.)) </span><br><span class="line"># 選擇提取出的三個維度進行分群</span><br><span class="line">set.seed(123)</span><br><span class="line">pamk.best &lt;- pamk(select(tscs2007, ends_with(&quot;rot&quot;)), </span><br><span class="line">                  # 從區分成2到10群的結果中找出最佳分群組數</span><br><span class="line">                  krange=2:10) </span><br><span class="line"># 將分群結果併回資料集</span><br><span class="line">tscs2007 %&lt;&gt;% mutate(cluster = pamk.best$pamobject$clustering) </span><br><span class="line"></span><br><span class="line">tscs2007 %&gt;% # 檢視分群結果</span><br><span class="line">  group_by(cluster)%&gt;%</span><br><span class="line">  select(ends_with(&quot;rot&quot;)) %&gt;%</span><br><span class="line">  summarise_all(funs(mean(.)))</span><br></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">cluster</th><th style="text-align:center">dim1.rot</th><th style="text-align:center">dim2.rot</th><th style="text-align:center">dim3.rot</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:center">-0.17</td><td style="text-align:center">-1.01</td><td style="text-align:center">1.30</td></tr><tr><td style="text-align:left">2</td><td style="text-align:center">-1.09</td><td style="text-align:center">1.03</td><td style="text-align:center">0.08</td></tr><tr><td style="text-align:left">3</td><td style="text-align:center">2.17</td><td style="text-align:center">0.70</td><td style="text-align:center">0.24</td></tr><tr><td style="text-align:left">4</td><td style="text-align:center">-0.26</td><td style="text-align:center">-0.85</td><td style="text-align:center">-1.74</td></tr></tbody></table><p>從以上的結果顯示，分群1只消費流行文化，但是在高雅文化與傳統文化的分數都是負值，我將這群人稱為「純流行」。分群2則偏好傳統文化與流行文化，(但流行文化的係數非常接近0)，並且不消費高雅文化，我將其稱為「純傳統」。分群三在每種文化品味上都有正面的偏好，其中最偏好高雅文化，另外在傳統、流行文化也會涉略，我將其稱為「雜食」。最後分群4則與分群3相反，對於每種音樂品味的偏好都是負值，我將其稱為「不聽音樂」。</p><h1>多項邏輯迴歸</h1><p>到目前為止我們已經從原始的9項音樂品味創造出一個「4分類的音樂品味」變項囉，這4分類分別是「純流行(popular)」、「純傳統(tradition)」、「雜食(omnivore)」與「不聽音樂(none)」的四個標籤。而這也就是我們後續分析的目標變數，接著我將運用受訪者的性別、年齡、教育年數、父親教育年數、主觀社會地位以及父親的階級來解釋文化品味。操作過程如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tscs2007 %&lt;&gt;%  </span><br><span class="line">  # 將解釋變數標準化</span><br><span class="line">  mutate_at(vars(a1, age, c1, c8, f1, d2b3_r),</span><br><span class="line">            funs(as.numeric(scale(.)))) %&gt;%</span><br><span class="line">  mutate(cluster = factor(cluster,c(1, 2, 3, 4), c(&quot;popular&quot;,&quot;tradition&quot;,&quot;omnivore&quot;,&quot;none&quot;)),</span><br><span class="line">         cluster = relevel(cluster, ref = &quot;omnivore&quot;))</span><br><span class="line"></span><br><span class="line">mlogist &lt;- multinom(cluster ~ a1 + age + c8+ d2b3_r + c1 + f1 , # 運用a1、age、c8、d2b3_r、c1、f1等變數解釋文化品味</span><br><span class="line">                    data = tscs2007)</span><br><span class="line"></span><br><span class="line">PseudoR2(mlogist, c(&quot;McFaddenAdj&quot;,&quot;CoxSnell&quot;, &quot;Nagelkerke&quot;, &quot;BIC&quot;, &quot;LogLik&quot;))</span><br><span class="line">stargazer(mlogist, type=&quot;text&quot;, out=&quot;multi1.htm&quot;,summary = T)</span><br></pre></td></tr></table></figure><p>輸出並整理以上的分析結果如下表</p><table><thead><tr><th style="text-align:left"></th><th style="text-align:center">Popular<br>vs.<br>omnivore</th><th style="text-align:center">Tradition<br>vs.<br>omnivore</th><th style="text-align:center">none<br>vs.<br>omnivore</th></tr></thead><tbody><tr><td style="text-align:left">性別</td><td style="text-align:center">0.039<br>(0.075)</td><td style="text-align:center">0.286***<br>(0.075)</td><td style="text-align:center">0.136*<br>(0.082)</td></tr><tr><td style="text-align:left">年齡</td><td style="text-align:center">-0.936***<br>(0.110)</td><td style="text-align:center">-0.445***<br>(0.103)</td><td style="text-align:center">0.072<br>(0.109)</td></tr><tr><td style="text-align:left">父親教育</td><td style="text-align:center">-0.172*<br>(0.104)</td><td style="text-align:center">-0.407***<br>(0.104)</td><td style="text-align:center">-0.471***<br>(0.113)</td></tr><tr><td style="text-align:left">父親職位</td><td style="text-align:center">0.118**<br>(0.088)</td><td style="text-align:center">0.002<br>(0.091)</td><td style="text-align:center">0.043<br>(0.100)</td></tr><tr><td style="text-align:left">教育年數</td><td style="text-align:center">0.075<br>(0.136)</td><td style="text-align:center">-1.085***<br>(0.125)</td><td style="text-align:center">-0.918***<br>(0.132)</td></tr><tr><td style="text-align:left">主觀地位</td><td style="text-align:center">0.002<br>(0.084)</td><td style="text-align:center">-0.115<br>(0.079)</td><td style="text-align:center">-0.321***<br>(0.086)</td></tr><tr><td style="text-align:left">常數</td><td style="text-align:center">-0.179*<br>(0.100)</td><td style="text-align:center">0.491***<br>(0.078)</td><td style="text-align:center">0.042<br>(0.088)</td></tr><tr><td style="text-align:left">PseudoR2</td><td style="text-align:center">0.150(McFaddenAdj)<br>0.353(CoxSnell)<br>0.377(Nagelkerke)</td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><p>如上表，整體模型的PseudoR^2值為0.150(McFaddenAdj)、0.353(CoxSnell)與0.377(Nagelkerke)。首先在Popular vs. omnivore 的模型中，年齡與父親教育年數越高者容易成為雜食，而父親職業地位的增加則較容易成為純流行。其次在Tradition vs. omnivore的模型來看，男性較容易成為純傳統，而年齡、父親教育年數與教育年數的增加則較容易成為雜食。最後在none vs. omnivore的模型來看，男性較容易成為不聽音樂，而父親教育年數、教育年數與主觀地位的增加則較容易成為雜食。</p><p>整體來看，從文化品味的分群結果可以發現，文化品味並非如同源論的論述(各階級僅偏好屬於自己階級的文化)也非個人化論的論述(文化品味完全是屬於個人的偏好)，實際上台灣的文化品味較接近走在這兩個論點中間的雜食論，固然有一群消費者僅消費流行文化或傳統文化，然而也有消費各式各樣文化品味的群體以及都不消費任何文化活動的群體。並且我們從多項邏輯回歸中也發現在相關文獻中提到的個人特質(性別、年齡)、家庭背景(父親教育、父親地位)與代表個人的階級地位(教育年數、主觀地位)等都是影響文化品味的重要變數。</p><p>另外一項有趣的發現是同樣都是代表家庭背景的父親教育與父親職位有著截然不同的發現，如果研究者使用父親教育作為家庭背景的操作化指標，會發現父親教育越高越容易成為雜食者；然而若是使用父親職位作為操作化指標，則會發現父親職業地位越高受訪者較容易成為純流行。這會隨著受訪者採用不同的指標來測量家庭背景的概念會有不同的研究發現。</p><h1>總結</h1><p>這篇文章主要是從我的碩士論文修改而來，主要演示了一次運用PCA與PAM分群的方法提取潛在結構，將「多個文化品味的消費頻率」題項簡化為一個多分類的文化品味標籤。並將其作為目標變數，運用性別、年齡、教育程度、父親教育、父親職位等變項來建立多項邏輯回歸的模型的方法。<br>透過上述分析我們可以發現許多有意思的發現，如文化品味可以如何分群，以及文化品味與階級之間的關聯性又是如何。另外在原始資料中，除了測量音樂品味消費頻率的題項之外還有休閒品味、閱讀品味的消費頻率題項。有興趣的讀者可以修改上述的代碼，再進行一次以上分析，相信還會有許多有意思的發現！</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>需要留意的是，將順序尺度的資料當作等距尺度進行分析是存在統計上的疑慮的。實務上為了資料分析的方便性，我們仍然經常直接將這類資料進行當作等距資料進行加減、平均等等操作。若是追求精確、嚴謹的研究者，可以考慮運用<a href="https://www.jstatsoft.org/article/view/v031i04/v31i04.pdf" target="_blank" rel="noopener">homals套件</a>將順序尺度的資料轉換為等距尺度後，再進行後續的統計分析。 <a href="#fnref1" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
      
      
        <tags>
            
            <tag> R programming language </tag>
            
            <tag> k-medoids cluster </tag>
            
            <tag> multinomial logistic regression </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
